{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#01_data_processing\n",
    "- Loads job descriptions from CSV (either from S3 or locally)\n",
    "- Extracts qualifications from JSON format\n",
    "- Assigns unique IDs to each job\n",
    "- Saves individual job description files as JSON\n",
    "- Creates formatted text for reference purposes"
   ],
   "metadata": {
    "id": "W_QjVRyydaBc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "metadata": {
    "id": "xtQNBd-ZM0uX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# S3 Configuration\n",
    "S3_ENABLED = True  # Set to False to use local storage only\n",
    "S3_BUCKET = \"mirra-matcher-325\"\n",
    "S3_RAW_PATH = \"data/raw/job_description_extraction_samples_1.csv\"  # S3 path to CSV file\n",
    "S3_PROCESSED_PREFIX = \"data/processed/jobs/\"  # S3 prefix for processed JSON files\n",
    "\n",
    "# for local storage or temporary files when using S3\n",
    "LOCAL_RAW_DIR = \"../data/raw\"\n",
    "LOCAL_PROCESSED_DIR = \"../data/processed/jobs\"\n",
    "LOCAL_RAW_FILE = os.path.join(LOCAL_RAW_DIR, \"job_descriptions.csv\")\n",
    "\n",
    "# create local directories for temporary storage\n",
    "os.makedirs(LOCAL_RAW_DIR, exist_ok=True)\n",
    "os.makedirs(LOCAL_PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# S3 utility functions\n",
    "def s3_file_exists(bucket, key):\n",
    "    \"\"\"Check if a file exists in an S3 bucket.\"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket, Key=key)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        # The file does not exist or we don't have permission\n",
    "        return False"
   ],
   "outputs": [],
   "metadata": {
    "id": "fhN32Q4jM4mL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def download_from_s3(bucket, key, local_path):\n",
    "    \"\"\"Download a file from S3 to local storage.\"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        print(f\"Downloading s3://{bucket}/{key} to {local_path}\")\n",
    "        s3_client.download_file(bucket, key, local_path)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"Error downloading from S3: {e}\")\n",
    "        return False\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "W-ADE2ohNgqJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def upload_to_s3(local_path, bucket, key):\n",
    "    \"\"\"Upload a file from local storage to S3.\"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        print(f\"Uploading {local_path} to s3://{bucket}/{key}\")\n",
    "        s3_client.upload_file(local_path, bucket, key)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"Error uploading to S3: {e}\")\n",
    "        return False"
   ],
   "outputs": [],
   "metadata": {
    "id": "Ic0OdX0gca3B"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# function to load data from either s3 or local storage\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load job description data from CSV file.\n",
    "    First tries S3 if enabled, then local file system.\n",
    "    If no file is found, provides options for upload or sample data.\n",
    "    \"\"\"\n",
    "    # will try loading from S3 first if enabled\n",
    "    df = None\n",
    "    if S3_ENABLED:\n",
    "        if s3_file_exists(S3_BUCKET, S3_RAW_PATH):\n",
    "            # Download to local temporary file\n",
    "            if download_from_s3(S3_BUCKET, S3_RAW_PATH, LOCAL_RAW_FILE):\n",
    "                df = pd.read_csv(LOCAL_RAW_FILE)\n",
    "                print(f\"Loaded {len(df)} rows from S3: s3://{S3_BUCKET}/{S3_RAW_PATH}\")\n",
    "                return df\n",
    "            else:\n",
    "                print(\"Failed to download file from S3\")\n",
    "\n",
    "    # If S3 loading failed or is disabled, try local file\n",
    "    if os.path.exists(LOCAL_RAW_FILE):\n",
    "        df = pd.read_csv(LOCAL_RAW_FILE)\n",
    "        print(f\"Loaded {len(df)} rows from local file: {LOCAL_RAW_FILE}\")\n",
    "        return df\n",
    "\n",
    "    # If no file is found, try interactive methods or provide sample\n",
    "    print(f\"File not found locally: {LOCAL_RAW_FILE}\")\n",
    "    try:\n",
    "        # Try using Colab's file upload (if in Colab environment)\n",
    "        from google.colab import files\n",
    "        print(\"Uploading file via Colab...\")\n",
    "        uploaded = files.upload()\n",
    "        filename = list(uploaded.keys())[0]\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        # Save to the expected local location\n",
    "        df.to_csv(LOCAL_RAW_FILE, index=False)\n",
    "        print(f\"Saved uploaded file to {LOCAL_RAW_FILE}\")\n",
    "\n",
    "        # Also upload to S3 if enabled\n",
    "        if S3_ENABLED:\n",
    "            upload_to_s3(LOCAL_RAW_FILE, S3_BUCKET, S3_RAW_PATH)\n",
    "    except ImportError:\n",
    "        print(\"using sample data\")\n",
    "        # Create a minimal sample dataframe for testing\n",
    "        df = pd.DataFrame({\n",
    "            \"id\": [\"sample_001\"],\n",
    "            \"title\": [\"Software Engineer - ML Tools Support\"],\n",
    "            \"description\": [\"Sample job description\"],\n",
    "            \"label\": ['''{\"details\":{\"job_title\":[\"Software Engineer - ML Tools Support - 314915\"],\"job_title_base\":[\"Software Engineer\"],\"company_name\":[\"Trillium Teamologies\"],\"industry\":[],\"employment_type\":[\"Full-time\"],\"wage\":[],\"location\":[{\"city\":\"\",\"state\":\"MI\",\"country\":\"US\"}],\"wfh_policy\":[\"Hybrid\"],\"travel_required\":{\"required\":false,\"hours_weekly\":0},\"benefits\":{\"medical\":false,\"dental\":false,\"vision\":false,\"mental_health\":false,\"hsa\":false,\"fsa\":false,\"401k_match\":false,\"equity\":false,\"unlimited_pto\":false,\"tuition_reimbursement\":false,\"bonus\":false,\"other\":[]},\"company_stage\":[],\"tax_terms\":[\"Direct-hire\"],\"experience_level\":[\"Senior\"],\"work_schedule\":[],\"work_authorization\":[\"Does Not Offer Sponsorship\"]},\"mandatory\":{\"hard_skills\":[{\"skill\":[\"Agile practices in solution delivery\"],\"minyears\":[0]}],\"education\":[{\"education_level\":[\"Bachelor's\"],\"field_of_study\":[\"Computer Science\"]}],\"credentials\":[{\"credential\":[\"General Coding Proficiency Test\"]}],\"professional_background\":[{\"background\":[\"software engineer\"],\"minyears\":[4],\"industry\":[]}]},\"preferred\":{\"hard_skills\":[{\"skill\":[\"Python\"],\"minyears\":[0]}],\"education\":[],\"credentials\":[],\"professional_background\":[]},\"responsibility\":{\"hard_skills\":[{\"skill\":[\"Manage machine learning tools using Agile Methodology\"]}],\"professional_background\":[{\"background\":[\"Collaborate with other software engineers\"]}]}}''']\n",
    "        })\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {
    "id": "vV9LjkyYNi4D"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "df = load_data()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading s3://mirra-matcher-325/data/raw/job_description_extraction_samples_1.csv to ../data/raw/job_descriptions.csv\n",
      "Loaded 25 rows from S3: s3://mirra-matcher-325/data/raw/job_description_extraction_samples_1.csv\n"
     ]
    }
   ],
   "metadata": {
    "id": "CQjocC3sa2co"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(\"\\nDataFrame columns:\")\n",
    "print(df.columns.tolist())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "DataFrame columns:\n",
      "['snapshot_id', 'id', 'title', 'description', 'location', 'salary', 'pay_rate', 'telecommute_option', 'job_length', 'tax_terms', 'source_app', 'company_name', 'web_url', 'company', 'year', 'month', 'day', 'id_rank', 'rn', 'description_length', 'description_new', 'reformatted', 'label']\n"
     ]
    }
   ],
   "metadata": {
    "id": "2WHCBSsNa4TV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def extract_qualifications(label_json):\n",
    "    \"\"\"\n",
    "    Parses the JSON string from the 'label' column and returns a dictionary.\n",
    "    Returns an empty dict on error.\n",
    "    \"\"\"\n",
    "    if pd.isna(label_json):\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        if isinstance(label_json, dict):\n",
    "            return label_json\n",
    "        return json.loads(label_json)\n",
    "    except (json.JSONDecodeError, TypeError) as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        return {}"
   ],
   "outputs": [],
   "metadata": {
    "id": "CRMAkJPAa7eM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "df[\"qualifications\"] = df[\"label\"].apply(extract_qualifications)"
   ],
   "outputs": [],
   "metadata": {
    "id": "t5Sz_ENJa-Xq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# create a unique ID for each job if not present\n",
    "if \"id\" not in df.columns or df[\"id\"].isna().any():\n",
    "    print(\"Adding job IDs where missing...\")\n",
    "    if \"id\" not in df.columns:\n",
    "        df[\"id\"] = [f\"job_{uuid.uuid4()}\" for _ in range(len(df))]\n",
    "    else:\n",
    "        df.loc[df[\"id\"].isna(), \"id\"] = [f\"job_{uuid.uuid4()}\" for _ in range(df[\"id\"].isna().sum())]\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "IBbZlO1wbAbe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# save each job as an individual JSON file\n",
    "saved_count = 0\n",
    "for _, row in df.iterrows():\n",
    "    job_id = row[\"id\"]\n",
    "    qualifications = row[\"qualifications\"]\n",
    "\n",
    "    # Skip rows with empty qualifications\n",
    "    if not qualifications:\n",
    "        continue\n",
    "\n",
    "    # Ensure job_id is in the qualifications\n",
    "    qualifications[\"job_id\"] = job_id\n",
    "\n",
    "    # Save locally first\n",
    "    local_output_path = os.path.join(LOCAL_PROCESSED_DIR, f\"{job_id}.json\")\n",
    "    with open(local_output_path, 'w') as f:\n",
    "        json.dump(qualifications, f, indent=2)\n",
    "\n",
    "    # Then upload to S3 if S3 is enabled\n",
    "    if S3_ENABLED:\n",
    "        s3_key = f\"{S3_PROCESSED_PREFIX}{job_id}.json\"\n",
    "        try:\n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.upload_file(local_output_path, S3_BUCKET, s3_key)\n",
    "            print(f\"Uploaded to s3://{S3_BUCKET}/{s3_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading to S3: {str(e)}\")\n",
    "\n",
    "    saved_count += 1\n",
    "\n",
    "print(f\"\\nProcessed and saved {saved_count} job descriptions locally to {LOCAL_PROCESSED_DIR}\")\n",
    "if S3_ENABLED:\n",
    "    print(f\"and to S3 at s3://{S3_BUCKET}/{S3_PROCESSED_PREFIX}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/a2998f35-d332-4fee-8170-c9c946fab8e0.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/5e00aafa-57c3-4c06-8b06-1fe311bc8e4e.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/f20e91c6-5042-44a7-a065-6a9454797a47.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/0d652f5d-9822-4b9b-8274-65674eb68b42.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/90a9f50c-584c-49a4-ac5e-5928ff8a4b94.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/44c8bc5c-49c1-4467-b7b4-fdb2e7e11790.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/f71df3a6-441f-44af-bd75-f6d922e08a37.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/69c34396-8455-47cf-a8df-d09dbd7e31ea.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/9d15576d-1d0f-4396-88d7-4ea76a13d639.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/babf4bb0-7f3a-48e6-be4b-53e3934f9e0a.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/901969dd-7034-47f0-b539-30dacd5e928d.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/3acaf400-50b0-4906-8c86-ed7184df8712.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/0ca4cd66-bba5-4efa-8b26-ecc57bb3939e.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/11afeb1b-bfe3-414e-9946-4f199f93f011.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/cd540421-60f8-4a39-acb5-e27a0659e83f.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/c51dca25-7ccf-4b90-a640-2a095c3aaea1.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/04a9e436-fbc3-4d2f-a543-a6474dc485f9.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/b5c6e3df-df3a-4e5b-a850-8ebe8be6fcc6.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/a7fe9a21-6a8d-4069-ae47-57083bb97667.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/f6aeaba8-87f5-498c-919a-8d25d63d3d9e.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/1f58b149-ed8e-4fb2-96dd-d83d266f5310.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/57cea150-f3f7-4238-95a5-1db5b6e940eb.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/98ab83df-021a-4889-9af3-bd6664d725e2.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/d84838c8-db39-4750-9021-64c5e2c14fe1.json\n",
      "Uploaded to s3://mirra-matcher-325/data/processed/jobs/d645a2b1-4888-40f7-be2b-efed95803965.json\n",
      "\n",
      "Processed and saved 25 job descriptions locally to ../data/processed/jobs\n",
      "and to S3 at s3://mirra-matcher-325/data/processed/jobs/\n"
     ]
    }
   ],
   "metadata": {
    "id": "tLssFhzqbGW5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# formatted text version for backup/reference\n",
    "def format_for_embedding(qualifications):\n",
    "    \"\"\"\n",
    "    Formats the structured qualifications JSON into a clean text string.\n",
    "    This is for reference only - the embedding process will use the JSON files.\n",
    "    \"\"\"\n",
    "    details = qualifications.get(\"details\", {})\n",
    "    job_title = \", \".join(details.get(\"job_title\", []))\n",
    "\n",
    "    mandatory = qualifications.get(\"mandatory\", {})\n",
    "    # Process hard skills\n",
    "    hard_skills = \", \".join([item[\"skill\"][0] for item in mandatory.get(\"hard_skills\", []) if item.get(\"skill\")])\n",
    "    # Process education information\n",
    "    education = \", \".join([\n",
    "        (edu[\"education_level\"][0] if edu.get(\"education_level\") else \"\") +\n",
    "        (\" in \" + \", \".join(edu.get(\"field_of_study\", [])) if edu.get(\"field_of_study\") else \"\")\n",
    "        for edu in mandatory.get(\"education\", [])\n",
    "    ])\n",
    "\n",
    "    formatted_text = (\n",
    "        f\"Job Title: {job_title}\\n\"\n",
    "        f\"Skills: {hard_skills}\\n\"\n",
    "        f\"Education: {education}\"\n",
    "    )\n",
    "    return formatted_text\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "s6Zrx8ATbN4G"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "if len(df) > 0:\n",
    "    sample_index = 0\n",
    "    sample_job = df.iloc[sample_index]\n",
    "    print(\"\\nSample job formatted text:\")\n",
    "    print(format_for_embedding(sample_job[\"qualifications\"]))\n",
    "\n",
    "    # Also show the path to the saved JSON\n",
    "    job_id = sample_job[\"id\"]\n",
    "    # Show both local and S3 paths\n",
    "    print(f\"\\nSaved locally to: {os.path.join(LOCAL_PROCESSED_DIR, f'{job_id}.json')}\")\n",
    "    if S3_ENABLED:\n",
    "        print(f\"Uploaded to S3: s3://{S3_BUCKET}/{S3_PROCESSED_PREFIX}{job_id}.json\")\n",
    "\n",
    "print(\"\\nData processing complete!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Sample job formatted text:\n",
      "Job Title: Software Engineer - ML Tools Support - 314915\n",
      "Skills: Agile practices in solution delivery, OpenShift, Google Cloud Platform, Gen AI projects, coding and software engineering best practices, Docker, Kubernetes, GitHub, end-to-end Machine Learning and Gen AI technology stack\n",
      "Education: Bachelor’s in Computer Science, Computer Engineering, Related\n",
      "\n",
      "Saved locally to: ../data/processed/jobs/a2998f35-d332-4fee-8170-c9c946fab8e0.json\n",
      "Uploaded to S3: s3://mirra-matcher-325/data/processed/jobs/a2998f35-d332-4fee-8170-c9c946fab8e0.json\n",
      "\n",
      "Data processing complete!\n"
     ]
    }
   ],
   "metadata": {
    "id": "sFJeo3e2bS4Z"
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}