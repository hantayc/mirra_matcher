{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#03_embedding_generation\n",
    "- Sets up the embedding generator using SageMaker\n",
    "- Loads job descriptions from files\n",
    "- Creates chunks from job descriptions using a custom chunker\n",
    "- Generates embeddings for each chunk\n",
    "- Uploads the embedded vectors to Pinecone\n",
    "- Includes batch processing for efficiency"
   ],
   "metadata": {
    "id": "dynvT9cEdx4s"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install pinecone"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!pip show pinecone"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Name: pinecone\n",
      "Version: 6.0.2\n",
      "Summary: Pinecone client and SDK\n",
      "Home-page: https://www.pinecone.io\n",
      "Author: Pinecone Systems, Inc.\n",
      "Author-email: support@pinecone.io\n",
      "License: Apache-2.0\n",
      "Location: /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages\n",
      "Requires: certifi, pinecone-plugin-interface, python-dateutil, typing-extensions, urllib3\n",
      "Required-by: \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Any, Union \n",
    "from tqdm import tqdm\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "Eh1Rdi4NVCIP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# SageMaker endpoint configuration\n",
    "\n",
    "SAGEMAKER_ENDPOINT_NAME = \"e5-embeddings-pooled-2\" \n",
    "AWS_REGION = \"us-east-1\"\n",
    "\n",
    "# Set Pinecone credentials directly\n",
    "PINECONE_API_KEY = \"\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\" # matched with AWS region\n",
    "PINECONE_INDEX_NAME = \"mirra-embeddings\"\n",
    "\n",
    "EMBEDDING_DIMENSION = 1024\n",
    "\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIMENSION}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Embedding dimension: 1024\n"
     ]
    }
   ],
   "metadata": {
    "id": "7rXs4HpmWK-w"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Initialize Pinecone with credentials\n",
    "from pinecone import Pinecone\n",
    "\n",
    "def initialize_pinecone():\n",
    "    \"\"\"Initialize Pinecone and return the index\"\"\"\n",
    "    try:\n",
    "        # Initialize Pinecone client\n",
    "        pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        \n",
    "        # Check if the index exists\n",
    "        existing_indexes = pc.list_indexes().names()\n",
    "        print(f\"Available Pinecone indexes: {existing_indexes}\")\n",
    "        \n",
    "        if PINECONE_INDEX_NAME not in existing_indexes:\n",
    "            print(f\"Creating new index '{PINECONE_INDEX_NAME}'...\")\n",
    "            \n",
    "            # Create the index\n",
    "            pc.create_index(\n",
    "                name=PINECONE_INDEX_NAME,\n",
    "                dimension=EMBEDDING_DIMENSION,\n",
    "                metric=\"cosine\",\n",
    "                metadata_config={\n",
    "                    \"indexed\": [\n",
    "                        \"source_type\",\n",
    "                        \"requirement_level\",\n",
    "                        \"job_id\",\n",
    "                        \"resume_id\"\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            print(f\"Index '{PINECONE_INDEX_NAME}' created successfully\")\n",
    "        \n",
    "        # Connect to the index\n",
    "        index = pc.Index(PINECONE_INDEX_NAME)\n",
    "        print(f\"Connected to Pinecone index: {PINECONE_INDEX_NAME}\")\n",
    "        \n",
    "        return index\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Pinecone: {str(e)}\")\n",
    "        print(\"Please check your API key and environment settings.\")\n",
    "        # Return None to indicate initialization failed\n",
    "        return None\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone_index = initialize_pinecone()\n",
    "\n",
    "if pinecone_index:\n",
    "    # Check index stats\n",
    "    index_stats = pinecone_index.describe_index_stats()\n",
    "    # Print directly to avoid serialization issues\n",
    "    print(f\"Index statistics:\")\n",
    "    print(index_stats)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Available Pinecone indexes: ['mirra-embeddings', 'mirra-filtering', 'mirra']\n",
      "Connected to Pinecone index: mirra-embeddings\n",
      "Index statistics:\n",
      "{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 1614}},\n",
      " 'total_vector_count': 1614,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class ResilientSageMakerEmbedder:\n",
    "    \"\"\"\n",
    "    A wrapper around SageMaker embedding endpoints with resilience features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint_name, max_text_length=512, region=\"us-east-1\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedder with a SageMaker endpoint.\n",
    "        \n",
    "        Args:\n",
    "            endpoint_name: The name of the SageMaker endpoint\n",
    "            max_text_length: Maximum text length to truncate to\n",
    "            region: AWS region for the endpoint\n",
    "        \"\"\"\n",
    "        import boto3\n",
    "        import json\n",
    "        import numpy as np\n",
    "        \n",
    "        self.sagemaker_runtime = boto3.client('sagemaker-runtime', region_name=region)\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.max_text_length = max_text_length\n",
    "        print(f\"Initialized ResilientSageMakerEmbedder for endpoint: {endpoint_name}\")\n",
    "\n",
    "    def _prepare_text(self, text):\n",
    "            \"\"\"Clean and prepare text for the embedding model.\"\"\"\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)\n",
    "            \n",
    "            # Remove excessive whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "            # Add E5 prefix\n",
    "            if not text.startswith(\"passage:\"):\n",
    "                text = f\"passage: {text}\"\n",
    "            \n",
    "            # Truncate if needed\n",
    "            if len(text) > self.max_text_length:\n",
    "                text = text[:self.max_text_length]\n",
    "                \n",
    "            return text\n",
    "    def generate_embeddings(self, texts):\n",
    "        \"\"\"Generate embeddings using SageMaker E5 endpoint\"\"\"\n",
    "        # Ensure texts is a list\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "            \n",
    "        try:\n",
    "            # Prepare input for E5 model\n",
    "            prepared_texts = [self._prepare_text(text) for text in texts]\n",
    "            \n",
    "            # Prepare payload with explicit pooling parameters\n",
    "            payload = {\n",
    "                \"inputs\": prepared_texts,\n",
    "                \"parameters\": {\n",
    "                    \"normalize\": True,\n",
    "                    \"pooling\": \"mean\",\n",
    "                    \"return_sentence_embedding\": True\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Call SageMaker endpoint\n",
    "            response = self.sagemaker_runtime.invoke_endpoint(\n",
    "                EndpointName=self.endpoint_name,\n",
    "                ContentType='application/json',\n",
    "                Body=json.dumps(payload)\n",
    "            )\n",
    "            \n",
    "            # Parse response\n",
    "            response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
    "            \n",
    "            # Process embeddings with proper pooling\n",
    "            embeddings = []\n",
    "            for emb in response_body:\n",
    "                emb_array = np.array(emb)\n",
    "                \n",
    "                # Handle token-level embeddings by taking mean across tokens\n",
    "                if len(emb_array.shape) > 1:\n",
    "                    # Average across all but the last dimension\n",
    "                    while len(emb_array.shape) > 1:\n",
    "                        emb_array = np.mean(emb_array, axis=0)\n",
    "                \n",
    "                # Ensure we have the right dimension (1024)\n",
    "                if emb_array.shape[0] != EMBEDDING_DIMENSION:\n",
    "                    if emb_array.shape[0] > EMBEDDING_DIMENSION:\n",
    "                        emb_array = emb_array[:EMBEDDING_DIMENSION]\n",
    "                    else:\n",
    "                        padded = np.zeros(EMBEDDING_DIMENSION)\n",
    "                        padded[:emb_array.shape[0]] = emb_array\n",
    "                        emb_array = padded\n",
    "                \n",
    "                embeddings.append(emb_array.tolist())\n",
    "            \n",
    "            return embeddings\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {str(e)}\")\n",
    "            return [self._create_random_unit_vector() for _ in range(len(texts))]\n",
    "    \n",
    "    def _create_random_unit_vector(self, dim=1024):\n",
    "        \"\"\"Create a random unit vector for fallback\"\"\"\n",
    "        import numpy as np\n",
    "        vec = np.random.normal(0, 1, size=dim)\n",
    "        return (vec / np.linalg.norm(vec)).tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def process_jobs_and_upload(jobs, pinecone_index, batch_size=4):\n",
    "    \"\"\"\n",
    "    Process jobs, generate embeddings, and upload to Pinecone\n",
    "    \n",
    "    Args:\n",
    "        jobs: List of job dictionaries\n",
    "        pinecone_index: Pinecone index instance\n",
    "        batch_size: Batch size for processing\n",
    "        \n",
    "    Returns:\n",
    "        Statistics about the processing\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    if not jobs:\n",
    "        print(\"Error: No job descriptions provided\")\n",
    "        return {\n",
    "            \"total_jobs\": 0,\n",
    "            \"total_strings\": 0,\n",
    "            \"strings_embedded\": 0,\n",
    "            \"vectors_uploaded\": 0,\n",
    "            \"processing_time_seconds\": 0\n",
    "        }\n",
    "    \n",
    "    if not pinecone_index:\n",
    "        print(\"Error: No Pinecone index provided\")\n",
    "        return {\n",
    "            \"total_jobs\": len(jobs),\n",
    "            \"total_strings\": 0,\n",
    "            \"strings_embedded\": 0,\n",
    "            \"vectors_uploaded\": 0,\n",
    "            \"processing_time_seconds\": 0\n",
    "        }\n",
    "    \n",
    "    print(f\"Processing {len(jobs)} job descriptions...\")\n",
    "    \n",
    "    # Initialize the embedder\n",
    "    embedder = ResilientSageMakerEmbedder(\n",
    "        endpoint_name=SAGEMAKER_ENDPOINT_NAME,\n",
    "        region=AWS_REGION\n",
    "    )\n",
    "    \n",
    "    # Process all jobs and compute embeddings\n",
    "    start_time = time.time()\n",
    "    print(f\"Extracting strings from {len(jobs)} jobs...\")\n",
    "    \n",
    "    # Extract all strings\n",
    "    string_to_embedding = {}\n",
    "    job_strings = {}\n",
    "    \n",
    "    for i, job in enumerate(jobs):\n",
    "        job_id = job.get(\"job_id\", \"unknown\")\n",
    "        try:\n",
    "            strings = extract_all_strings_from_job(job)\n",
    "            \n",
    "            if strings:\n",
    "                job_strings[job_id] = strings\n",
    "                print(f\"Extracted {len(strings)} strings from job {job_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting strings from job {job_id}: {str(e)}\")\n",
    "    \n",
    "    # Gather all unique strings\n",
    "    all_strings = set()\n",
    "    for strings in job_strings.values():\n",
    "        all_strings.update(strings)\n",
    "    \n",
    "    all_strings_list = list(all_strings)\n",
    "    print(f\"Total unique strings: {len(all_strings_list)}\")\n",
    "    \n",
    "    # Process strings in batches\n",
    "    for i in range(0, len(all_strings_list), batch_size):\n",
    "        batch = all_strings_list[i:i+batch_size]\n",
    "        try:\n",
    "            print(f\"Processing batch {i//batch_size + 1}/{len(all_strings_list)//batch_size + 1}...\")\n",
    "            batch_embeddings = embedder.generate_embeddings(batch)\n",
    "            \n",
    "            # Store embeddings\n",
    "            for string, embedding in zip(batch, batch_embeddings):\n",
    "                string_to_embedding[string] = embedding\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "    \n",
    "    # Upload to Pinecone\n",
    "    vectors_uploaded = 0\n",
    "    if string_to_embedding and pinecone_index:\n",
    "        vectors_to_upload = []\n",
    "        \n",
    "        for job_id, strings in job_strings.items():\n",
    "            for i, string_text in enumerate(strings):\n",
    "                if string_text in string_to_embedding:\n",
    "                    vector_id = f\"{job_id}_string_{i}\"\n",
    "                    \n",
    "                    # Simple metadata\n",
    "                    metadata = {\n",
    "                        \"text\": string_text,\n",
    "                        \"job_id\": job_id,\n",
    "                        \"source_type\": \"job_description\"\n",
    "                    }\n",
    "                    \n",
    "                    vectors_to_upload.append({\n",
    "                        \"id\": vector_id,\n",
    "                        \"values\": string_to_embedding[string_text],\n",
    "                        \"metadata\": metadata\n",
    "                    })\n",
    "        \n",
    "        # Upload in batches\n",
    "        for i in range(0, len(vectors_to_upload), batch_size):\n",
    "            batch = vectors_to_upload[i:i+batch_size]\n",
    "            try:\n",
    "                pinecone_index.upsert(vectors=batch)\n",
    "                vectors_uploaded += len(batch)\n",
    "                print(f\"Uploaded batch {i//batch_size + 1}/{len(vectors_to_upload)//batch_size + 1}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading batch to Pinecone: {str(e)}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Return statistics\n",
    "    return {\n",
    "        \"total_jobs\": len(jobs),\n",
    "        \"total_strings\": len(all_strings_list),\n",
    "        \"strings_embedded\": len(string_to_embedding),\n",
    "        \"vectors_uploaded\": vectors_uploaded,\n",
    "        \"processing_time_seconds\": total_time\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def run_embedding_pipeline():\n",
    "    \"\"\"\n",
    "    Complete pipeline to load jobs, generate embeddings, and upload to Pinecone\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Verify configuration\n",
    "    if not PINECONE_API_KEY:\n",
    "        print(\"ERROR: Pinecone API key is not set. Please set PINECONE_API_KEY in the configuration cell.\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Configuring embedding pipeline...\")\n",
    "    print(f\"Embedding dimension: {EMBEDDING_DIMENSION}\")\n",
    "    print(f\"Using SageMaker endpoint: {SAGEMAKER_ENDPOINT_NAME}\")\n",
    "    print(f\"Using Pinecone index: {PINECONE_INDEX_NAME}\")\n",
    "    \n",
    "    # Initialize Pinecone if it's not already initialized\n",
    "    if 'pinecone_index' not in globals() or pinecone_index is None:\n",
    "        print(\"Initializing Pinecone...\")\n",
    "        pinecone_index = initialize_pinecone()\n",
    "        if not pinecone_index:\n",
    "            print(\"ERROR: Failed to initialize Pinecone. Check your API key and index settings.\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"Using existing Pinecone connection\")\n",
    "    \n",
    "    # Display index stats\n",
    "    try:\n",
    "        index_stats = pinecone_index.describe_index_stats()\n",
    "        print(f\"Current index statistics:\")\n",
    "        print(f\"  - Total vectors: {index_stats.get('total_vector_count', 0)}\")\n",
    "        print(f\"  - Dimension: {index_stats.get('dimension', 0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not retrieve index stats: {str(e)}\")\n",
    "    \n",
    "    # Load jobs - use existing sample_jobs if available\n",
    "    if 'sample_jobs' in globals() and sample_jobs:\n",
    "        print(f\"Using {len(sample_jobs)} previously loaded jobs\")\n",
    "        jobs = sample_jobs\n",
    "    else:\n",
    "        print(\"Loading new job samples...\")\n",
    "        jobs = load_job_descriptions(limit=25)\n",
    "        if not jobs:\n",
    "            print(\"ERROR: No jobs loaded. Cannot proceed.\")\n",
    "            return False\n",
    "    \n",
    "    # Process jobs and upload\n",
    "    start_time = time.time()\n",
    "    stats = process_jobs_and_upload(jobs, pinecone_index, batch_size=4)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Print stats\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Processed {stats['total_jobs']} jobs\")\n",
    "    print(f\"Total strings extracted: {stats['total_strings']}\")\n",
    "    print(f\"Strings embedded: {stats['strings_embedded']}\")\n",
    "    print(f\"Vectors uploaded to Pinecone: {stats['vectors_uploaded']}\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Verify final index state\n",
    "    try:\n",
    "        final_stats = pinecone_index.describe_index_stats()\n",
    "        print(f\"Final index statistics:\")\n",
    "        print(f\"  - Total vectors: {final_stats.get('total_vector_count', 0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not retrieve final index stats: {str(e)}\")\n",
    "    \n",
    "    return stats"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def load_job_descriptions(limit=100, bucket=\"mirra-matcher-32\", prefix=\"data/processed/jobs/jd2-\"):\n",
    "    \"\"\"\n",
    "    Load job descriptions from S3.\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of jobs to load\n",
    "        bucket: S3 bucket name\n",
    "        prefix: S3 prefix for job files\n",
    "        \n",
    "    Returns:\n",
    "        List of job description dictionaries\n",
    "    \"\"\"\n",
    "    jobs = []\n",
    "    \n",
    "    # Load from S3\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading job descriptions from S3: s3://{bucket}/{prefix}\")\n",
    "        response = s3_client.list_objects_v2(\n",
    "            Bucket=bucket,\n",
    "            Prefix=prefix,\n",
    "            MaxKeys=limit\n",
    "        )\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith(\".json\"):\n",
    "                    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "                    job_data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "                    # Add job_id if not present\n",
    "                    if \"job_id\" not in job_data:\n",
    "                        job_data[\"job_id\"] = key.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "                    jobs.append(job_data)\n",
    "                    print(f\"Loaded job: {job_data.get('job_id')}\")\n",
    "        else:\n",
    "            print(f\"No job files found in S3 bucket {bucket}/{prefix}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading files from S3: {str(e)}\")\n",
    "\n",
    "    print(f\"Loaded {len(jobs)} job descriptions from S3\")\n",
    "    return jobs"
   ],
   "outputs": [],
   "metadata": {
    "id": "J6431PKCWaUC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "sample_jobs = load_job_descriptions(limit=100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading job descriptions from S3: s3://mirra-matcher-32/data/processed/jobs/jd2-\n",
      "Loaded job: 41e06724-0b8d-41f8-8c6e-40b3cf68f03e\n",
      "Loaded job: ba800cce-6e7e-46c4-870e-ab6bf5a29598\n",
      "Loaded 2 job descriptions from S3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def safe_preview_job(job):\n",
    "    \"\"\"\n",
    "    Safely preview a job description with fallback values.\n",
    "    \n",
    "    Args:\n",
    "        job (dict): Job description dictionary\n",
    "    \n",
    "    Returns:\n",
    "        dict: Preview of job with safe access to values\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"job_id\": job.get(\"job_id\", \"Unknown\"),\n",
    "        \"job_title\": (\n",
    "            job.get(\"details\", {}).get(\"job_title\", [\"Unknown\"])[0] \n",
    "            if job.get(\"details\", {}).get(\"job_title\") \n",
    "            else \"Unknown\"\n",
    "        ),\n",
    "        \"company_name\": (\n",
    "            job.get(\"details\", {}).get(\"company_name\", [\"Unknown\"])[0] \n",
    "            if job.get(\"details\", {}).get(\"company_name\") \n",
    "            else \"Unknown\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "if sample_jobs:\n",
    "    try:\n",
    "        job_preview = safe_preview_job(sample_jobs[0])\n",
    "        print(\"\\nPreview of first job:\")\n",
    "        print(json.dumps(job_preview, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Error previewing job: {e}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Preview of first job:\n",
      "{\n",
      "  \"job_id\": \"41e06724-0b8d-41f8-8c6e-40b3cf68f03e\",\n",
      "  \"job_title\": \"SAP Master Data Governance Consultant\",\n",
      "  \"company_name\": \"Unknown\"\n",
      "}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''# Display a preview of the first job\n",
    "if sample_jobs:\n",
    "    print(\"\\nPreview of first job:\")\n",
    "    job_preview = {\n",
    "        \"job_id\": sample_jobs[0].get(\"job_id\"),\n",
    "        \"job_title\": sample_jobs[0].get(\"details\", {}).get(\"job_title\", [\"Unknown\"])[0],\n",
    "        \"company_name\": sample_jobs[0].get(\"details\", {}).get(\"company_name\", [\"Unknown\"])[0]\n",
    "    }\n",
    "    print(json.dumps(job_preview, indent=2))'''"
   ],
   "outputs": [],
   "metadata": {
    "id": "z5EZaDp4Wdxa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def extract_all_strings_from_job(job_data):\n",
    "    \"\"\"\n",
    "    Extract all relevant strings from a job description for embedding.\n",
    "    Skips the 'details' section entirely and extracts only raw strings from other sections\n",
    "    without additional metadata.\n",
    "    \n",
    "    Args:\n",
    "        job_data: Job description dictionary\n",
    "        \n",
    "    Returns:\n",
    "        List of strings to be embedded\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    # Skip 'details' section entirely\n",
    "    \n",
    "    # Process remaining sections: mandatory, preferred, responsibility\n",
    "    sections_to_process = ['mandatory', 'preferred', 'responsibility']\n",
    "    \n",
    "    for section in sections_to_process:\n",
    "        if section in job_data:\n",
    "            section_data = job_data[section]\n",
    "            # Process each subsection within this section\n",
    "            for subsection, content in section_data.items():\n",
    "                if isinstance(content, list):\n",
    "                    for item in content:\n",
    "                        # Extract only the skill strings without metadata\n",
    "                        if isinstance(item, dict):\n",
    "                            for key, value in item.items():\n",
    "                                if isinstance(value, list):\n",
    "                                    for string_item in value:\n",
    "                                        if isinstance(string_item, str) and string_item.strip():\n",
    "                                            result.append(string_item.strip())\n",
    "                                elif isinstance(value, str) and value.strip():\n",
    "                                    result.append(value.strip())\n",
    "    \n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def ensure_correct_vector_format(vector_data):\n",
    "    \"\"\"\n",
    "    Ensures that the vector values are in the correct format for Pinecone:\n",
    "    - Must be a flat list of float values, not nested lists\n",
    "    \n",
    "    Args:\n",
    "        vector_data: The vector data to check/fix\n",
    "        \n",
    "    Returns:\n",
    "        Properly formatted vector data\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Check if the vector values are nested lists\n",
    "    if isinstance(vector_data, list) and vector_data and isinstance(vector_data[0], list):\n",
    "        # It's a nested list - flatten it to a 1D array\n",
    "        print(\"WARNING: Found nested list in vector data, flattening to 1D array\")\n",
    "        flat_array = np.array(vector_data).flatten()\n",
    "        return flat_array.tolist()\n",
    "    \n",
    "    # Ensure it's a list of floats\n",
    "    if isinstance(vector_data, np.ndarray):\n",
    "        return vector_data.tolist()\n",
    "    \n",
    "    return vector_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def process_and_upload_job_embeddings(jobs, pinecone_index, batch_size=4, max_retries=3):\n",
    "    \"\"\"\n",
    "    Function to process jobs, generate embeddings, and upload to Pinecone\n",
    "    \n",
    "    Args:\n",
    "        jobs: List of job description dictionaries\n",
    "        pinecone_index: Pinecone index instance\n",
    "        batch_size: Batch size for processing and uploading\n",
    "        max_retries: Maximum number of retries for failed operations\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with processing statistics\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import random\n",
    "    import uuid\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Validate inputs\n",
    "    if not jobs:\n",
    "        print(\"Error: No job descriptions provided\")\n",
    "        return {\"status\": \"error\", \"reason\": \"no_jobs\"}\n",
    "        \n",
    "    if not pinecone_index:\n",
    "        print(\"Error: No Pinecone index provided\")\n",
    "        return {\"status\": \"error\", \"reason\": \"no_index\"}\n",
    "    \n",
    "    # Initialize metrics\n",
    "    start_time = time.time()\n",
    "    all_strings = set()\n",
    "    job_strings = {}\n",
    "    string_to_embedding = {}\n",
    "    vectors_uploaded = 0\n",
    "    \n",
    "    # Initialize the embedder\n",
    "    embedder = ResilientSageMakerEmbedder(\n",
    "        endpoint_name=SAGEMAKER_ENDPOINT_NAME,\n",
    "        region=AWS_REGION\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing {len(jobs)} job descriptions...\")\n",
    "    \n",
    "    # Phase 1: Extract all strings from jobs\n",
    "    for job_id, job in [(job.get(\"job_id\", str(uuid.uuid4())), job) for job in jobs]:\n",
    "        try:\n",
    "            strings = extract_all_strings_from_job(job)\n",
    "            if strings:\n",
    "                job_strings[job_id] = strings\n",
    "                all_strings.update(strings)\n",
    "                print(f\"Extracted {len(strings)} strings from job {job_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting strings from job {job_id}: {str(e)}\")\n",
    "    \n",
    "    all_strings_list = list(all_strings)\n",
    "    print(f\"Total unique strings: {len(all_strings_list)}\")\n",
    "    \n",
    "    # Phase 2: Generate embeddings for all strings\n",
    "    if all_strings_list:\n",
    "        # Shuffle to avoid batching similar strings together\n",
    "        random.shuffle(all_strings_list)\n",
    "        \n",
    "        # Process in batches with retries\n",
    "        for i in range(0, len(all_strings_list), batch_size):\n",
    "            batch = all_strings_list[i:i+batch_size]\n",
    "            batch_num = i//batch_size + 1\n",
    "            total_batches = (len(all_strings_list) - 1) // batch_size + 1\n",
    "            \n",
    "            # Progress reporting\n",
    "            print(f\"Processing batch {batch_num}/{total_batches} \"\n",
    "                  f\"({batch_num/total_batches*100:.1f}%)...\")\n",
    "            \n",
    "            # Retry logic\n",
    "            for retry in range(max_retries + 1):\n",
    "                try:\n",
    "                    # Generate embeddings\n",
    "                    batch_embeddings = embedder.generate_embeddings(batch)\n",
    "                    \n",
    "                    # Store embeddings\n",
    "                    for text, embedding in zip(batch, batch_embeddings):\n",
    "                        string_to_embedding[text] = embedding\n",
    "                    \n",
    "                    break  # Success, exit retry loop\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if retry < max_retries:\n",
    "                        sleep_time = 2 ** retry\n",
    "                        print(f\"Error in batch {batch_num}, retrying in {sleep_time}s: {str(e)}\")\n",
    "                        time.sleep(sleep_time)\n",
    "                    else:\n",
    "                        print(f\"Failed after {max_retries} retries for batch {batch_num}: {str(e)}\")\n",
    "    \n",
    "    # Phase 3: Prepare and upload vectors\n",
    "    if string_to_embedding:\n",
    "        vectors_to_upload = []\n",
    "        \n",
    "        # Prepare vectors\n",
    "        for job_id, strings in job_strings.items():\n",
    "            for i, text in enumerate(strings):\n",
    "                if text in string_to_embedding:\n",
    "                    # Create vector with proper format\n",
    "                    vector_values = ensure_correct_vector_format(string_to_embedding[text])\n",
    "                    \n",
    "                    # Create metadata\n",
    "                    metadata = {\n",
    "                        \"text\": text[:1000] if len(text) > 1000 else text,\n",
    "                        \"job_id\": job_id,\n",
    "                        \"source_type\": \"job_description\"\n",
    "                    }\n",
    "                    \n",
    "                    # Add to upload list\n",
    "                    vectors_to_upload.append({\n",
    "                        \"id\": f\"{job_id}_string_{i}\",\n",
    "                        \"values\": vector_values,\n",
    "                        \"metadata\": metadata\n",
    "                    })\n",
    "        \n",
    "        print(f\"Prepared {len(vectors_to_upload)} vectors for upload\")\n",
    "        \n",
    "        # Test a single vector first\n",
    "        if vectors_to_upload:\n",
    "            try:\n",
    "                test_vector = vectors_to_upload[0].copy()\n",
    "                print(f\"Testing upload with vector ID: {test_vector['id']} \"\n",
    "                      f\"(dimension: {len(test_vector['values'])})\")\n",
    "                pinecone_index.upsert(vectors=[test_vector])\n",
    "                print(\"Single vector test successful\")\n",
    "            except Exception as e:\n",
    "                print(f\"Single vector test failed: {str(e)}\")\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"reason\": \"upload_test_failed\",\n",
    "                    \"strings_extracted\": len(all_strings_list),\n",
    "                    \"strings_embedded\": len(string_to_embedding),\n",
    "                    \"vectors_prepared\": len(vectors_to_upload),\n",
    "                    \"vectors_uploaded\": 0,\n",
    "                    \"processing_time\": time.time() - start_time\n",
    "                }\n",
    "        \n",
    "        # Upload in batches with progress bar\n",
    "        with tqdm(total=len(vectors_to_upload), desc=\"Uploading to Pinecone\") as pbar:\n",
    "            for i in range(0, len(vectors_to_upload), batch_size):\n",
    "                batch = vectors_to_upload[i:i+batch_size]\n",
    "                \n",
    "                # Try upload with retries\n",
    "                for retry in range(max_retries + 1):\n",
    "                    try:\n",
    "                        # Final check and correction of vectors\n",
    "                        for vector in batch:\n",
    "                            if isinstance(vector[\"values\"], list) and vector[\"values\"] and isinstance(vector[\"values\"][0], list):\n",
    "                                vector[\"values\"] = np.array(vector[\"values\"]).flatten().tolist()\n",
    "                        \n",
    "                        # Upload batch\n",
    "                        pinecone_index.upsert(vectors=batch)\n",
    "                        vectors_uploaded += len(batch)\n",
    "                        pbar.update(len(batch))\n",
    "                        break  # Success, exit retry loop\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        if retry < max_retries:\n",
    "                            sleep_time = 2 ** retry\n",
    "                            print(f\"Upload error, retrying in {sleep_time}s: {str(e)}\")\n",
    "                            time.sleep(sleep_time)\n",
    "                        else:\n",
    "                            print(f\"Failed to upload batch after {max_retries} retries: {str(e)}\")\n",
    "    \n",
    "    # Return comprehensive statistics\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"total_jobs\": len(jobs),\n",
    "        \"total_strings\": len(all_strings_list),\n",
    "        \"strings_embedded\": len(string_to_embedding),\n",
    "        \"vectors_prepared\": len(vectors_to_upload) if 'vectors_to_upload' in locals() else 0,\n",
    "        \"vectors_uploaded\": vectors_uploaded,\n",
    "        \"processing_time\": total_time,\n",
    "        \"strings_per_job_avg\": sum(len(strings) for strings in job_strings.values()) / len(job_strings) if job_strings else 0\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load jobs\n",
    "jobs = load_job_descriptions(limit=51)\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone_index = initialize_pinecone()\n",
    "\n",
    "# Process and upload in one step\n",
    "stats = process_and_upload_job_embeddings(jobs, pinecone_index, batch_size=4)\n",
    "\n",
    "# Print results\n",
    "print(f\"Processing complete in {stats['processing_time']:.2f} seconds\")\n",
    "print(f\"Uploaded {stats['vectors_uploaded']} vectors to Pinecone\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def get_uploaded_job_ids(pinecone_index):\n",
    "    \"\"\"\n",
    "    Query Pinecone to get all job IDs that have been uploaded\n",
    "    \n",
    "    Args:\n",
    "        pinecone_index: Initialized Pinecone index instance\n",
    "        \n",
    "    Returns:\n",
    "        List of unique job IDs that exist in the index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get index stats first to see if there are any vectors\n",
    "        stats = pinecone_index.describe_index_stats()\n",
    "        total_vectors = stats.get('total_vector_count', 0)\n",
    "        \n",
    "        if total_vectors == 0:\n",
    "            print(\"No vectors found in the index\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {total_vectors} total vectors in the index\")\n",
    "        \n",
    "        # Query with empty vector to get metadata only\n",
    "        # Create a zero/random vector of the correct dimension\n",
    "        import numpy as np\n",
    "        query_vector = np.zeros(EMBEDDING_DIMENSION).tolist()\n",
    "        \n",
    "        # Set a high top_k to get as many results as possible\n",
    "        # Note: Pinecone may have limits on how many you can fetch at once\n",
    "        batch_size = 1000\n",
    "        total_fetched = 0\n",
    "        job_ids = set()\n",
    "        \n",
    "        # Use pagination to fetch all results if there are many\n",
    "        while total_fetched < total_vectors:\n",
    "            results = pinecone_index.query(\n",
    "                vector=query_vector,\n",
    "                top_k=batch_size,\n",
    "                include_metadata=True\n",
    "            )\n",
    "            \n",
    "            # Extract job_ids from metadata\n",
    "            for match in results.get('matches', []):\n",
    "                metadata = match.get('metadata', {})\n",
    "                if 'job_id' in metadata:\n",
    "                    job_ids.add(metadata['job_id'])\n",
    "            \n",
    "            total_fetched += len(results.get('matches', []))\n",
    "            \n",
    "            if len(results.get('matches', [])) < batch_size:\n",
    "                break\n",
    "                \n",
    "            print(f\"Fetched {total_fetched} vectors so far, found {len(job_ids)} unique job IDs\")\n",
    "        \n",
    "        return sorted(list(job_ids))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Pinecone: {str(e)}\")\n",
    "        return []\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def job_id_retrieval(pinecone_index):\n",
    "    job_ids = set()\n",
    "    stats = pinecone_index.describe_index_stats()\n",
    "    total_vectors = stats.get('total_vector_count', 0)\n",
    "    \n",
    "    # Try different metadata key variations\n",
    "    potential_keys = ['job_id', 'jobId', 'id', 'job_identifier']\n",
    "    \n",
    "    for namespace, ns_stats in stats.get('namespaces', {}).items():\n",
    "        print(f\"Checking namespace: {namespace}\")\n",
    "        try:\n",
    "            # Fetch vectors in this namespace\n",
    "            for key in potential_keys:\n",
    "                namespace_vectors = pinecone_index.query(\n",
    "                    vector=[0]*EMBEDDING_DIMENSION, \n",
    "                    top_k=total_vectors, \n",
    "                    namespace=namespace,\n",
    "                    filter={key: {'$exists': True}},\n",
    "                    include_metadata=True\n",
    "                )\n",
    "                \n",
    "                for match in namespace_vectors.get('matches', []):\n",
    "                    metadata = match.get('metadata', {})\n",
    "                    for potential_key in potential_keys:\n",
    "                        if potential_key in metadata:\n",
    "                            job_ids.add(metadata[potential_key])\n",
    "        except Exception as e:\n",
    "            print(f\"Error in namespace {namespace}: {e}\")\n",
    "    \n",
    "    return sorted(list(job_ids))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Execute and print results\n",
    "if pinecone_index:\n",
    "    print(\"Querying Pinecone for uploaded job IDs...\")\n",
    "    job_ids = job_id_retrieval(pinecone_index)\n",
    "    \n",
    "    if job_ids:\n",
    "        print(f\"\\nFound {len(job_ids)} unique job IDs in Pinecone:\")\n",
    "        for i, job_id in enumerate(job_ids):\n",
    "            print(f\"{i+1}. {job_id}\")\n",
    "    else:\n",
    "        print(\"No job IDs found in Pinecone\")\n",
    "else:\n",
    "    print(\"Pinecone index not initialized\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}