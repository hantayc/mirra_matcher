{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#03_embedding_generation\n",
    "- Sets up the embedding generator using SageMaker\n",
    "- Loads job descriptions from files\n",
    "- Creates chunks from job descriptions using a custom chunker\n",
    "- Generates embeddings for each chunk\n",
    "- Uploads the embedded vectors to Pinecone\n",
    "- Includes batch processing for efficiency"
   ],
   "metadata": {
    "id": "dynvT9cEdx4s"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install pinecone"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip show pinecone"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Any, Union \n",
    "from tqdm import tqdm\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "Eh1Rdi4NVCIP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# SageMaker endpoint configuration\n",
    "\n",
    "SAGEMAKER_ENDPOINT_NAME = \"e5-embeddings-pooled-2\" \n",
    "AWS_REGION = \"us-east-1\"\n",
    "\n",
    "# Set Pinecone credentials directly\n",
    "PINECONE_API_KEY = \"\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\" # matched with AWS region\n",
    "PINECONE_INDEX_NAME = \"sample-100-strings\"\n",
    "\n",
    "EMBEDDING_DIMENSION = 1024\n",
    "\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIMENSION}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Embedding dimension: 1024\n"
     ]
    }
   ],
   "metadata": {
    "id": "7rXs4HpmWK-w"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Initialize Pinecone with credentials\n",
    "from pinecone import Pinecone\n",
    "\n",
    "def initialize_pinecone():\n",
    "    \"\"\"Initialize Pinecone and return the index\"\"\"\n",
    "    try:\n",
    "        # Initialize Pinecone client\n",
    "        pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        \n",
    "        # Check if the index exists\n",
    "        existing_indexes = pc.list_indexes().names()\n",
    "        print(f\"Available Pinecone indexes: {existing_indexes}\")\n",
    "        \n",
    "        if PINECONE_INDEX_NAME not in existing_indexes:\n",
    "            print(f\"Creating new index '{PINECONE_INDEX_NAME}'...\")\n",
    "            \n",
    "            # Create the index\n",
    "            pc.create_index(\n",
    "                name=PINECONE_INDEX_NAME,\n",
    "                dimension=EMBEDDING_DIMENSION,\n",
    "                metric=\"cosine\",\n",
    "                metadata_config={\n",
    "                    \"indexed\": [\n",
    "                        \"source_type\",\n",
    "                        \"requirement_level\",\n",
    "                        \"job_id\",\n",
    "                        \"resume_id\"\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            print(f\"Index '{PINECONE_INDEX_NAME}' created successfully\")\n",
    "        \n",
    "        # Connect to the index\n",
    "        index = pc.Index(PINECONE_INDEX_NAME)\n",
    "        print(f\"Connected to Pinecone index: {PINECONE_INDEX_NAME}\")\n",
    "        \n",
    "        return index\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Pinecone: {str(e)}\")\n",
    "        print(\"Please check your API key and environment settings.\")\n",
    "        # Return None to indicate initialization failed\n",
    "        return None\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone_index = initialize_pinecone()\n",
    "\n",
    "if pinecone_index:\n",
    "    # Check index stats\n",
    "    index_stats = pinecone_index.describe_index_stats()\n",
    "    # Print directly to avoid serialization issues\n",
    "    print(f\"Index statistics:\")\n",
    "    print(index_stats)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Available Pinecone indexes: ['mirra-embeddings', 'sample-100-strings', 'sample-100', 'mirra-filtering', 'mirra']\n",
      "Connected to Pinecone index: sample-100-strings\n",
      "Index statistics:\n",
      "{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class ResilientSageMakerEmbedder:\n",
    "    \"\"\"\n",
    "    A wrapper around SageMaker embedding endpoints with resilience features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint_name, max_text_length=512, region=\"us-east-1\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedder with a SageMaker endpoint.\n",
    "        \n",
    "        Args:\n",
    "            endpoint_name: The name of the SageMaker endpoint\n",
    "            max_text_length: Maximum text length to truncate to\n",
    "            region: AWS region for the endpoint\n",
    "        \"\"\"\n",
    "        import boto3\n",
    "        import json\n",
    "        import numpy as np\n",
    "        \n",
    "        self.sagemaker_runtime = boto3.client('sagemaker-runtime', region_name=region)\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.max_text_length = max_text_length\n",
    "        print(f\"Initialized ResilientSageMakerEmbedder for endpoint: {endpoint_name}\")\n",
    "\n",
    "    def _prepare_text(self, text):\n",
    "            \"\"\"Clean and prepare text for the embedding model.\"\"\"\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)\n",
    "            \n",
    "            # Remove excessive whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "            # Add E5 prefix\n",
    "            if not text.startswith(\"passage:\"):\n",
    "                text = f\"passage: {text}\"\n",
    "            \n",
    "            # Truncate if needed\n",
    "            if len(text) > self.max_text_length:\n",
    "                text = text[:self.max_text_length]\n",
    "                \n",
    "            return text\n",
    "    def generate_embeddings(self, texts):\n",
    "        \"\"\"Generate embeddings using SageMaker E5 endpoint\"\"\"\n",
    "        # Ensure texts is a list\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "            \n",
    "        try:\n",
    "            # Prepare input for E5 model\n",
    "            prepared_texts = [self._prepare_text(text) for text in texts]\n",
    "            \n",
    "            # Prepare payload with explicit pooling parameters\n",
    "            payload = {\n",
    "                \"inputs\": prepared_texts,\n",
    "                \"parameters\": {\n",
    "                    \"normalize\": True,\n",
    "                    \"pooling\": \"mean\",\n",
    "                    \"return_sentence_embedding\": True\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = self.sagemaker_runtime.invoke_endpoint(\n",
    "                EndpointName=self.endpoint_name,\n",
    "                ContentType='application/json',\n",
    "                Body=json.dumps(payload)\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
    "            \n",
    "            embeddings = []\n",
    "            for emb in response_body:\n",
    "                emb_array = np.array(emb)\n",
    "                \n",
    "                if len(emb_array.shape) > 1:\n",
    "                    while len(emb_array.shape) > 1:\n",
    "                        emb_array = np.mean(emb_array, axis=0)\n",
    "                \n",
    "                if emb_array.shape[0] != EMBEDDING_DIMENSION:\n",
    "                    if emb_array.shape[0] > EMBEDDING_DIMENSION:\n",
    "                        emb_array = emb_array[:EMBEDDING_DIMENSION]\n",
    "                    else:\n",
    "                        padded = np.zeros(EMBEDDING_DIMENSION)\n",
    "                        padded[:emb_array.shape[0]] = emb_array\n",
    "                        emb_array = padded\n",
    "                \n",
    "                embeddings.append(emb_array.tolist())\n",
    "            \n",
    "            return embeddings\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {str(e)}\")\n",
    "            return [self._create_random_unit_vector() for _ in range(len(texts))]\n",
    "    \n",
    "    def _create_random_unit_vector(self, dim=1024):\n",
    "        \"\"\"Create a random unit vector for fallback\"\"\"\n",
    "        import numpy as np\n",
    "        vec = np.random.normal(0, 1, size=dim)\n",
    "        return (vec / np.linalg.norm(vec)).tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def run_embedding_pipeline():\n",
    "    \"\"\"\n",
    "    Complete pipeline to load jobs, generate embeddings, and upload to Pinecone\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Verify configuration\n",
    "    if not PINECONE_API_KEY:\n",
    "        print(\"ERROR: Pinecone API key is not set. Please set PINECONE_API_KEY in the configuration cell.\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Configuring embedding pipeline...\")\n",
    "    print(f\"Embedding dimension: {EMBEDDING_DIMENSION}\")\n",
    "    print(f\"Using SageMaker endpoint: {SAGEMAKER_ENDPOINT_NAME}\")\n",
    "    print(f\"Using Pinecone index: {PINECONE_INDEX_NAME}\")\n",
    "    \n",
    "    # Initialize Pinecone if it's not already initialized\n",
    "    if 'pinecone_index' not in globals() or pinecone_index is None:\n",
    "        print(\"Initializing Pinecone...\")\n",
    "        pinecone_index = initialize_pinecone()\n",
    "        if not pinecone_index:\n",
    "            print(\"ERROR: Failed to initialize Pinecone. Check your API key and index settings.\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"Using existing Pinecone connection\")\n",
    "    \n",
    "    # Display index stats\n",
    "    try:\n",
    "        index_stats = pinecone_index.describe_index_stats()\n",
    "        print(f\"Current index statistics:\")\n",
    "        print(f\"  - Total vectors: {index_stats.get('total_vector_count', 0)}\")\n",
    "        print(f\"  - Dimension: {index_stats.get('dimension', 0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not retrieve index stats: {str(e)}\")\n",
    "    \n",
    "    # Load jobs - use existing sample_jobs if available\n",
    "    if 'sample_jobs' in globals() and sample_jobs:\n",
    "        print(f\"Using {len(sample_jobs)} previously loaded jobs\")\n",
    "        jobs = sample_jobs\n",
    "    else:\n",
    "        print(\"Loading new job samples...\")\n",
    "        jobs = load_job_descriptions(limit=101)\n",
    "        if not jobs:\n",
    "            print(\"ERROR: No jobs loaded. Cannot proceed.\")\n",
    "            return False\n",
    "    \n",
    "    # Process jobs and upload\n",
    "    start_time = time.time()\n",
    "    stats = process_jobs_and_upload(jobs, pinecone_index, batch_size=4)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Print stats\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Processed {stats['total_jobs']} jobs\")\n",
    "    print(f\"Total strings extracted: {stats['total_strings']}\")\n",
    "    print(f\"Strings embedded: {stats['strings_embedded']}\")\n",
    "    print(f\"Vectors uploaded to Pinecone: {stats['vectors_uploaded']}\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Verify final index state\n",
    "    try:\n",
    "        final_stats = pinecone_index.describe_index_stats()\n",
    "        print(f\"Final index statistics:\")\n",
    "        print(f\"  - Total vectors: {final_stats.get('total_vector_count', 0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not retrieve final index stats: {str(e)}\")\n",
    "    \n",
    "    return stats"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def load_job_descriptions(limit=101, bucket=\"mirra-matcher-325\", prefix=\"data/processed/jobs/\"):\n",
    "    \"\"\"\n",
    "    Load job descriptions from S3.\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of jobs to load\n",
    "        bucket: S3 bucket name\n",
    "        prefix: S3 prefix for job files\n",
    "        \n",
    "    Returns:\n",
    "        List of job description dictionaries\n",
    "    \"\"\"\n",
    "    jobs = []\n",
    "    \n",
    "    # Load from S3\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading job descriptions from S3: s3://{bucket}/{prefix}\")\n",
    "        response = s3_client.list_objects_v2(\n",
    "            Bucket=bucket,\n",
    "            Prefix=prefix,\n",
    "            MaxKeys=limit\n",
    "        )\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith(\".json\"):\n",
    "                    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "                    job_data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "                    # Add job_id if not present\n",
    "                    if \"job_id\" not in job_data:\n",
    "                        job_data[\"job_id\"] = key.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "                    jobs.append(job_data)\n",
    "                    print(f\"Loaded job: {job_data.get('job_id')}\")\n",
    "        else:\n",
    "            print(f\"No job files found in S3 bucket {bucket}/{prefix}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading files from S3: {str(e)}\")\n",
    "\n",
    "    print(f\"Loaded {len(jobs)} job descriptions from S3\")\n",
    "    return jobs"
   ],
   "outputs": [],
   "metadata": {
    "id": "J6431PKCWaUC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "sample_jobs = load_job_descriptions(limit=101)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading job descriptions from S3: s3://mirra-matcher-325/data/processed/jobs/\n",
      "Loaded job: 06ba6c67-c4eb-4358-aa2a-64dc8d186198\n",
      "Loaded job: 08d80a05-d86e-4fbf-9203-502dfad0f0f2\n",
      "Loaded job: 0b3eccfd-15c1-4a4a-b9f5-87d0b26a6de7\n",
      "Loaded job: 0c9591b9-68aa-4d93-81a4-c090753a43d5\n",
      "Loaded job: 1154d66d-c922-48ea-bb60-59c719b3c77d\n",
      "Loaded job: 128354d9-d13b-413b-8c3c-5dc85a608cd7\n",
      "Loaded job: 137098ec-6b03-47cc-adcc-4f2d672a20f9\n",
      "Loaded job: 137510b3-5847-44c2-a16a-61558634845a\n",
      "Loaded job: 158df241-dc50-4aff-86de-114acd3bd2e5\n",
      "Loaded job: 181b9a3d-71ca-420e-8ea8-08b348ad69f2\n",
      "Loaded job: 18df64e2-17f0-45b0-8759-ba1b89fad7dd\n",
      "Loaded job: 195f1df8-b444-4118-ad8f-b2d8a9959c34\n",
      "Loaded job: 1a8b8afd-5bf1-43a5-abb8-b533e7ba6fdc\n",
      "Loaded job: 1aa0834d-5e19-4a9d-a0bf-e58d417ea413\n",
      "Loaded job: 23092c01-f933-4711-ac83-cbc310045763\n",
      "Loaded job: 2510ddf5-282f-4f1c-bc72-44e34c8f67d8\n",
      "Loaded job: 27b81e47-a357-4bc9-bf00-3a854b2c03f1\n",
      "Loaded job: 28b3cec4-9cc9-4817-b133-181c8958cc3a\n",
      "Loaded job: 29cd66f5-7ab3-44a3-8633-29dfc9557a08\n",
      "Loaded job: 2b2477c7-fafb-411f-b890-11cbb24d02e0\n",
      "Loaded job: 2bc7d30f-d8db-48e2-a76b-040ef73044a5\n",
      "Loaded job: 2ce38b1b-2ff5-44f3-b8af-45dbd5e8c4fc\n",
      "Loaded job: 2d321fc0-5661-4154-a156-b49b749b0d0f\n",
      "Loaded job: 2fd8af11-4583-4d06-822b-fad7c0c3bc9a\n",
      "Loaded job: 383d7b67-16d1-4ee6-bd40-bca2b17c835b\n",
      "Loaded job: 3aa02d0d-814b-4acc-ab15-06c897ebdec8\n",
      "Loaded job: 3d6abfd2-971f-4279-bd5c-e2608afb27b8\n",
      "Loaded job: 3e3466a5-1799-4648-883d-ab43f1278a53\n",
      "Loaded job: 4016ff28-f2a9-4e9d-8ce8-4498679286a2\n",
      "Loaded job: 42fa4a08-2737-4d2a-aa7a-5e17dfa822f8\n",
      "Loaded job: 44221648-dbaf-4b42-af68-0703ebb16835\n",
      "Loaded job: 4488a328-4cc5-4080-b231-b381cbd7a9a8\n",
      "Loaded job: 4d6ce93b-5350-49ee-a1d8-1e2025567418\n",
      "Loaded job: 53acdd0a-467a-413e-9486-42a2b86b5c66\n",
      "Loaded job: 560b033b-5ba3-43d7-a9f3-b62d81c290eb\n",
      "Loaded job: 5cdf2c5a-05fa-40e7-b85b-cefccd795706\n",
      "Loaded job: 5e75d9a0-228b-463d-9025-f1b3ad048cf2\n",
      "Loaded job: 5f80e584-49e8-4956-9865-2924fe1c288f\n",
      "Loaded job: 60111acc-73d9-4fe8-8a84-c826d9eeeca4\n",
      "Loaded job: 61d0b702-10e7-44b0-a077-ad555e7c2091\n",
      "Loaded job: 639fb093-26a8-4745-9656-ee93f3b79f19\n",
      "Loaded job: 63b55b4d-1162-4151-8878-d55c1262156a\n",
      "Loaded job: 63f3d671-571d-4fcb-aa06-2200ea5d6313\n",
      "Loaded job: 65ad643c-f389-494e-aaba-210e41917b8d\n",
      "Loaded job: 67cca7d7-0e75-49cf-9f27-4d6ec2bd7766\n",
      "Loaded job: 71bb20b6-57a4-40e3-b3bd-13b052ef904b\n",
      "Loaded job: 74cf841a-faa9-4fc6-88ee-63f6d1313a22\n",
      "Loaded job: 750c7271-ac87-4b39-a381-59a1f2bb07fc\n",
      "Loaded job: 75a94499-c585-4181-b4de-09f1355a18a2\n",
      "Loaded job: 77773fdc-9c4e-4bdf-8e4a-2b82b08061b0\n",
      "Loaded job: 7837d31d-8065-4c42-8852-5a6cdaf6335e\n",
      "Loaded job: 78802d2b-8437-4bbf-83ca-62f7166f3954\n",
      "Loaded job: 794dda95-c86e-4575-90dd-6d5fe9b34268\n",
      "Loaded job: 7db57868-4db2-4be9-94fe-67574010017f\n",
      "Loaded job: 7e8c1297-5fb7-4431-bfd0-ff76a071a24b\n",
      "Loaded job: 7f2250c2-f4d6-41c5-8e44-0e125e726238\n",
      "Loaded job: 7ffce82e-823b-45ef-8a98-edbc5ea065d8\n",
      "Loaded job: 84b4a4d4-4fdb-45e8-bd40-c5679947f8b0\n",
      "Loaded job: 84eae8a7-b4d0-4c5a-8137-2de7de1cbdb3\n",
      "Loaded job: 8aa2b635-edbf-4901-bb3f-4d4f2c69e7fe\n",
      "Loaded job: 8bd1bc07-2777-4c8e-89f8-7895e81b0abe\n",
      "Loaded job: 90822d5e-c54f-49ad-a345-17a00f8b0c56\n",
      "Loaded job: 96051542-318e-4fd1-aea9-035cd9a74fe5\n",
      "Loaded job: 9ccc15c2-060b-4fa5-8b88-4d8c2b65c251\n",
      "Loaded job: 9ff8b57c-9f01-448b-9ba9-d7d03b8db9dd\n",
      "Loaded job: a19af6ea-09ff-4d3e-874e-88fe4ad2b8db\n",
      "Loaded job: a25c98ff-be8f-4ea4-b740-e459455d16b8\n",
      "Loaded job: a45a153b-9da6-4c44-9390-b46a38181eca\n",
      "Loaded job: a4faa7e9-74f9-4c68-9221-ff49cbfd22d2\n",
      "Loaded job: a7df6bed-41c2-4ccd-a3a1-d0bfb90adf5e\n",
      "Loaded job: abcf05ac-0424-4858-8f1b-d5e6678a2889\n",
      "Loaded job: abddf6a9-cfcd-4a98-adf2-d8970d128e76\n",
      "Loaded job: ac01e246-fa9b-4e5f-a7ed-fdd9cd1995b3\n",
      "Loaded job: ac212808-481d-4eba-a913-e04f86e660db\n",
      "Loaded job: ae020386-078f-4192-8ef4-089eaeb40125\n",
      "Loaded job: b4e53087-18bf-44fc-bc38-4e68dae31a5c\n",
      "Loaded job: b7a5df94-215b-465a-af5b-0a1c1a40d38e\n",
      "Loaded job: bb34a8a9-3de9-4707-ad05-9283e6bc44d2\n",
      "Loaded job: bb464343-964a-4eb5-851d-a7cc5753625e\n",
      "Loaded job: be46fbc9-8e00-4381-a545-9ed71b079a3a\n",
      "Loaded job: bf9aa16e-181d-46fd-9b34-999b21221885\n",
      "Loaded job: c0ac98d7-dd65-4ab5-8d37-dfa26300f333\n",
      "Loaded job: c405dd8e-b2c5-436f-be66-bbf602c46f76\n",
      "Loaded job: c50cd8cd-60d1-48a7-8db8-7e828210c020\n",
      "Loaded job: c540dda2-7f78-4584-9a93-0d4c5ee2981b\n",
      "Loaded job: cbbc9bbc-2f21-451f-ac20-01e4f2c611e1\n",
      "Loaded job: d54b9f7b-d3ea-49b2-995c-c47d1a78576e\n",
      "Loaded job: d6e54f22-efe1-4dc7-8e5a-cb8618634691\n",
      "Loaded job: d8961472-c59c-48d1-a5b5-d49c778fcac5\n",
      "Loaded job: d9a357a7-5e50-4bf8-a771-ef50414c2bac\n",
      "Loaded job: db53eb13-7c11-416d-9e19-577d654d9f84\n",
      "Loaded job: e301b10c-7e2b-485e-8978-4ee2c9669953\n",
      "Loaded job: e92e6676-9a70-42b5-81d6-907782ebad16\n",
      "Loaded job: e9310831-2398-4e49-8338-b3d344a9c73e\n",
      "Loaded job: edcd8df0-7dc7-4896-8d28-c435462059b6\n",
      "Loaded job: f0b34589-98f6-4b6a-a379-394e4e7f088f\n",
      "Loaded job: f12ad9e6-0c44-4c19-8f5d-37c2f3f14c18\n",
      "Loaded job: f6ee0835-b07f-4ccf-8109-1d2c5b309061\n",
      "Loaded job: fa055b06-313a-4ed6-88e9-282c1f073da3\n",
      "Loaded job: fdf5ddcf-6ea4-4c8c-a4aa-be278cf6f017\n",
      "Loaded 100 job descriptions from S3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def safe_preview_job(job):\n",
    "    \"\"\"\n",
    "    Safely preview a job description with fallback values.\n",
    "    \n",
    "    Args:\n",
    "        job (dict): Job description dictionary\n",
    "    \n",
    "    Returns:\n",
    "        dict: Preview of job with safe access to values\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"job_id\": job.get(\"job_id\", \"Unknown\"),\n",
    "        \"job_title\": (\n",
    "            job.get(\"details\", {}).get(\"job_title\", [\"Unknown\"])[0] \n",
    "            if job.get(\"details\", {}).get(\"job_title\") \n",
    "            else \"Unknown\"\n",
    "        ),\n",
    "        \"company_name\": (\n",
    "            job.get(\"details\", {}).get(\"company_name\", [\"Unknown\"])[0] \n",
    "            if job.get(\"details\", {}).get(\"company_name\") \n",
    "            else \"Unknown\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "if sample_jobs:\n",
    "    try:\n",
    "        job_preview = safe_preview_job(sample_jobs[0])\n",
    "        print(\"\\nPreview of first job:\")\n",
    "        print(json.dumps(job_preview, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Error previewing job: {e}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Preview of first job:\n",
      "{\n",
      "  \"job_id\": \"06ba6c67-c4eb-4358-aa2a-64dc8d186198\",\n",
      "  \"job_title\": \"ReactJS Web Architect\",\n",
      "  \"company_name\": \"Unknown\"\n",
      "}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def extract_job_strings(job_data):\n",
    "    \"\"\"\n",
    "    Extract all relevant strings from a job description for embedding.\n",
    "    Does not add prefix information to strings.\n",
    "    \n",
    "    Args:\n",
    "        job_data: Job description dictionary\n",
    "        \n",
    "    Returns:\n",
    "        List of unique strings to be embedded\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    def extract_strings(data):\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                # Add the key itself if it's a string\n",
    "                if isinstance(key, str):\n",
    "                    result.append(key)\n",
    "                \n",
    "                # Process the value\n",
    "                extract_strings(value)\n",
    "                \n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                extract_strings(item)\n",
    "                \n",
    "        elif isinstance(data, str) and data.strip():\n",
    "            # For string values, add only the raw string\n",
    "            result.append(data.strip())\n",
    "    \n",
    "    extract_strings(job_data)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_result = []\n",
    "    for item in result:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            unique_result.append(item)\n",
    "    \n",
    "    return unique_result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def ensure_correct_vector_format(vector_data):\n",
    "    \"\"\"\n",
    "    Ensures that the vector values are in the correct format for Pinecone:\n",
    "    - Must be a flat list of float values, not nested lists\n",
    "    \n",
    "    Args:\n",
    "        vector_data: The vector data to check/fix\n",
    "        \n",
    "    Returns:\n",
    "        Properly formatted vector data\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Check if the vector values are nested lists\n",
    "    if isinstance(vector_data, list) and vector_data and isinstance(vector_data[0], list):\n",
    "        # It's a nested list - flatten it to a 1D array\n",
    "        print(\"WARNING: Found nested list in vector data, flattening to 1D array\")\n",
    "        flat_array = np.array(vector_data).flatten()\n",
    "        return flat_array.tolist()\n",
    "    \n",
    "    # Ensure it's a list of floats\n",
    "    if isinstance(vector_data, np.ndarray):\n",
    "        return vector_data.tolist()\n",
    "    \n",
    "    return vector_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def process_and_vectorize_strings(jobs, pinecone_index, embedder, batch_size=4, max_retries=3):\n",
    "    \"\"\"\n",
    "    Process job descriptions, extract unique strings, vectorize them, and upload to Pinecone.\n",
    "    - Excludes 'details' section\n",
    "    - Removes duplicate strings\n",
    "    - Stores only the string text in the metadata (no job_id)\n",
    "    \n",
    "    Args:\n",
    "        jobs: List of job description dictionaries\n",
    "        pinecone_index: Pinecone index instance\n",
    "        embedder: Embedding generator instance\n",
    "        batch_size: Batch size for processing\n",
    "        max_retries: Maximum number of retries for failed operations\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with processing statistics\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import uuid\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Validate inputs\n",
    "    if not jobs:\n",
    "        print(\"Error: No job descriptions provided\")\n",
    "        return {\"status\": \"error\", \"reason\": \"no_jobs\"}\n",
    "        \n",
    "    if not pinecone_index:\n",
    "        print(\"Error: No Pinecone index provided\")\n",
    "        return {\"status\": \"error\", \"reason\": \"no_index\"}\n",
    "    \n",
    "    # Initialize metrics\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract all unique strings across all jobs\n",
    "    print(f\"Extracting strings from {len(jobs)} job descriptions...\")\n",
    "    all_unique_strings = set()\n",
    "    \n",
    "    for job in jobs:\n",
    "        try:\n",
    "            job_strings = extract_job_strings(job)\n",
    "            all_unique_strings.update(job_strings)\n",
    "        except Exception as e:\n",
    "            job_id = job.get(\"job_id\", \"unknown\")\n",
    "            print(f\"Error extracting strings from job {job_id}: {str(e)}\")\n",
    "    \n",
    "    all_strings_list = list(all_unique_strings)\n",
    "    print(f\"Total unique strings: {len(all_strings_list)}\")\n",
    "    \n",
    "    # Generate embeddings for all unique strings\n",
    "    string_to_embedding = {}\n",
    "    if all_strings_list:\n",
    "        # Process in batches\n",
    "        for i in range(0, len(all_strings_list), batch_size):\n",
    "            batch = all_strings_list[i:i+batch_size]\n",
    "            batch_num = i//batch_size + 1\n",
    "            total_batches = (len(all_strings_list) - 1) // batch_size + 1\n",
    "            \n",
    "            print(f\"Processing batch {batch_num}/{total_batches} ({batch_num/total_batches*100:.1f}%)...\")\n",
    "            \n",
    "            # Retry logic\n",
    "            for retry in range(max_retries + 1):\n",
    "                try:\n",
    "                    # Generate embeddings\n",
    "                    batch_embeddings = embedder.generate_embeddings(batch)\n",
    "                    \n",
    "                    # Store embeddings\n",
    "                    for text, embedding in zip(batch, batch_embeddings):\n",
    "                        string_to_embedding[text] = embedding\n",
    "                    \n",
    "                    break  # Success, exit retry loop\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if retry < max_retries:\n",
    "                        sleep_time = 2 ** retry\n",
    "                        print(f\"Error in batch {batch_num}, retrying in {sleep_time}s: {str(e)}\")\n",
    "                        time.sleep(sleep_time)\n",
    "                    else:\n",
    "                        print(f\"Failed after {max_retries} retries for batch {batch_num}: {str(e)}\")\n",
    "    \n",
    "    # Upload vector embeddings to Pinecone\n",
    "    vectors_uploaded = 0\n",
    "    if string_to_embedding:\n",
    "        vectors_to_upload = []\n",
    "        \n",
    "        # Prepare vectors - one for each unique string, no job association\n",
    "        for i, (text, embedding) in enumerate(string_to_embedding.items()):\n",
    "            # Ensure proper vector format\n",
    "            vector_values = ensure_correct_vector_format(embedding)\n",
    "            \n",
    "            # Create a unique ID for each string\n",
    "            vector_id = f\"string_{i}_{uuid.uuid4()}\"\n",
    "            \n",
    "            # Create metadata with only the text\n",
    "            metadata = {\n",
    "                \"text\": text[:1000] if len(text) > 1000 else text\n",
    "            }\n",
    "            \n",
    "            # Add to upload list\n",
    "            vectors_to_upload.append({\n",
    "                \"id\": vector_id,\n",
    "                \"values\": vector_values,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "        \n",
    "        print(f\"Prepared {len(vectors_to_upload)} vectors for upload\")\n",
    "        \n",
    "        # Upload in batches\n",
    "        with tqdm(total=len(vectors_to_upload), desc=\"Uploading to Pinecone\") as pbar:\n",
    "            for i in range(0, len(vectors_to_upload), batch_size):\n",
    "                batch = vectors_to_upload[i:i+batch_size]\n",
    "                \n",
    "                # Try upload with retries\n",
    "                for retry in range(max_retries + 1):\n",
    "                    try:\n",
    "                        pinecone_index.upsert(vectors=batch)\n",
    "                        vectors_uploaded += len(batch)\n",
    "                        pbar.update(len(batch))\n",
    "                        break  # Success, exit retry loop\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        if retry < max_retries:\n",
    "                            sleep_time = 2 ** retry\n",
    "                            print(f\"Upload error, retrying in {sleep_time}s: {str(e)}\")\n",
    "                            time.sleep(sleep_time)\n",
    "                        else:\n",
    "                            print(f\"Failed to upload batch after {max_retries} retries: {str(e)}\")\n",
    "    \n",
    "    # Return comprehensive statistics\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"total_jobs\": len(jobs),\n",
    "        \"total_unique_strings\": len(all_strings_list),\n",
    "        \"strings_embedded\": len(string_to_embedding),\n",
    "        \"vectors_uploaded\": vectors_uploaded,\n",
    "        \"processing_time\": total_time\n",
    "    }\n",
    "\n",
    "\n",
    "# Modified search function to work with string-only vectors\n",
    "def semantic_string_search(query, pinecone_index, embedder, similarity_threshold=0.8, top_k=100):\n",
    "    \"\"\"\n",
    "    Search for semantically similar strings in Pinecone.\n",
    "    \n",
    "    Args:\n",
    "        query: The text to search for\n",
    "        pinecone_index: The Pinecone index to search\n",
    "        embedder: Embedding generator instance\n",
    "        similarity_threshold: Minimum similarity score (0-1) to include in results\n",
    "        top_k: Maximum number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with text content and similarity score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_embedding = embedder.generate_embeddings([query])[0]\n",
    "        \n",
    "        search_results = pinecone_index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            include_values=False\n",
    "        )\n",
    "        \n",
    "        similar_strings = []\n",
    "        \n",
    "        matches = getattr(search_results, 'matches', [])\n",
    "        \n",
    "        for match in matches:\n",
    "\n",
    "            score = getattr(match, 'score', 0)\n",
    "            metadata = getattr(match, 'metadata', {})\n",
    "            \n",
    "            if score < similarity_threshold:\n",
    "                continue\n",
    "                \n",
    "            text = metadata.get('text', '')\n",
    "            \n",
    "            similar_strings.append({\n",
    "                'text': text,\n",
    "                'similarity': score\n",
    "            })\n",
    "        \n",
    "        return similar_strings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in semantic search: {str(e)}\")\n",
    "        return []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def run_vectorization_pipeline():\n",
    "    # Load jobs\n",
    "    jobs = load_job_descriptions(limit=101)\n",
    "    \n",
    "    # Initialize Pinecone and embedder\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    embedder = ResilientSageMakerEmbedder(\n",
    "        endpoint_name=SAGEMAKER_ENDPOINT_NAME, \n",
    "        region=AWS_REGION\n",
    "    )\n",
    "    \n",
    "    # Process and upload\n",
    "    stats = process_and_vectorize_strings(\n",
    "        jobs=jobs,\n",
    "        pinecone_index=pinecone_index,\n",
    "        embedder=embedder,\n",
    "        batch_size=4\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing complete!\")\n",
    "    print(f\"Total unique strings: {stats.get('total_unique_strings', 0)}\")\n",
    "    print(f\"Strings embedded: {stats.get('strings_embedded', 0)}\")\n",
    "    print(f\"Vectors uploaded: {stats.get('vectors_uploaded', 0)}\")\n",
    "    \n",
    "    # Test a search\n",
    "    query = \"Python\"\n",
    "    results = semantic_string_search(\n",
    "        query=query,\n",
    "        pinecone_index=pinecone_index,\n",
    "        embedder=embedder,\n",
    "        similarity_threshold=0.8\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nStrings similar to '{query}':\")\n",
    "    for i, result in enumerate(results[:10]):  # Show top 10\n",
    "        print(f\"{i+1}. {result['text']} (Score: {result['similarity']:.4f})\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# For a specific job, list all its strings\n",
    "if job_ids:\n",
    "    job_id = job_ids[0]  # Take the first job\n",
    "    strings = list_all_strings_for_job(pinecone_index, job_id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedder = ResilientSageMakerEmbedder(\n",
    "        endpoint_name=SAGEMAKER_ENDPOINT_NAME,\n",
    "        region=AWS_REGION\n",
    "    )\n",
    "\n",
    "def semantic_search(query_title, pinecone_index, embedder=None, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Search for semantically similar job titles in Pinecone.\n",
    "    \n",
    "    Args:\n",
    "        query_title: The job title to search for\n",
    "        pinecone_index: The Pinecone index to search\n",
    "        embedder: Embedding generator instance (optional)\n",
    "        similarity_threshold: Minimum similarity score (0-1) to include in results\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with job_id, job_title, and similarity score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create embedder if not provided\n",
    "        if embedder is None:\n",
    "            from random_embedding import RandomEmbeddingGenerator\n",
    "            print(\"No embedder provided. Creating RandomEmbeddingGenerator as fallback.\")\n",
    "            embedder = RandomEmbeddingGenerator()\n",
    "            \n",
    "        \n",
    "        query_text = f\"{query_title}\"  # You may need to adjust the format\n",
    "        query_embedding = embedder.generate_embeddings([query_text])[0]\n",
    "        \n",
    "        search_results = pinecone_index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=100,  # Get enough results to filter\n",
    "            include_metadata=True,\n",
    "            include_values=False  # only need the scores, not the vectors\n",
    "        )\n",
    "        \n",
    "        similar_text = []\n",
    "        \n",
    "        # Pinecone v6+ returns objects instead of dictionaries\n",
    "        matches = getattr(search_results, 'matches', [])\n",
    "        \n",
    "        for match in matches:\n",
    "            # Extract match properties\n",
    "            score = getattr(match, 'score', 0)\n",
    "            metadata = getattr(match, 'metadata', {})\n",
    "            \n",
    "            # Skip if below threshold\n",
    "            if score < similarity_threshold:\n",
    "                continue\n",
    "                \n",
    "            # Extract text\n",
    "            text = metadata.get('text', metadata.get('text', '')).replace('Text: ', '')\n",
    "            \n",
    "            similar_text.append({\n",
    "                'text': text,\n",
    "                'similarity': score\n",
    "            })\n",
    "        \n",
    "        return similar_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in semantic search: {str(e)}\")\n",
    "        return []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test the function with a sample job title\n",
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "pinecone_index = pc.Index(PINECONE_INDEX_NAME)\n",
    "\n",
    "test_query = \"Data Scientist\"\n",
    "results = semantic_search(\n",
    "    test_query, \n",
    "    pinecone_index=pinecone_index,\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "print(f\"Jobs similar to '{test_query}' (similarity > 0.8):\")\n",
    "for job in results:\n",
    "    print(f\"- {job['text']}, Score: {job['similarity']:.4f})\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}