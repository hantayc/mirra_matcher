{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Any, Union\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "SAGEMAKER_ENDPOINT_NAME = \"e5-embeddings-huggingface\"\n",
    "AWS_REGION = \"us-east-1\"\n",
    "\n",
    "PINECONE_API_KEY = \"\"  \n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"  # matched with AWS region\n",
    "PINECONE_INDEX_NAME = \"mirra-filtering\"\n",
    "\n",
    "EMBEDDING_DIMENSION = 1024"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class ResilientSageMakerEmbedder:\n",
    "    \"\"\"\n",
    "    A wrapper around SageMaker embedding endpoints with resilience features.\n",
    "    Includes text length limits, proper error handling, and fallbacks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint_name, max_text_length=512, region=\"us-east-1\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedder with a SageMaker endpoint.\n",
    "        \n",
    "        Args:\n",
    "            endpoint_name: The name of the SageMaker endpoint\n",
    "            max_text_length: Maximum text length to truncate to\n",
    "            region: AWS region for the endpoint\n",
    "        \"\"\"\n",
    "        self.sagemaker_runtime = boto3.client('sagemaker-runtime', region_name=region)\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.max_text_length = max_text_length\n",
    "        print(f\"Initialized ResilientSageMakerEmbedder for endpoint: {endpoint_name}\")\n",
    "\n",
    "    def _prepare_text(self, text):\n",
    "        \"\"\"Clean and prepare text for the embedding model.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Add E5 prefix\n",
    "        if not text.startswith(\"passage:\"):\n",
    "            text = f\"passage: {text}\"\n",
    "        \n",
    "        # Truncate if needed\n",
    "        if len(text) > self.max_text_length:\n",
    "            text = text[:self.max_text_length]\n",
    "            \n",
    "        return text\n",
    "\n",
    "    def generate_embeddings(self, texts, max_retries=3):\n",
    "        \"\"\"\n",
    "        Generate embeddings using SageMaker E5 endpoint with retries and fallbacks\n",
    "        \n",
    "        Args:\n",
    "            texts: String or list of texts to embed\n",
    "            max_retries: Maximum retry attempts for API failures\n",
    "            \n",
    "        Returns:\n",
    "            List of embedding vectors\n",
    "        \"\"\"\n",
    "        # Ensure texts is a list\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "            \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            # Process with retries\n",
    "            for retry in range(max_retries):\n",
    "                try:\n",
    "                    # Prepare the single text\n",
    "                    prepared_text = self._prepare_text(text)\n",
    "                    \n",
    "                    # Prepare payload with explicit pooling parameters\n",
    "                    payload = {\n",
    "                        \"inputs\": [prepared_text],\n",
    "                        \"parameters\": {\n",
    "                            \"normalize\": True,\n",
    "                            \"pooling\": \"mean\",\n",
    "                            \"return_sentence_embedding\": True\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    # Call SageMaker endpoint\n",
    "                    response = self.sagemaker_runtime.invoke_endpoint(\n",
    "                        EndpointName=self.endpoint_name,\n",
    "                        ContentType='application/json',\n",
    "                        Body=json.dumps(payload)\n",
    "                    )\n",
    "                    \n",
    "                    response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
    "                    \n",
    "                    emb_array = np.array(response_body[0])\n",
    "                    \n",
    "                    if len(emb_array.shape) > 1:\n",
    "                        # Average across all but the last dimension\n",
    "                        while len(emb_array.shape) > 1:\n",
    "                            emb_array = np.mean(emb_array, axis=0)\n",
    "                    \n",
    "                    if emb_array.shape[0] != EMBEDDING_DIMENSION:\n",
    "                        if emb_array.shape[0] > EMBEDDING_DIMENSION:\n",
    "                            emb_array = emb_array[:EMBEDDING_DIMENSION]\n",
    "                        else:\n",
    "                            padded = np.zeros(EMBEDDING_DIMENSION)\n",
    "                            padded[:emb_array.shape[0]] = emb_array\n",
    "                            emb_array = padded\n",
    "                    \n",
    "                    all_embeddings.append(emb_array.tolist())\n",
    "                    \n",
    "                    # Small delay to prevent overwhelming the endpoint\n",
    "                    if i < len(texts) - 1:\n",
    "                        time.sleep(0.1)\n",
    "                        \n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if retry < max_retries - 1:\n",
    "                        wait_time = (2 ** retry) * 0.5  # Exponential backoff\n",
    "                        print(f\"Retry {retry+1} for text {i+1}: {str(e)}\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"Error generating embedding for text {i+1}: {str(e)}\")\n",
    "                        \n",
    "                        all_embeddings.append(self._create_random_unit_vector())\n",
    "    \n",
    "        return all_embeddings\n",
    "    \n",
    "    def _create_random_unit_vector(self, dim=1024):\n",
    "        \"\"\"Create a random unit vector for fallback\"\"\"\n",
    "        vec = np.random.normal(0, 1, size=dim)\n",
    "        return (vec / np.linalg.norm(vec)).tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def initialize_pinecone():\n",
    "    \"\"\"Initialize Pinecone and return the index\"\"\"\n",
    "    try:\n",
    "        # Initialize Pinecone client\n",
    "        from pinecone import Pinecone\n",
    "        pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        \n",
    "        # Check if the index exists\n",
    "        existing_indexes = pc.list_indexes().names()\n",
    "        print(f\"Available Pinecone indexes: {existing_indexes}\")\n",
    "        \n",
    "        if PINECONE_INDEX_NAME not in existing_indexes:\n",
    "            print(f\"Creating new index '{PINECONE_INDEX_NAME}'...\")\n",
    "            \n",
    "            # Create the index with the metadata fields we want to be searchable\n",
    "            pc.create_index(\n",
    "                name=PINECONE_INDEX_NAME,\n",
    "                dimension=EMBEDDING_DIMENSION,\n",
    "                metric=\"cosine\",\n",
    "                metadata_config={\n",
    "                    \"indexed\": [\n",
    "                        \"emp_type\",\n",
    "                        \"job_title\",\n",
    "                        \"exp_level\",\n",
    "                        \"domain\",\n",
    "                        \"location\",\n",
    "                        \"visa_sponsor\",\n",
    "                        \"salary_range_from\",\n",
    "                        \"salary_range_to\"\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            print(f\"Index '{PINECONE_INDEX_NAME}' created successfully\")\n",
    "        \n",
    "        # Connect to the index\n",
    "        index = pc.Index(PINECONE_INDEX_NAME)\n",
    "        print(f\"Connected to Pinecone index: {PINECONE_INDEX_NAME}\")\n",
    "        \n",
    "        return index\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Pinecone: {str(e)}\")\n",
    "        print(\"Please check your API key and environment settings.\")\n",
    "        return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def load_job_descriptions(start=0, limit=32300, bucket=\"mirra-matcher-325\", prefix=\"data/processed/jobs/\"):\n",
    "    \"\"\"\n",
    "    Load job descriptions from S3 with pagination.\n",
    "\n",
    "    Args:\n",
    "        start: Index to start loading from\n",
    "        limit: Maximum number of jobs to load\n",
    "        bucket: S3 bucket name\n",
    "        prefix: S3 prefix for job files\n",
    "\n",
    "    Returns:\n",
    "        List of job description dicts\n",
    "    \"\"\"\n",
    "    import boto3\n",
    "    import json\n",
    "\n",
    "    jobs = []\n",
    "    s3_client = boto3.client('s3')\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "    seen = 0\n",
    "    for page in page_iterator:\n",
    "        if 'Contents' not in page:\n",
    "            continue\n",
    "\n",
    "        for obj in page['Contents']:\n",
    "            if seen < start:\n",
    "                seen += 1\n",
    "                continue\n",
    "            if len(jobs) >= limit:\n",
    "                return jobs\n",
    "\n",
    "            key = obj['Key']\n",
    "            if key.endswith(\".json\"):\n",
    "                response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "                job_data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "                if \"job_id\" not in job_data:\n",
    "                    job_data[\"job_id\"] = key.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "                jobs.append(job_data)\n",
    "                print(f\"Loaded job: {job_data.get('job_id')}\")\n",
    "            seen += 1\n",
    "\n",
    "    print(f\"Loaded {len(jobs)} job descriptions from S3\")\n",
    "    return jobs\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def format_job_for_embedding(job):\n",
    "    \"\"\"\n",
    "    Format a job JSON into a comprehensive text representation for embedding.\n",
    "    \n",
    "    Args:\n",
    "        job: Job description dictionary\n",
    "    \n",
    "    Returns:\n",
    "        String representation of the job with all key attributes and\n",
    "        Dictionary of metadata for storage in Pinecone\n",
    "    \"\"\"\n",
    "    # Extract job details\n",
    "    details = job.get(\"details\", {})\n",
    "    \n",
    "    # Extract each attribute, handling potential missing data\n",
    "    # Job title\n",
    "    job_title_data = details.get(\"job_title\", [\"Unknown\"]) \n",
    "    job_title = job_title_data[0] if isinstance(job_title_data, list) and job_title_data else \"Unknown\"\n",
    "    \n",
    "    # Location handling\n",
    "    location_data = details.get(\"location\", [])\n",
    "    location = \"Remote\"  # Default\n",
    "    \n",
    "    if isinstance(location_data, list) and location_data:\n",
    "        location_item = location_data[0]\n",
    "        if isinstance(location_item, dict):\n",
    "            city = location_item.get(\"city\", \"\")\n",
    "            state = location_item.get(\"state\", \"\")\n",
    "            country = location_item.get(\"country\", \"\")\n",
    "            location = \", \".join(filter(None, [city, state, country]))\n",
    "    \n",
    "    # Company name\n",
    "    company_data = details.get(\"company_name\", [])\n",
    "    company_name = company_data[0] if isinstance(company_data, list) and company_data else \"Unknown\"\n",
    "    \n",
    "    # Employment type - Default to \"Full-time\" if empty\n",
    "    employment_data = details.get(\"employment_type\", [])\n",
    "    if not employment_data and details.get(\"tax_terms\"):\n",
    "        # Use tax_terms if employment_type is empty\n",
    "        employment_data = details.get(\"tax_terms\", [\"Full-time\"])\n",
    "    employment_type = employment_data[0] if isinstance(employment_data, list) and employment_data else \"Full-time\"\n",
    "    \n",
    "    # Experience level (from required years in hard skills or from details.experience_level)\n",
    "    experience_level = \"Entry-level\"  # Default\n",
    "    \n",
    "    # First check if experience_level is directly specified\n",
    "    exp_level_data = details.get(\"experience_level\", [])\n",
    "    if isinstance(exp_level_data, list) and exp_level_data:\n",
    "        experience_level = exp_level_data[0]\n",
    "    # If not directly specified, try to infer from the required years in hard skills\n",
    "    elif job.get(\"mandatory\", {}).get(\"hard_skills\"):\n",
    "        max_years = 0\n",
    "        for skill in job[\"mandatory\"][\"hard_skills\"]:\n",
    "            min_years_data = skill.get(\"minyears\", [0])\n",
    "            min_years = min_years_data[0] if isinstance(min_years_data, list) and min_years_data else 0\n",
    "            \n",
    "            # Convert to numeric if needed\n",
    "            if not isinstance(min_years, (int, float)):\n",
    "                try:\n",
    "                    min_years = float(min_years)\n",
    "                except (ValueError, TypeError):\n",
    "                    min_years = 0\n",
    "            \n",
    "            max_years = max(max_years, min_years)\n",
    "        \n",
    "        if max_years >= 7:\n",
    "            experience_level = \"Senior\"\n",
    "        elif max_years >= 3:\n",
    "            experience_level = \"Mid-level\"\n",
    "        else:\n",
    "            experience_level = \"Entry-level\"\n",
    "    \n",
    "    # Process and standardize salary information\n",
    "    # Initialize default values\n",
    "    salary_from = 0\n",
    "    salary_to = 0\n",
    "    \n",
    "    # Helper function to convert hourly rate to annual salary\n",
    "    def hourly_to_annual(hourly_rate):\n",
    "        try:\n",
    "            hourly = float(hourly_rate)\n",
    "            # 40 hours per week, 52 weeks per year\n",
    "            return int(hourly * 40 * 52)\n",
    "        except (ValueError, TypeError):\n",
    "            return 0\n",
    "    \n",
    "    # Helper function to determine if a value is likely an hourly rate\n",
    "    def is_likely_hourly(value):\n",
    "        try:\n",
    "            num_value = float(value)\n",
    "            # Most hourly rates are under 200, most annual salaries are over 20000\n",
    "            return num_value > 0 and num_value < 200\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "    \n",
    "    # Check for direct salary values at the top level of the job object\n",
    "    # Convert to annual if they look like hourly rates\n",
    "    if \"salary_range_from\" in job:\n",
    "        try:\n",
    "            from_val = float(job[\"salary_range_from\"])\n",
    "            if is_likely_hourly(from_val):\n",
    "                salary_from = hourly_to_annual(from_val)\n",
    "            else:\n",
    "                salary_from = int(from_val)\n",
    "        except (ValueError, TypeError):\n",
    "            salary_from = 0\n",
    "    \n",
    "    if \"salary_range_to\" in job:\n",
    "        try:\n",
    "            to_val = float(job[\"salary_range_to\"])\n",
    "            if is_likely_hourly(to_val):\n",
    "                salary_to = hourly_to_annual(to_val)\n",
    "            else:\n",
    "                salary_to = int(to_val)\n",
    "        except (ValueError, TypeError):\n",
    "            salary_to = 0\n",
    "    \n",
    "    # Try to get salary information from wage field in details\n",
    "    wage_data = details.get(\"wage\", [])\n",
    "    \n",
    "    if isinstance(wage_data, list) and wage_data:\n",
    "        for wage_item in wage_data:\n",
    "            # Check if the wage item is a dictionary\n",
    "            if isinstance(wage_item, dict):\n",
    "                min_wage = wage_item.get(\"min\", 0)\n",
    "                max_wage = wage_item.get(\"max\", 0)\n",
    "                pay_type = wage_item.get(\"pay_type\", \"\")\n",
    "                \n",
    "                try:\n",
    "                    # Standardize the salary values based on pay type\n",
    "                    if pay_type.lower() == \"hourly\":\n",
    "                        # Convert hourly to annual (40 hours/week, 52 weeks/year)\n",
    "                        annual_min = hourly_to_annual(min_wage)\n",
    "                        annual_max = hourly_to_annual(max_wage)\n",
    "                        \n",
    "                        if annual_min > 0 and (annual_min > salary_from or salary_from == 0):\n",
    "                            salary_from = annual_min\n",
    "                        if annual_max > 0 and (annual_max > salary_to or salary_to == 0):\n",
    "                            salary_to = annual_max\n",
    "                            \n",
    "                    elif pay_type.lower() == \"salary\":\n",
    "                        # For salary, use the values directly but ensure they're annualized\n",
    "                        if min_wage and float(min_wage) > 0:\n",
    "                            min_value = int(float(min_wage))\n",
    "                            if is_likely_hourly(min_value):\n",
    "                                min_value = hourly_to_annual(min_value)\n",
    "                            if min_value > salary_from or salary_from == 0:\n",
    "                                salary_from = min_value\n",
    "                                \n",
    "                        if max_wage and float(max_wage) > 0:\n",
    "                            max_value = int(float(max_wage))\n",
    "                            if is_likely_hourly(max_value):\n",
    "                                max_value = hourly_to_annual(max_value)\n",
    "                            if max_value > salary_to or salary_to == 0:\n",
    "                                salary_to = max_value\n",
    "                except (ValueError, TypeError):\n",
    "                    # If there's an error converting, continue to the next item\n",
    "                    continue\n",
    "    \n",
    "    if salary_from > 0 and salary_from < 200:\n",
    "        salary_from = hourly_to_annual(salary_from)\n",
    "        \n",
    "    if salary_to > 0 and salary_to < 200:\n",
    "        salary_to = hourly_to_annual(salary_to)\n",
    "    \n",
    "    if salary_to > 0 and salary_from > 0 and salary_to < salary_from:\n",
    "        salary_to = salary_from\n",
    "    \n",
    "    salary_range = \"Not specified\"\n",
    "    if salary_from > 0 or salary_to > 0:\n",
    "        if salary_from > 0 and salary_to > 0:\n",
    "            if salary_from == salary_to:\n",
    "                salary_range = f\"${salary_from:,}\"\n",
    "            else:\n",
    "                salary_range = f\"${salary_from:,} - ${salary_to:,}\"\n",
    "        elif salary_from > 0:\n",
    "            salary_range = f\"From ${salary_from:,}\"\n",
    "        elif salary_to > 0:\n",
    "            salary_range = f\"Up to ${salary_to:,}\"\n",
    "    \n",
    "    company_industry = details.get(\"company_industry\", [])\n",
    "    domain = company_industry[0] if isinstance(company_industry, list) and company_industry else \"Technology\"\n",
    "    \n",
    "    if \"domain\" in job and job[\"domain\"]:\n",
    "        domain = job[\"domain\"].strip('\"\\'')\n",
    "    \n",
    "    work_authorization = details.get(\"work_authorization\", [])\n",
    "    visa_sponsorship = \"No\"  \n",
    "    \n",
    "    if \"visa_sponsor\" in job:\n",
    "        visa_sponsorship = job[\"visa_sponsor\"].strip('\"\\'')\n",
    "    else:\n",
    "        if isinstance(work_authorization, list) and work_authorization:\n",
    "            for auth in work_authorization:\n",
    "                auth_lower = str(auth).lower()\n",
    "                if \"sponsor\" in auth_lower and \"not\" not in auth_lower and \"no\" not in auth_lower:\n",
    "                    visa_sponsorship = \"Yes\"\n",
    "                    break\n",
    "    \n",
    "    # Construct job text that includes responsibilities and required skills\n",
    "    skills_text = \"\"\n",
    "    if job.get(\"mandatory\", {}).get(\"hard_skills\"):\n",
    "        skills_list = []\n",
    "        for skill_item in job[\"mandatory\"][\"hard_skills\"]:\n",
    "            if skill_item.get(\"skill\"):\n",
    "                for skill_group in skill_item[\"skill\"]:\n",
    "                    if isinstance(skill_group, list):\n",
    "                        skills_list.append(\" \".join(skill_group))\n",
    "                    else:\n",
    "                        skills_list.append(skill_group)\n",
    "        if skills_list:\n",
    "            skills_text = \"Required skills: \" + \", \".join(skills_list)\n",
    "    \n",
    "    # Construct responsibilities text\n",
    "    responsibilities_text = \"\"\n",
    "    if job.get(\"responsibility\", {}).get(\"hard_skills\"):\n",
    "        resp_list = []\n",
    "        for resp_item in job[\"responsibility\"][\"hard_skills\"]:\n",
    "            if resp_item.get(\"skill\"):\n",
    "                for resp_group in resp_item[\"skill\"]:\n",
    "                    if isinstance(resp_group, list):\n",
    "                        resp_list.append(\" \".join(resp_group))\n",
    "                    else:\n",
    "                        resp_list.append(resp_group)\n",
    "        if resp_list:\n",
    "            responsibilities_text = \"Responsibilities: \" + \", \".join(resp_list)\n",
    "    \n",
    "    # Combine all attributes into a comprehensive text\n",
    "    job_text = f\"\"\"\n",
    "Job Title: {job_title}\n",
    "Company: {company_name}\n",
    "Location: {location}\n",
    "Employment Type: {employment_type}\n",
    "Experience Level: {experience_level}\n",
    "Industry/Domain: {domain}\n",
    "Salary Range: {salary_range}\n",
    "Visa Sponsorship: {visa_sponsorship}\n",
    "\n",
    "{skills_text}\n",
    "\n",
    "{responsibilities_text}\n",
    "\n",
    "Job Description Summary:\n",
    "This is a {employment_type} position for a {job_title} located in {location}. \n",
    "The role requires {experience_level} experience in the {domain} industry.\n",
    "{f\"Salary range: {salary_range}. \" if salary_range != \"Not specified\" else \"\"}\n",
    "Visa sponsorship is {visa_sponsorship}.\n",
    "\"\"\"\n",
    "    \n",
    "    return job_text.strip(), {\n",
    "        \"job_id\": job.get(\"job_id\", str(uuid.uuid4())),\n",
    "        \"job_title\": job_title,\n",
    "        \"emp_type\": employment_type,\n",
    "        \"exp_level\": experience_level,\n",
    "        \"domain\": domain,\n",
    "        \"location\": location,\n",
    "        \"salary_range_from\": salary_from,\n",
    "        \"salary_range_to\": salary_to,\n",
    "        \"visa_sponsor\": visa_sponsorship\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def process_and_upload_jobs(jobs, pinecone_index, batch_size=100, max_retries=3):\n",
    "    \"\"\"\n",
    "    Process jobs, generate embeddings, and upload to Pinecone\n",
    "    \n",
    "    Args:\n",
    "        jobs: List of job dictionaries\n",
    "        pinecone_index: Pinecone index instance\n",
    "        batch_size: Batch size for processing (keep low for stability)\n",
    "        max_retries: Maximum retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with processing statistics\n",
    "    \"\"\"\n",
    "    if not jobs:\n",
    "        print(\"No jobs provided for processing\")\n",
    "        return {\"status\": \"error\", \"reason\": \"no_jobs\"}\n",
    "    \n",
    "    if not pinecone_index:\n",
    "        print(\"No Pinecone index provided\")\n",
    "        return {\"status\": \"error\", \"reason\": \"no_index\"}\n",
    "    \n",
    "    print(f\"Processing {len(jobs)} jobs...\")\n",
    "    \n",
    "    embedder = ResilientSageMakerEmbedder(\n",
    "        endpoint_name=SAGEMAKER_ENDPOINT_NAME,\n",
    "        max_text_length=512, \n",
    "        region=AWS_REGION\n",
    "    )\n",
    "    \n",
    "    # Process all jobs\n",
    "    job_texts = []\n",
    "    job_metadata = []\n",
    "    \n",
    "    for job in jobs:\n",
    "        try:\n",
    "            # Format the job and extract metadata\n",
    "            job_text, metadata = format_job_for_embedding(job)\n",
    "            job_texts.append(job_text)\n",
    "            job_metadata.append(metadata)\n",
    "            print(f\"Processed job {metadata['job_id']} - {metadata['job_title']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job {job.get('job_id', 'unknown')}: {str(e)}\")\n",
    "    \n",
    "    vectors_uploaded = 0\n",
    "    total_vectors = len(job_texts)  \n",
    "    \n",
    "    with tqdm(total=total_vectors, desc=\"Processing and uploading\") as progress_bar:\n",
    "        for i in range(0, total_vectors, batch_size):\n",
    "            batch_texts = job_texts[i:i+batch_size]\n",
    "            batch_metadata = job_metadata[i:i+batch_size]\n",
    "            \n",
    "            print(f\"Processing batch {i//batch_size + 1}/{(total_vectors-1)//batch_size + 1}...\")\n",
    "            \n",
    "            try:\n",
    "                # Process strictly one text at a time with explicit error handling\n",
    "                batch_embeddings = []\n",
    "                for idx, text in enumerate(batch_texts):\n",
    "                    try:\n",
    "                        print(f\"Generating embedding for text {i+idx+1}/{total_vectors}\")\n",
    "                        embedding = embedder.generate_embeddings([text])[0]\n",
    "                        batch_embeddings.append(embedding)\n",
    "                        print(f\"Successfully generated embedding: length={len(embedding)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR generating embedding for text {i+idx+1}: {str(e)}\")\n",
    "                        # Use fallback vector and continue\n",
    "                        fallback_vector = embedder._create_random_unit_vector()\n",
    "                        batch_embeddings.append(fallback_vector)\n",
    "                        print(f\"Using fallback vector instead\")\n",
    "                    time.sleep(0.5)  # Brief pause between embeddings\n",
    "                \n",
    "                # Create vectors - with explicit length checks\n",
    "                vectors_to_upload = []\n",
    "                for j, embedding in enumerate(batch_embeddings):\n",
    "                    if not embedding or len(embedding) != EMBEDDING_DIMENSION:\n",
    "                        print(f\"WARNING: Invalid embedding at index {j}, using fallback\")\n",
    "                        embedding = embedder._create_random_unit_vector()\n",
    "                    \n",
    "                    if j < len(batch_metadata):\n",
    "                        job_id = batch_metadata[j]['job_id']\n",
    "                        vector_id = f\"job_{job_id}\"\n",
    "                        \n",
    "                        # Ensure all metadata values are properly formatted\n",
    "                        metadata_copy = {}\n",
    "                        for key, value in batch_metadata[j].items():\n",
    "                            if key in [\"salary_range_from\", \"salary_range_to\"]:\n",
    "                                metadata_copy[key] = int(value) if isinstance(value, (int, float)) else 0\n",
    "                            else:\n",
    "                                metadata_copy[key] = str(value) if not isinstance(value, str) else value\n",
    "                        \n",
    "                        vectors_to_upload.append({\n",
    "                            \"id\": vector_id,\n",
    "                            \"values\": embedding,\n",
    "                            \"metadata\": metadata_copy\n",
    "                        })\n",
    "                \n",
    "                # Print vector counts for clarity\n",
    "                print(f\"Prepared {len(vectors_to_upload)} vectors for upload\")\n",
    "                \n",
    "                # Explicit upload with better error logging\n",
    "                if vectors_to_upload:\n",
    "                    upload_success = False\n",
    "                    for retry in range(max_retries):\n",
    "                        try:\n",
    "                            pinecone_index.upsert(vectors=vectors_to_upload)\n",
    "                            vectors_uploaded += len(vectors_to_upload)\n",
    "                            progress_bar.update(len(vectors_to_upload))\n",
    "                            upload_success = True\n",
    "                            print(f\"Successfully uploaded {len(vectors_to_upload)} vectors\")\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            print(f\"Upload attempt {retry+1} failed: {str(e)}\")\n",
    "                            import traceback\n",
    "                            traceback.print_exc()\n",
    "                            if retry < max_retries - 1:\n",
    "                                wait_time = (2 ** retry) * 2.0  # Longer wait time\n",
    "                                print(f\"Waiting {wait_time}s before retry...\")\n",
    "                                time.sleep(wait_time)\n",
    "                    \n",
    "                    if not upload_success:\n",
    "                        print(\"WARNING: Failed to upload batch after all retries\")\n",
    "                else:\n",
    "                    print(\"No vectors to upload for this batch\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Critical error in batch {i//batch_size + 1}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    return {\n",
    "        \"total_jobs\": len(jobs),\n",
    "        \"vectors_uploaded\": vectors_uploaded\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def job_embedding_pipeline(limit=2001):\n",
    "    \"\"\"\n",
    "    Main pipeline to load, process, and upload job embeddings\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of jobs to process\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with processing statistics\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize Pinecone\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    if not pinecone_index:\n",
    "        return {\"status\": \"error\", \"reason\": \"Failed to initialize Pinecone\"}\n",
    "    \n",
    "    # Load jobs\n",
    "    jobs = load_job_descriptions(limit=limit)\n",
    "    if not jobs:\n",
    "        return {\"status\": \"error\", \"reason\": \"No jobs loaded\"}\n",
    "    \n",
    "    # Process and upload jobs\n",
    "    result = process_and_upload_jobs(jobs, pinecone_index, batch_size=1)\n",
    "    \n",
    "    # Get final index stats\n",
    "    try:\n",
    "        stats = pinecone_index.describe_index_stats()\n",
    "        total_vectors = stats.get('total_vector_count', 0)\n",
    "        print(f\"\\nFinal index statistics:\")\n",
    "        print(f\"  - Total vectors: {total_vectors}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting index stats: {str(e)}\")\n",
    "        total_vectors = result.get(\"vectors_uploaded\", 0)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"jobs_processed\": len(jobs),\n",
    "        \"vectors_uploaded\": result.get(\"vectors_uploaded\", 0),\n",
    "        \"total_vectors_in_index\": total_vectors,\n",
    "        \"processing_time_seconds\": processing_time\n",
    "    }\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def process_jobs_in_batches(batch_size=2000, start_index=0, max_jobs=37312):\n",
    "    \"\"\"\n",
    "    Process jobs in batches of specified size, keeping track of progress.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of jobs to process in each batch\n",
    "        start_index: Index to start from (for resuming)\n",
    "        max_jobs: Maximum number of jobs to process in total\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistics and the last processed index\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import os\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    progress_file = \"embedding_progress.json\"\n",
    "    log_file = f\"embedding_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    \n",
    "    # Setup logging\n",
    "    def log_message(message):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        formatted_msg = f\"[{timestamp}] {message}\"\n",
    "        print(formatted_msg)\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(formatted_msg + '\\n')\n",
    "    \n",
    "    log_message(f\"Starting job processing with batch size {batch_size}\")\n",
    "    \n",
    "    # Load progress if exists\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            progress = json.load(f)\n",
    "            processed_ids = set(progress.get(\"processed_job_ids\", []))\n",
    "            start_index = progress.get(\"last_index\", start_index)\n",
    "            log_message(f\"Resuming from index {start_index} with {len(processed_ids)} jobs already processed\")\n",
    "    else:\n",
    "        processed_ids = set()\n",
    "        log_message(\"Starting new processing job\")\n",
    "    \n",
    "    # Initialize Pinecone and embedder\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    if not pinecone_index:\n",
    "        log_message(\"Failed to initialize Pinecone. Check your API key and settings.\")\n",
    "        return {\"error\": \"Pinecone initialization failed\"}\n",
    "        \n",
    "    embedder = ResilientSageMakerEmbedder(\n",
    "        endpoint_name=SAGEMAKER_ENDPOINT_NAME, \n",
    "        region=AWS_REGION\n",
    "    )\n",
    "    \n",
    "    total_processed = len(processed_ids)\n",
    "    current_index = start_index\n",
    "    overall_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        while current_index < max_jobs:\n",
    "            # Load a chunk of jobs\n",
    "            chunk_size = min(batch_size * 2, max_jobs - current_index)  # Load more than we'll process\n",
    "            log_message(f\"Loading {chunk_size} jobs from index {current_index}\")\n",
    "            \n",
    "            chunk_jobs = load_job_descriptions(start=current_index, limit=chunk_size)\n",
    "\n",
    "            if not chunk_jobs:\n",
    "                log_message(\"No more jobs available to process\")\n",
    "                break\n",
    "                \n",
    "            # Process this chunk in batches\n",
    "            for batch_start in range(0, len(chunk_jobs), batch_size):\n",
    "                batch_end = min(batch_start + batch_size, len(chunk_jobs))\n",
    "                batch_jobs = chunk_jobs[batch_start:batch_end]\n",
    "                \n",
    "                # Filter out already processed jobs\n",
    "                new_batch_jobs = []\n",
    "                for job in batch_jobs:\n",
    "                    job_id = job.get(\"job_id\")\n",
    "                    if job_id and job_id not in processed_ids:\n",
    "                        new_batch_jobs.append(job)\n",
    "                \n",
    "                if new_batch_jobs:\n",
    "                    log_message(f\"Processing batch of {len(new_batch_jobs)} jobs\")\n",
    "                    batch_start_time = time.time()\n",
    "                    \n",
    "                    # Process the batch\n",
    "                    result = process_and_upload_jobs(\n",
    "                        jobs=new_batch_jobs,\n",
    "                        pinecone_index=pinecone_index,\n",
    "                        batch_size=10  # Smaller batch for embeddings\n",
    "                    )\n",
    "                    \n",
    "                    # Update processed IDs and stats\n",
    "                    for job in new_batch_jobs:\n",
    "                        processed_ids.add(job.get(\"job_id\"))\n",
    "                    \n",
    "                    total_processed += len(new_batch_jobs)\n",
    "                    \n",
    "                    # Save progress after each batch\n",
    "                    progress = {\n",
    "                        \"last_index\": current_index + batch_end,\n",
    "                        \"total_processed\": total_processed,\n",
    "                        \"processed_job_ids\": list(processed_ids),\n",
    "                        \"last_updated\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    with open(progress_file, 'w') as f:\n",
    "                        json.dump(progress, f)\n",
    "                    \n",
    "                    # Report batch statistics\n",
    "                    batch_time = time.time() - batch_start_time\n",
    "                    log_message(f\"Batch completed in {batch_time:.2f} seconds\")\n",
    "                    log_message(f\"Vectors uploaded: {result.get('vectors_uploaded', 0)}\")\n",
    "                    \n",
    "                    # Calculate and report progress\n",
    "                    elapsed = time.time() - overall_start_time\n",
    "                    jobs_per_second = total_processed / elapsed if elapsed > 0 else 0\n",
    "                    estimated_remaining = (max_jobs - total_processed) / jobs_per_second if jobs_per_second > 0 else \"unknown\"\n",
    "                    \n",
    "                    if isinstance(estimated_remaining, float):\n",
    "                        hours, remainder = divmod(estimated_remaining, 3600)\n",
    "                        minutes, seconds = divmod(remainder, 60)\n",
    "                        time_remaining = f\"{int(hours)}h {int(minutes)}m {int(seconds)}s\"\n",
    "                    else:\n",
    "                        time_remaining = \"unknown\"\n",
    "                    \n",
    "                    log_message(f\"Progress: {total_processed}/{max_jobs} jobs ({total_processed/max_jobs*100:.2f}%)\")\n",
    "                    log_message(f\"Speed: {jobs_per_second:.2f} jobs/second, Estimated time remaining: {time_remaining}\")\n",
    "                else:\n",
    "                    log_message(\"No new jobs to process in this batch - all already processed\")\n",
    "            \n",
    "            # Move to next chunk\n",
    "            current_index += chunk_size\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error during processing: {str(e)}\")\n",
    "        import traceback\n",
    "        log_message(traceback.format_exc())\n",
    "    \n",
    "    # Final report\n",
    "    total_time = time.time() - overall_start_time\n",
    "    hours, remainder = divmod(total_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    log_message(f\"Job processing complete!\")\n",
    "    log_message(f\"Total jobs processed: {total_processed}\")\n",
    "    log_message(f\"Total time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "    log_message(f\"Average processing speed: {total_processed/total_time:.2f} jobs/second\")\n",
    "    \n",
    "    return {\n",
    "        \"total_processed\": total_processed,\n",
    "        \"last_index\": current_index,\n",
    "        \"processed_ids\": len(processed_ids),\n",
    "        \"total_time_seconds\": total_time\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = process_jobs_in_batches(batch_size=1000)\n",
    "print(f\"Processing complete! {result['total_processed']} jobs processed in {result['total_time_seconds']/3600:.2f} hours.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pinecone import Pinecone\n",
    "\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "PINECONE_INDEX_NAME = \"mirra-filtering\"\n",
    "PROGRESS_FILE = \"embedding_progress.json\"\n",
    "LOG_FILE = f\"reconciliation_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "\n",
    "# Number of vectors to fetch in each batch when paginating\n",
    "FETCH_BATCH_SIZE = 1000\n",
    "\n",
    "def log_message(message):\n",
    "    \"\"\"Log a message to both console and file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    formatted_msg = f\"[{timestamp}] {message}\"\n",
    "    print(formatted_msg)\n",
    "    with open(LOG_FILE, 'a') as f:\n",
    "        f.write(formatted_msg + '\\n')\n",
    "\n",
    "def initialize_pinecone():\n",
    "    \"\"\"Initialize Pinecone and return the index\"\"\"\n",
    "    try:\n",
    "        # Initialize Pinecone client\n",
    "        pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        \n",
    "        # Check if the index exists\n",
    "        existing_indexes = pc.list_indexes().names()\n",
    "        log_message(f\"Available Pinecone indexes: {existing_indexes}\")\n",
    "        \n",
    "        if PINECONE_INDEX_NAME not in existing_indexes:\n",
    "            log_message(f\"Error: Index '{PINECONE_INDEX_NAME}' does not exist\")\n",
    "            return None\n",
    "        \n",
    "        # Connect to the index\n",
    "        index = pc.Index(PINECONE_INDEX_NAME)\n",
    "        log_message(f\"Connected to Pinecone index: {PINECONE_INDEX_NAME}\")\n",
    "        \n",
    "        return index\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error initializing Pinecone: {str(e)}\")\n",
    "        log_message(\"Please check your API key and environment settings.\")\n",
    "        return None\n",
    "\n",
    "def get_all_job_ids_from_pinecone(pinecone_index):\n",
    "    \"\"\"\n",
    "    Retrieve all job IDs from Pinecone using pagination to handle large indexes.\n",
    "    Returns a set of job IDs (without 'job_' prefix).\n",
    "    \"\"\"\n",
    "    all_job_ids = set()\n",
    "    total_fetched = 0\n",
    "    \n",
    "    try:\n",
    "        # Get total vector count\n",
    "        stats = pinecone_index.describe_index_stats()\n",
    "        total_vectors = stats.get('total_vector_count', 0)\n",
    "        log_message(f\"Pinecone index contains {total_vectors} total vectors\")\n",
    "        \n",
    "        log_message(\"Starting to fetch all vector IDs from Pinecone using pagination...\")\n",
    "        \n",
    "        # Use list_vectors if available (newer Pinecone SDK versions)\n",
    "        # Otherwise, fall back to fetching with dummy queries\n",
    "        try:\n",
    "            # First approach: Try using list_vectors (if available in your Pinecone version)\n",
    "            # This is the preferred method when available\n",
    "            page_token = None\n",
    "            batch_count = 0\n",
    "            \n",
    "            while True:\n",
    "                batch_count += 1\n",
    "                log_message(f\"Fetching batch {batch_count} of vector IDs...\")\n",
    "                \n",
    "                # Get the next batch of vectors\n",
    "                response = pinecone_index.list_vectors(\n",
    "                    limit=FETCH_BATCH_SIZE,\n",
    "                    page_token=page_token\n",
    "                )\n",
    "                \n",
    "                # Extract vector IDs\n",
    "                vector_ids = response.get('vectors', [])\n",
    "                for vector_id in vector_ids:\n",
    "                    if vector_id.startswith('job_'):\n",
    "                        job_id = vector_id[4:]  # Remove 'job_' prefix\n",
    "                    else:\n",
    "                        job_id = vector_id\n",
    "                    all_job_ids.add(job_id)\n",
    "                \n",
    "                total_fetched += len(vector_ids)\n",
    "                log_message(f\"Fetched {len(vector_ids)} vector IDs, total so far: {total_fetched}/{total_vectors}\")\n",
    "                \n",
    "                # Check if there are more vectors to fetch\n",
    "                page_token = response.get('page_token')\n",
    "                if not page_token:\n",
    "                    break\n",
    "                \n",
    "                # Add a small delay to avoid overwhelming the API\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "        except (AttributeError, TypeError) as e:\n",
    "           \n",
    "            log_message(f\"list_vectors not available, using alternative method: {str(e)}\")\n",
    "            log_message(\"Falling back to dummy query approach for fetching vector IDs...\")\n",
    "            \n",
    "            # Generate a dummy vector for querying (all zeros)\n",
    "            dimension = 1024 \n",
    "            dummy_vector = [0.0] * dimension\n",
    "            \n",
    "            offset = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            while offset < total_vectors:\n",
    "                batch_count += 1\n",
    "                batch_size = min(FETCH_BATCH_SIZE, total_vectors - offset)\n",
    "                \n",
    "                log_message(f\"Fetching batch {batch_count} of vector IDs (offset: {offset}, size: {batch_size})...\")\n",
    "                \n",
    "                # Query Pinecone with our dummy vector\n",
    "                response = pinecone_index.query(\n",
    "                    vector=dummy_vector,\n",
    "                    top_k=batch_size,\n",
    "                    include_metadata=True,\n",
    "                    include_values=False\n",
    "                )\n",
    "                \n",
    "                # Extract IDs from matches\n",
    "                matches = getattr(response, 'matches', response.get('matches', []))\n",
    "                \n",
    "                if not matches:\n",
    "                    log_message(f\"Warning: No matches returned in batch {batch_count}\")\n",
    "                    break\n",
    "                \n",
    "                batch_ids = []\n",
    "                for match in matches:\n",
    "                    vector_id = getattr(match, 'id', match.get('id', ''))\n",
    "                    if vector_id.startswith('job_'):\n",
    "                        job_id = vector_id[4:]  # Remove 'job_' prefix\n",
    "                    else:\n",
    "                        job_id = vector_id\n",
    "                    \n",
    "                    all_job_ids.add(job_id)\n",
    "                    batch_ids.append(job_id)\n",
    "                \n",
    "                total_fetched += len(batch_ids)\n",
    "                log_message(f\"Fetched {len(batch_ids)} vector IDs, total so far: {total_fetched}/{total_vectors}\")\n",
    "                \n",
    "                if len(batch_ids) < batch_size:\n",
    "                    break\n",
    "                \n",
    "                offset += batch_size\n",
    "                \n",
    "                # Adding a small delay to avoid overwhelming the API\n",
    "                time.sleep(1.0)\n",
    "        \n",
    "        log_message(f\"Completed fetching all vector IDs from Pinecone\")\n",
    "        log_message(f\"Total unique job IDs collected: {len(all_job_ids)}\")\n",
    "        \n",
    "        return all_job_ids\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error retrieving job IDs from Pinecone: {str(e)}\")\n",
    "        import traceback\n",
    "        log_message(traceback.format_exc())\n",
    "        return all_job_ids  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def reconcile_tracking_with_pinecone():\n",
    "    \"\"\"\n",
    "    Reconcile the local tracking file with the actual state in Pinecone.\n",
    "    Updates the local tracking to match Pinecone's count and sets IDs to match.\n",
    "    \"\"\"\n",
    "    log_message(\"Starting full reconciliation between local tracking and Pinecone\")\n",
    "    \n",
    "    # Initialize Pinecone\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    if not pinecone_index:\n",
    "        log_message(\"Reconciliation failed: Could not connect to Pinecone\")\n",
    "        return False\n",
    "    \n",
    "    # Get Pinecone stats\n",
    "    try:\n",
    "        stats = pinecone_index.describe_index_stats()\n",
    "        pinecone_count = stats.get('total_vector_count', 0)\n",
    "        log_message(f\"Pinecone reports {pinecone_count} total vectors\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error getting Pinecone stats: {str(e)}\")\n",
    "        return False\n",
    "    \n",
    "    # Load local tracking data\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        try:\n",
    "            with open(PROGRESS_FILE, 'r') as f:\n",
    "                progress_data = json.load(f)\n",
    "                local_count = progress_data.get(\"total_processed\", 0)\n",
    "                local_ids = set(progress_data.get(\"processed_job_ids\", []))\n",
    "                last_index = progress_data.get(\"last_index\", 0)\n",
    "                last_updated = progress_data.get(\"last_updated\", \"\")\n",
    "                \n",
    "                log_message(f\"Local tracking reports {local_count} processed jobs\")\n",
    "                log_message(f\"Local tracking has {len(local_ids)} job IDs\")\n",
    "                log_message(f\"Last index: {last_index}\")\n",
    "                log_message(f\"Last updated: {last_updated}\")\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error reading progress file: {str(e)}\")\n",
    "            log_message(\"Creating new progress file\")\n",
    "            progress_data = {\n",
    "                \"last_index\": 0,\n",
    "                \"total_processed\": 0,\n",
    "                \"processed_job_ids\": [],\n",
    "                \"last_updated\": datetime.now().isoformat()\n",
    "            }\n",
    "            local_count = 0\n",
    "            local_ids = set()\n",
    "            last_index = 0\n",
    "    else:\n",
    "        log_message(\"No local tracking file found. Creating new one.\")\n",
    "        progress_data = {\n",
    "            \"last_index\": 0,\n",
    "            \"total_processed\": 0,\n",
    "            \"processed_job_ids\": [],\n",
    "            \"last_updated\": datetime.now().isoformat()\n",
    "        }\n",
    "        local_count = 0\n",
    "        local_ids = set()\n",
    "        last_index = 0\n",
    "    \n",
    "    # Get all job IDs from Pinecone\n",
    "    pinecone_job_ids = get_all_job_ids_from_pinecone(pinecone_index)\n",
    "    log_message(f\"Retrieved {len(pinecone_job_ids)} job IDs from Pinecone\")\n",
    "    \n",
    "    # Find IDs in Pinecone but not in local tracking\n",
    "    missing_locally = pinecone_job_ids - local_ids\n",
    "    \n",
    "    # Find IDs in local tracking but not in Pinecone\n",
    "    missing_in_pinecone = local_ids - pinecone_job_ids\n",
    "    \n",
    "    log_message(f\"Found {len(missing_locally)} job IDs in Pinecone that are missing from local tracking\")\n",
    "    log_message(f\"Found {len(missing_in_pinecone)} job IDs in local tracking that are missing from Pinecone\")\n",
    "    \n",
    "    # Update local tracking with the complete set of job IDs from Pinecone\n",
    "    if missing_locally:\n",
    "        log_message(f\"Adding {len(missing_locally)} missing job IDs to local tracking\")\n",
    "        for job_id in missing_locally:\n",
    "            progress_data[\"processed_job_ids\"].append(job_id)\n",
    "    \n",
    "    # Log some info about IDs that are in local tracking but not in Pinecone\n",
    "    if missing_in_pinecone:\n",
    "        log_message(f\"Warning: {len(missing_in_pinecone)} job IDs in local tracking are not found in Pinecone\")\n",
    "        log_message(\"This could indicate jobs that failed to upload or were removed from Pinecone\")\n",
    "        \n",
    "        # Optionally, remove these IDs from local tracking\n",
    "        # Uncomment the following code to remove them:\n",
    "        \"\"\"\n",
    "        log_message(f\"Removing {len(missing_in_pinecone)} job IDs from local tracking that are not in Pinecone\")\n",
    "        progress_data[\"processed_job_ids\"] = list(pinecone_job_ids)\n",
    "        \"\"\"\n",
    "    \n",
    "    # Update the total count to match Pinecone\n",
    "    log_message(f\"Updating total_processed from {progress_data['total_processed']} to {pinecone_count}\")\n",
    "    progress_data[\"total_processed\"] = pinecone_count\n",
    "    \n",
    "    # Update last_updated timestamp\n",
    "    progress_data[\"last_updated\"] = datetime.now().isoformat()\n",
    "    \n",
    "    # Save updated progress data\n",
    "    try:\n",
    "        with open(PROGRESS_FILE, 'w') as f:\n",
    "            json.dump(progress_data, f, indent=2)\n",
    "        log_message(f\"Successfully updated local tracking file: {PROGRESS_FILE}\")\n",
    "        log_message(f\"New total_processed: {progress_data['total_processed']}\")\n",
    "        log_message(f\"Number of job IDs in tracking: {len(progress_data['processed_job_ids'])}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error saving progress file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = reconcile_tracking_with_pinecone()\n",
    "    if success:\n",
    "        log_message(\"Reconciliation completed successfully\")\n",
    "    else:\n",
    "        log_message(\"Reconciliation encountered errors\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run the pipeline\n",
    "    result = job_embedding_pipeline(limit=101)  \n",
    "    print(f\"\\nPipeline results: {result}\")\n",
    "    \n",
    "    # Example search\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    if pinecone_index:\n",
    "        embedder = ResilientSageMakerEmbedder(\n",
    "            endpoint_name=SAGEMAKER_ENDPOINT_NAME,\n",
    "            region=AWS_REGION\n",
    "        )\n",
    "        \n",
    "        # Search for data science jobs that sponsor visas\n",
    "        search_results = search_jobs(\n",
    "            query_text=\"Data Science\",\n",
    "            pinecone_index=pinecone_index,\n",
    "            embedder=embedder,\n",
    "            filters={\"visa_sponsor\": \"Yes\"}\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSearch results:\")\n",
    "        for i, result in enumerate(search_results):\n",
    "            print(f\"{i+1}. {result['job_title']} ({result['location']}) - Score: {result['similarity_score']:.4f}\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def search_jobs(query_text, pinecone_index, embedder=None, top_k=10, filters=None):\n",
    "    \"\"\"\n",
    "    Search for jobs using semantic embedding similarity\n",
    "    \n",
    "    Args:\n",
    "        query_text: Query text to search for\n",
    "        pinecone_index: Pinecone index to search\n",
    "        embedder: Embedding generator (optional)\n",
    "        top_k: Number of results to return\n",
    "        filters: Dictionary of metadata filters\n",
    "        \n",
    "    Returns:\n",
    "        List of job matches with scores\n",
    "    \"\"\"\n",
    "    if not embedder:\n",
    "        embedder = ResilientSageMakerEmbedder(\n",
    "            endpoint_name=SAGEMAKER_ENDPOINT_NAME,\n",
    "            region=AWS_REGION\n",
    "        )\n",
    "    \n",
    "    # Generate embedding for query\n",
    "    query_embedding = embedder.generate_embeddings([query_text])[0]\n",
    "    \n",
    "    # Prepare filters if any\n",
    "    filter_dict = {}\n",
    "    if filters:\n",
    "        for key, value in filters.items():\n",
    "            if value:  # Only add non-empty filters\n",
    "                # Handle numeric values for salary filters\n",
    "                if key in [\"salary_range_from\", \"salary_range_to\"] and isinstance(value, (int, float)):\n",
    "                    filter_dict[key] = {\"$gte\": value} if key == \"salary_range_from\" else {\"$lte\": value}\n",
    "                else:\n",
    "                    filter_dict[key] = value\n",
    "    \n",
    "    print(f\"Searching with filters: {filter_dict}\")\n",
    "    \n",
    "    # Perform search\n",
    "    try:\n",
    "        search_results = pinecone_index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            filter=filter_dict if filter_dict else None\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        \n",
    "        # For Pinecone v6+ compatibility\n",
    "        matches = search_results.get('matches', [])\n",
    "        if hasattr(search_results, 'matches'):\n",
    "            matches = search_results.matches\n",
    "        \n",
    "        for match in matches:\n",
    "            # Handle different response formats\n",
    "            if hasattr(match, 'metadata'):\n",
    "                metadata = match.metadata\n",
    "                score = match.score\n",
    "            else:\n",
    "                metadata = match.get('metadata', {})\n",
    "                score = match.get('score', 0)\n",
    "            \n",
    "            results.append({\n",
    "                'job_id': metadata.get('job_id', 'unknown'),\n",
    "                'job_title': metadata.get('job_title', 'Unknown'),\n",
    "                'location': metadata.get('location', 'Unknown'),\n",
    "                'emp_type': metadata.get('emp_type', 'Unknown'),\n",
    "                'exp_level': metadata.get('exp_level', 'Unknown'),\n",
    "                'domain': metadata.get('domain', 'Unknown'),\n",
    "                'visa_sponsor': metadata.get('visa_sponsor', 'No'),\n",
    "                'similarity_score': score\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching jobs: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Example search\n",
    "pinecone_index = initialize_pinecone()\n",
    "if pinecone_index:\n",
    "        embedder = ResilientSageMakerEmbedder(\n",
    "            endpoint_name=SAGEMAKER_ENDPOINT_NAME,\n",
    "            region=AWS_REGION\n",
    "        )\n",
    "        \n",
    "        # Search for data science jobs that sponsor visas\n",
    "        search_results = search_jobs(\n",
    "            query_text=\"Data Science\",\n",
    "            pinecone_index=pinecone_index,\n",
    "            embedder=embedder,\n",
    "            filters={\"visa_sponsor\": \"Yes\"}\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSearch results:\")\n",
    "        for i, result in enumerate(search_results):\n",
    "            print(f\"{i+1}. {result['job_title']} ({result['location']}) - Score: {result['similarity_score']:.4f}\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}