{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Any, Union\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "SAGEMAKER_ENDPOINT_NAME = \"e5-embeddings-pooled-2\" \n",
    "AWS_REGION = \"us-east-1\"\n",
    "\n",
    "PINECONE_API_KEY = \"\"  \n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"  # matched with AWS region\n",
    "PINECONE_INDEX_NAME = \"mirra-filtering\"\n",
    "\n",
    "EMBEDDING_DIMENSION = 1024"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ResilientSageMakerEmbedder:\n",
    "    \"\"\"\n",
    "    A wrapper around SageMaker embedding endpoints with resilience features.\n",
    "    Includes text length limits, proper error handling, and fallbacks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint_name, max_text_length=512, region=\"us-east-1\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedder with a SageMaker endpoint.\n",
    "        \n",
    "        Args:\n",
    "            endpoint_name: The name of the SageMaker endpoint\n",
    "            max_text_length: Maximum text length to truncate to\n",
    "            region: AWS region for the endpoint\n",
    "        \"\"\"\n",
    "        self.sagemaker_runtime = boto3.client('sagemaker-runtime', region_name=region)\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.max_text_length = max_text_length\n",
    "        print(f\"Initialized ResilientSageMakerEmbedder for endpoint: {endpoint_name}\")\n",
    "\n",
    "    def _prepare_text(self, text):\n",
    "        \"\"\"Clean and prepare text for the embedding model.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        # Remove excessive whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Add E5 prefix\n",
    "        if not text.startswith(\"passage:\"):\n",
    "            text = f\"passage: {text}\"\n",
    "        \n",
    "        # Truncate if needed\n",
    "        if len(text) > self.max_text_length:\n",
    "            text = text[:self.max_text_length]\n",
    "            \n",
    "        return text\n",
    "\n",
    "    def generate_embeddings(self, texts, max_retries=3):\n",
    "        \"\"\"\n",
    "        Generate embeddings using SageMaker E5 endpoint with retries and fallbacks\n",
    "        \n",
    "        Args:\n",
    "            texts: String or list of texts to embed\n",
    "            max_retries: Maximum retry attempts for API failures\n",
    "            \n",
    "        Returns:\n",
    "            List of embedding vectors\n",
    "        \"\"\"\n",
    "        # Ensure texts is a list\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "            \n",
    "        # Process one text at a time for maximum resilience\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            # Process with retries\n",
    "            for retry in range(max_retries):\n",
    "                try:\n",
    "                    # Prepare the single text\n",
    "                    prepared_text = self._prepare_text(text)\n",
    "                    \n",
    "                    # Prepare payload with explicit pooling parameters\n",
    "                    payload = {\n",
    "                        \"inputs\": [prepared_text],\n",
    "                        \"parameters\": {\n",
    "                            \"normalize\": True,\n",
    "                            \"pooling\": \"mean\",\n",
    "                            \"return_sentence_embedding\": True\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    # Call SageMaker endpoint\n",
    "                    response = self.sagemaker_runtime.invoke_endpoint(\n",
    "                        EndpointName=self.endpoint_name,\n",
    "                        ContentType='application/json',\n",
    "                        Body=json.dumps(payload)\n",
    "                    )\n",
    "                    \n",
    "                    # Parse response\n",
    "                    response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
    "                    \n",
    "                    # Process embedding\n",
    "                    emb_array = np.array(response_body[0])\n",
    "                    \n",
    "                    # Handle token-level embeddings by taking mean across tokens\n",
    "                    if len(emb_array.shape) > 1:\n",
    "                        # Average across all but the last dimension\n",
    "                        while len(emb_array.shape) > 1:\n",
    "                            emb_array = np.mean(emb_array, axis=0)\n",
    "                    \n",
    "                    # Ensure we have the right dimension\n",
    "                    if emb_array.shape[0] != EMBEDDING_DIMENSION:\n",
    "                        if emb_array.shape[0] > EMBEDDING_DIMENSION:\n",
    "                            emb_array = emb_array[:EMBEDDING_DIMENSION]\n",
    "                        else:\n",
    "                            padded = np.zeros(EMBEDDING_DIMENSION)\n",
    "                            padded[:emb_array.shape[0]] = emb_array\n",
    "                            emb_array = padded\n",
    "                    \n",
    "                    all_embeddings.append(emb_array.tolist())\n",
    "                    \n",
    "                    # Small delay to prevent overwhelming the endpoint\n",
    "                    if i < len(texts) - 1:\n",
    "                        time.sleep(0.1)\n",
    "                        \n",
    "                    # Success - break the retry loop\n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if retry < max_retries - 1:\n",
    "                        wait_time = (2 ** retry) * 0.5  # Exponential backoff\n",
    "                        print(f\"Retry {retry+1} for text {i+1}: {str(e)}\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"Error generating embedding for text {i+1}: {str(e)}\")\n",
    "                        # Use fallback random vector\n",
    "                        all_embeddings.append(self._create_random_unit_vector())\n",
    "    \n",
    "        return all_embeddings\n",
    "    \n",
    "    def _create_random_unit_vector(self, dim=1024):\n",
    "        \"\"\"Create a random unit vector for fallback\"\"\"\n",
    "        vec = np.random.normal(0, 1, size=dim)\n",
    "        return (vec / np.linalg.norm(vec)).tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def initialize_pinecone():\n",
    "    \"\"\"Initialize Pinecone and return the index\"\"\"\n",
    "    try:\n",
    "        # Initialize Pinecone client\n",
    "        from pinecone import Pinecone\n",
    "        pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        \n",
    "        # Check if the index exists\n",
    "        existing_indexes = pc.list_indexes().names()\n",
    "        print(f\"Available Pinecone indexes: {existing_indexes}\")\n",
    "        \n",
    "        if PINECONE_INDEX_NAME not in existing_indexes:\n",
    "            print(f\"Creating new index '{PINECONE_INDEX_NAME}'...\")\n",
    "            \n",
    "            # Create the index with the metadata fields we want to be searchable\n",
    "            pc.create_index(\n",
    "                name=PINECONE_INDEX_NAME,\n",
    "                dimension=EMBEDDING_DIMENSION,\n",
    "                metric=\"cosine\",\n",
    "                metadata_config={\n",
    "                    \"indexed\": [\n",
    "                        \"emp_type\",\n",
    "                        \"job_title\",\n",
    "                        \"exp_level\",\n",
    "                        \"domain\",\n",
    "                        \"location\",\n",
    "                        \"visa_sponsor\",\n",
    "                        \"salary_range_from\",\n",
    "                        \"salary_range_to\"\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            print(f\"Index '{PINECONE_INDEX_NAME}' created successfully\")\n",
    "        \n",
    "        # Connect to the index\n",
    "        index = pc.Index(PINECONE_INDEX_NAME)\n",
    "        print(f\"Connected to Pinecone index: {PINECONE_INDEX_NAME}\")\n",
    "        \n",
    "        return index\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Pinecone: {str(e)}\")\n",
    "        print(\"Please check your API key and environment settings.\")\n",
    "        return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_job_descriptions(limit=101, bucket=\"mirra-matcher-325\", prefix=\"data/processed/jobs/\"):\n",
    "    \"\"\"\n",
    "    Load job descriptions from S3.\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of jobs to load\n",
    "        bucket: S3 bucket name\n",
    "        prefix: S3 prefix for job files\n",
    "        \n",
    "    Returns:\n",
    "        List of job description dictionaries\n",
    "    \"\"\"\n",
    "    jobs = []\n",
    "    \n",
    "    # Load from S3\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading job descriptions from S3: s3://{bucket}/{prefix}\")\n",
    "        response = s3_client.list_objects_v2(\n",
    "            Bucket=bucket,\n",
    "            Prefix=prefix,\n",
    "            MaxKeys=limit\n",
    "        )\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith(\".json\"):\n",
    "                    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "                    job_data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "                    # Add job_id if not present\n",
    "                    if \"job_id\" not in job_data:\n",
    "                        job_data[\"job_id\"] = key.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "                    jobs.append(job_data)\n",
    "                    print(f\"Loaded job: {job_data.get('job_id')}\")\n",
    "        else:\n",
    "            print(f\"No job files found in S3 bucket {bucket}/{prefix}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading files from S3: {str(e)}\")\n",
    "\n",
    "    print(f\"Loaded {len(jobs)} job descriptions from S3\")\n",
    "    return jobs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def format_job_for_embedding(job):\n",
    "    \"\"\"\n",
    "    Format a job JSON into a comprehensive text representation for embedding.\n",
    "    \n",
    "    Args:\n",
    "        job: Job description dictionary\n",
    "    \n",
    "    Returns:\n",
    "        String representation of the job with all key attributes\n",
    "    \"\"\"\n",
    "    # Extract job details\n",
    "    details = job.get(\"details\", {})\n",
    "    \n",
    "    # Extract each attribute, handling potential missing data\n",
    "    # Job title\n",
    "    job_title_data = details.get(\"job_title\", [\"Unknown\"]) \n",
    "    job_title = job_title_data[0] if isinstance(job_title_data, list) and job_title_data else \"Unknown\"\n",
    "    \n",
    "    # Location handling\n",
    "    location_data = details.get(\"location\", [])\n",
    "    location = \"Remote\"  # Default\n",
    "    \n",
    "    if isinstance(location_data, list) and location_data:\n",
    "        location_item = location_data[0]\n",
    "        if isinstance(location_item, dict):\n",
    "            city = location_item.get(\"city\", \"\")\n",
    "            state = location_item.get(\"state\", \"\")\n",
    "            country = location_item.get(\"country\", \"\")\n",
    "            location = \", \".join(filter(None, [city, state, country]))\n",
    "    \n",
    "    # Company name\n",
    "    company_data = details.get(\"company_name\", [])\n",
    "    company_name = company_data[0] if isinstance(company_data, list) and company_data else \"Unknown\"\n",
    "    \n",
    "    # Employment type - Default to \"Full-time\" if empty\n",
    "    employment_data = details.get(\"employment_type\", [])\n",
    "    if not employment_data and details.get(\"tax_terms\"):\n",
    "        # Use tax_terms if employment_type is empty\n",
    "        employment_data = details.get(\"tax_terms\", [\"Full-time\"])\n",
    "    employment_type = employment_data[0] if isinstance(employment_data, list) and employment_data else \"Full-time\"\n",
    "    \n",
    "    # Experience level (from required years in hard skills)\n",
    "    experience_level = \"Entry-level\"  # Default\n",
    "    if job.get(\"mandatory\", {}).get(\"hard_skills\"):\n",
    "        max_years = 0\n",
    "        for skill in job[\"mandatory\"][\"hard_skills\"]:\n",
    "            min_years_data = skill.get(\"minyears\", [0])\n",
    "            min_years = min_years_data[0] if isinstance(min_years_data, list) and min_years_data else 0\n",
    "            \n",
    "            # Convert to numeric if needed\n",
    "            if not isinstance(min_years, (int, float)):\n",
    "                try:\n",
    "                    min_years = float(min_years)\n",
    "                except (ValueError, TypeError):\n",
    "                    min_years = 0\n",
    "            \n",
    "            max_years = max(max_years, min_years)\n",
    "        \n",
    "        if max_years >= 7:\n",
    "            experience_level = \"Senior\"\n",
    "        elif max_years >= 3:\n",
    "            experience_level = \"Mid-level\"\n",
    "        else:\n",
    "            experience_level = \"Entry-level\"\n",
    "    \n",
    "    # Try to get salary information from wage field\n",
    "    wage_data = details.get(\"wage\", [])\n",
    "    salary_range = \"Not specified\"\n",
    "    salary_from = 0\n",
    "    salary_to = 0\n",
    "    \n",
    "    # Parse company industry/domain\n",
    "    company_industry = details.get(\"company_industry\", [])\n",
    "    domain = company_industry[0] if isinstance(company_industry, list) and company_industry else \"Technology\"\n",
    "    \n",
    "    # Check if work_authorization indicates visa sponsorship\n",
    "    work_authorization = details.get(\"work_authorization\", [])\n",
    "    visa_sponsorship = \"No\"  # Default\n",
    "    \n",
    "    # Construct job text that includes responsibilities and required skills\n",
    "    skills_text = \"\"\n",
    "    if job.get(\"mandatory\", {}).get(\"hard_skills\"):\n",
    "        skills_list = []\n",
    "        for skill_item in job[\"mandatory\"][\"hard_skills\"]:\n",
    "            if skill_item.get(\"skill\"):\n",
    "                for skill_group in skill_item[\"skill\"]:\n",
    "                    if isinstance(skill_group, list):\n",
    "                        skills_list.append(\" \".join(skill_group))\n",
    "                    else:\n",
    "                        skills_list.append(skill_group)\n",
    "        if skills_list:\n",
    "            skills_text = \"Required skills: \" + \", \".join(skills_list)\n",
    "    \n",
    "    # Construct responsibilities text\n",
    "    responsibilities_text = \"\"\n",
    "    if job.get(\"responsibility\", {}).get(\"hard_skills\"):\n",
    "        resp_list = []\n",
    "        for resp_item in job[\"responsibility\"][\"hard_skills\"]:\n",
    "            if resp_item.get(\"skill\"):\n",
    "                for resp_group in resp_item[\"skill\"]:\n",
    "                    if isinstance(resp_group, list):\n",
    "                        resp_list.append(\" \".join(resp_group))\n",
    "                    else:\n",
    "                        resp_list.append(resp_group)\n",
    "        if resp_list:\n",
    "            responsibilities_text = \"Responsibilities: \" + \", \".join(resp_list)\n",
    "    \n",
    "    # Combine all attributes into a comprehensive text\n",
    "    job_text = f\"\"\"\n",
    "Job Title: {job_title}\n",
    "Company: {company_name}\n",
    "Location: {location}\n",
    "Employment Type: {employment_type}\n",
    "Experience Level: {experience_level}\n",
    "Industry/Domain: {domain}\n",
    "Visa Sponsorship: {visa_sponsorship}\n",
    "\n",
    "{skills_text}\n",
    "\n",
    "{responsibilities_text}\n",
    "\n",
    "Job Description Summary:\n",
    "This is a {employment_type} position for a {job_title} located in {location}. \n",
    "The role requires {experience_level} experience in the {domain} industry.\n",
    "Visa sponsorship is {visa_sponsorship}.\n",
    "\"\"\"\n",
    "    \n",
    "    return job_text.strip(), {\n",
    "        \"job_id\": job.get(\"job_id\", str(uuid.uuid4())),\n",
    "        \"job_title\": job_title,\n",
    "        \"emp_type\": employment_type,\n",
    "        \"exp_level\": experience_level,\n",
    "        \"domain\": domain,\n",
    "        \"location\": location,\n",
    "        \"salary_range_from\": salary_from,\n",
    "        \"salary_range_to\": salary_to,\n",
    "        \"visa_sponsor\": visa_sponsorship\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def process_and_upload_jobs(jobs, pinecone_index, batch_size=1, max_retries=3):\n",
    "    \"\"\"\n",
    "    Process jobs, generate embeddings, and upload to Pinecone\n",
    "    \n",
    "    Args:\n",
    "        jobs: List of job dictionaries\n",
    "        pinecone_index: Pinecone index instance\n",
    "        batch_size: Batch size for processing (keep low for stability)\n",
    "        max_retries: Maximum retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with processing statistics\n",
    "    \"\"\"\n",
    "    if not jobs:\n",
    "        print(\"No jobs provided for processing\")\n",
    "        return {\"status\": \"error\", \"reason\": \"no_jobs\"}\n",
    "    \n",
    "    if not pinecone_index:\n",
    "        print(\"No Pinecone index provided\")\n",
    "        return {\"status\": \"error\", \"reason\": \"no_index\"}\n",
    "    \n",
    "    print(f\"Processing {len(jobs)} jobs...\")\n",
    "    \n",
    "    # Initialize the embedder with conservative settings\n",
    "    embedder = ResilientSageMakerEmbedder(\n",
    "        endpoint_name=SAGEMAKER_ENDPOINT_NAME,\n",
    "        max_text_length=512,  # Limit text length to prevent memory issues\n",
    "        region=AWS_REGION\n",
    "    )\n",
    "    \n",
    "    # Process all jobs\n",
    "    job_texts = []\n",
    "    job_metadata = []\n",
    "    \n",
    "    for job in jobs:\n",
    "        try:\n",
    "            # Format the job and extract metadata\n",
    "            job_text, metadata = format_job_for_embedding(job)\n",
    "            job_texts.append(job_text)\n",
    "            job_metadata.append(metadata)\n",
    "            print(f\"Processed job {metadata['job_id']} - {metadata['job_title']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job {job.get('job_id', 'unknown')}: {str(e)}\")\n",
    "    \n",
    "    vectors_uploaded = 0\n",
    "    total_vectors = len(job_texts)  \n",
    "    \n",
    "    with tqdm(total=total_vectors, desc=\"Processing and uploading\") as progress_bar:\n",
    "        for i in range(0, total_vectors, batch_size):\n",
    "            batch_texts = job_texts[i:i+batch_size]\n",
    "            batch_metadata = job_metadata[i:i+batch_size]\n",
    "            \n",
    "            print(f\"Processing batch {i//batch_size + 1}/{(total_vectors-1)//batch_size + 1}...\")\n",
    "            \n",
    "            try:\n",
    "                # Process strictly one text at a time with explicit error handling\n",
    "                batch_embeddings = []\n",
    "                for idx, text in enumerate(batch_texts):\n",
    "                    try:\n",
    "                        print(f\"Generating embedding for text {i+idx+1}/{total_vectors}\")\n",
    "                        embedding = embedder.generate_embeddings([text])[0]\n",
    "                        batch_embeddings.append(embedding)\n",
    "                        print(f\"Successfully generated embedding: length={len(embedding)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR generating embedding for text {i+idx+1}: {str(e)}\")\n",
    "                        # Use fallback vector and continue\n",
    "                        fallback_vector = embedder._create_random_unit_vector()\n",
    "                        batch_embeddings.append(fallback_vector)\n",
    "                        print(f\"Using fallback vector instead\")\n",
    "                    time.sleep(0.5)  # Brief pause between embeddings\n",
    "                \n",
    "                # Create vectors - with explicit length checks\n",
    "                vectors_to_upload = []\n",
    "                for j, embedding in enumerate(batch_embeddings):\n",
    "                    if not embedding or len(embedding) != EMBEDDING_DIMENSION:\n",
    "                        print(f\"WARNING: Invalid embedding at index {j}, using fallback\")\n",
    "                        embedding = embedder._create_random_unit_vector()\n",
    "                    \n",
    "                    if j < len(batch_metadata):\n",
    "                        job_id = batch_metadata[j]['job_id']\n",
    "                        vector_id = f\"job_{job_id}\"\n",
    "                        \n",
    "                        # Ensure all metadata values are properly formatted\n",
    "                        metadata_copy = {}\n",
    "                        for key, value in batch_metadata[idx].items():\n",
    "                            if key in [\"salary_range_from\", \"salary_range_to\"]:\n",
    "                                metadata_copy[key] = int(value) if isinstance(value, (int, float)) else 0\n",
    "                            else:\n",
    "                                metadata_copy[key] = str(value) if not isinstance(value, str) else value\n",
    "                        \n",
    "                        vectors_to_upload.append({\n",
    "                            \"id\": vector_id,\n",
    "                            \"values\": embedding,\n",
    "                            \"metadata\": metadata_copy\n",
    "                        })\n",
    "                \n",
    "                # Print vector counts for clarity\n",
    "                print(f\"Prepared {len(vectors_to_upload)} vectors for upload\")\n",
    "                \n",
    "                # Explicit upload with better error logging\n",
    "                if vectors_to_upload:\n",
    "                    upload_success = False\n",
    "                    for retry in range(max_retries):\n",
    "                        try:\n",
    "                            pinecone_index.upsert(vectors=vectors_to_upload)\n",
    "                            vectors_uploaded += len(vectors_to_upload)\n",
    "                            progress_bar.update(len(vectors_to_upload))\n",
    "                            upload_success = True\n",
    "                            print(f\"Successfully uploaded {len(vectors_to_upload)} vectors\")\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            print(f\"Upload attempt {retry+1} failed: {str(e)}\")\n",
    "                            import traceback\n",
    "                            traceback.print_exc()\n",
    "                            if retry < max_retries - 1:\n",
    "                                wait_time = (2 ** retry) * 2.0  # Longer wait time\n",
    "                                print(f\"Waiting {wait_time}s before retry...\")\n",
    "                                time.sleep(wait_time)\n",
    "                    \n",
    "                    if not upload_success:\n",
    "                        print(\"WARNING: Failed to upload batch after all retries\")\n",
    "                else:\n",
    "                    print(\"No vectors to upload for this batch\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Critical error in batch {i//batch_size + 1}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    return {\n",
    "        \"total_jobs\": len(jobs),\n",
    "        \"vectors_uploaded\": vectors_uploaded\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def job_embedding_pipeline(limit=101):\n",
    "    \"\"\"\n",
    "    Main pipeline to load, process, and upload job embeddings\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of jobs to process\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with processing statistics\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize Pinecone\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    if not pinecone_index:\n",
    "        return {\"status\": \"error\", \"reason\": \"Failed to initialize Pinecone\"}\n",
    "    \n",
    "    # Load jobs\n",
    "    jobs = load_job_descriptions(limit=limit)\n",
    "    if not jobs:\n",
    "        return {\"status\": \"error\", \"reason\": \"No jobs loaded\"}\n",
    "    \n",
    "    # Process and upload jobs\n",
    "    result = process_and_upload_jobs(jobs, pinecone_index, batch_size=1)\n",
    "    \n",
    "    # Get final index stats\n",
    "    try:\n",
    "        stats = pinecone_index.describe_index_stats()\n",
    "        total_vectors = stats.get('total_vector_count', 0)\n",
    "        print(f\"\\nFinal index statistics:\")\n",
    "        print(f\"  - Total vectors: {total_vectors}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting index stats: {str(e)}\")\n",
    "        total_vectors = result.get(\"vectors_uploaded\", 0)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"jobs_processed\": len(jobs),\n",
    "        \"vectors_uploaded\": result.get(\"vectors_uploaded\", 0),\n",
    "        \"total_vectors_in_index\": total_vectors,\n",
    "        \"processing_time_seconds\": processing_time\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run the pipeline\n",
    "    result = job_embedding_pipeline(limit=101)  \n",
    "    print(f\"\\nPipeline results: {result}\")\n",
    "    \n",
    "    # Example search\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    if pinecone_index:\n",
    "        embedder = ResilientSageMakerEmbedder(\n",
    "            endpoint_name=SAGEMAKER_ENDPOINT_NAME,\n",
    "            region=AWS_REGION\n",
    "        )\n",
    "        \n",
    "        # Search for data science jobs that sponsor visas\n",
    "        search_results = search_jobs(\n",
    "            query_text=\"Data Science\",\n",
    "            pinecone_index=pinecone_index,\n",
    "            embedder=embedder,\n",
    "            filters={\"visa_sponsor\": \"Yes\"}\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSearch results:\")\n",
    "        for i, result in enumerate(search_results):\n",
    "            print(f\"{i+1}. {result['job_title']} ({result['location']}) - Score: {result['similarity_score']:.4f}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def search_jobs(query_text, pinecone_index, embedder=None, top_k=10, filters=None):\n",
    "    \"\"\"\n",
    "    Search for jobs using semantic embedding similarity\n",
    "    \n",
    "    Args:\n",
    "        query_text: Query text to search for\n",
    "        pinecone_index: Pinecone index to search\n",
    "        embedder: Embedding generator (optional)\n",
    "        top_k: Number of results to return\n",
    "        filters: Dictionary of metadata filters\n",
    "        \n",
    "    Returns:\n",
    "        List of job matches with scores\n",
    "    \"\"\"\n",
    "    if not embedder:\n",
    "        embedder = ResilientSageMakerEmbedder(\n",
    "            endpoint_name=SAGEMAKER_ENDPOINT_NAME,\n",
    "            region=AWS_REGION\n",
    "        )\n",
    "    \n",
    "    # Generate embedding for query\n",
    "    query_embedding = embedder.generate_embeddings([query_text])[0]\n",
    "    \n",
    "    # Prepare filters if any\n",
    "    filter_dict = {}\n",
    "    if filters:\n",
    "        for key, value in filters.items():\n",
    "            if value:  # Only add non-empty filters\n",
    "                # Handle numeric values for salary filters\n",
    "                if key in [\"salary_range_from\", \"salary_range_to\"] and isinstance(value, (int, float)):\n",
    "                    filter_dict[key] = {\"$gte\": value} if key == \"salary_range_from\" else {\"$lte\": value}\n",
    "                else:\n",
    "                    filter_dict[key] = value\n",
    "    \n",
    "    print(f\"Searching with filters: {filter_dict}\")\n",
    "    \n",
    "    # Perform search\n",
    "    try:\n",
    "        search_results = pinecone_index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            filter=filter_dict if filter_dict else None\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        \n",
    "        # For Pinecone v6+ compatibility\n",
    "        matches = search_results.get('matches', [])\n",
    "        if hasattr(search_results, 'matches'):\n",
    "            matches = search_results.matches\n",
    "        \n",
    "        for match in matches:\n",
    "            # Handle different response formats\n",
    "            if hasattr(match, 'metadata'):\n",
    "                metadata = match.metadata\n",
    "                score = match.score\n",
    "            else:\n",
    "                metadata = match.get('metadata', {})\n",
    "                score = match.get('score', 0)\n",
    "            \n",
    "            results.append({\n",
    "                'job_id': metadata.get('job_id', 'unknown'),\n",
    "                'job_title': metadata.get('job_title', 'Unknown'),\n",
    "                'location': metadata.get('location', 'Unknown'),\n",
    "                'emp_type': metadata.get('emp_type', 'Unknown'),\n",
    "                'exp_level': metadata.get('exp_level', 'Unknown'),\n",
    "                'domain': metadata.get('domain', 'Unknown'),\n",
    "                'visa_sponsor': metadata.get('visa_sponsor', 'No'),\n",
    "                'similarity_score': score\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching jobs: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}