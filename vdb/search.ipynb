{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#03_embedding_generation\n",
    "- Sets up the embedding generator using SageMaker\n",
    "- Loads job descriptions from files\n",
    "- Creates chunks from job descriptions using a custom chunker\n",
    "- Generates embeddings for each chunk\n",
    "- Uploads the embedded vectors to Pinecone\n",
    "- Includes batch processing for efficiency"
   ],
   "metadata": {
    "id": "dynvT9cEdx4s"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip install pinecone"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pinecone in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (6.0.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pinecone) (2025.1.31)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pinecone) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pinecone) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!pip show pinecone"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Name: pinecone\n",
      "Version: 6.0.2\n",
      "Summary: Pinecone client and SDK\n",
      "Home-page: https://www.pinecone.io\n",
      "Author: Pinecone Systems, Inc.\n",
      "Author-email: support@pinecone.io\n",
      "License: Apache-2.0\n",
      "Location: /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages\n",
      "Requires: certifi, pinecone-plugin-interface, python-dateutil, typing-extensions, urllib3\n",
      "Required-by: \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Any, Union \n",
    "from tqdm import tqdm\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "Eh1Rdi4NVCIP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# SageMaker endpoint configuration\n",
    "\n",
    "SAGEMAKER_ENDPOINT_NAME = \"e5-embeddings-pooled-2\" \n",
    "AWS_REGION = \"us-east-1\"\n",
    "\n",
    "# Set Pinecone credentials directly\n",
    "PINECONE_API_KEY = \"\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\" # matched with AWS region\n",
    "PINECONE_INDEX_NAME = \"sample-100-strings\" ## important note - \"sample-100-strings\" if you don't need job_id to avoid dups, and \"sample-100\" if you may need strings by job_id\n",
    "\n",
    "EMBEDDING_DIMENSION = 1024\n",
    "\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIMENSION}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Embedding dimension: 1024\n"
     ]
    }
   ],
   "metadata": {
    "id": "7rXs4HpmWK-w"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Initialize Pinecone with credentials\n",
    "from pinecone import Pinecone\n",
    "\n",
    "def initialize_pinecone():\n",
    "    \"\"\"Initialize Pinecone and return the index\"\"\"\n",
    "    try:\n",
    "        # Initialize Pinecone client\n",
    "        pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        \n",
    "        # Check if the index exists\n",
    "        existing_indexes = pc.list_indexes().names()\n",
    "        print(f\"Available Pinecone indexes: {existing_indexes}\")\n",
    "        \n",
    "        if PINECONE_INDEX_NAME not in existing_indexes:\n",
    "            print(f\"Creating new index '{PINECONE_INDEX_NAME}'...\")\n",
    "            \n",
    "            # Create the index\n",
    "            pc.create_index(\n",
    "                name=PINECONE_INDEX_NAME,\n",
    "                dimension=EMBEDDING_DIMENSION,\n",
    "                metric=\"cosine\",\n",
    "                metadata_config={\n",
    "                    \"indexed\": [\n",
    "                        \"source_type\",\n",
    "                        \"requirement_level\",\n",
    "                        \"job_id\",\n",
    "                        \"resume_id\"\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            print(f\"Index '{PINECONE_INDEX_NAME}' created successfully\")\n",
    "        \n",
    "        # Connect to the index\n",
    "        index = pc.Index(PINECONE_INDEX_NAME)\n",
    "        print(f\"Connected to Pinecone index: {PINECONE_INDEX_NAME}\")\n",
    "        \n",
    "        return index\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Pinecone: {str(e)}\")\n",
    "        print(\"Please check your API key and environment settings.\")\n",
    "        # Return None to indicate initialization failed\n",
    "        return None\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone_index = initialize_pinecone()\n",
    "\n",
    "if pinecone_index:\n",
    "    # Check index stats\n",
    "    index_stats = pinecone_index.describe_index_stats()\n",
    "    # Print directly to avoid serialization issues\n",
    "    print(f\"Index statistics:\")\n",
    "    print(index_stats)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Available Pinecone indexes: ['mirra-embeddings', 'sample-100-strings', 'sample-100', 'mirra-filtering', 'mirra']\n",
      "Connected to Pinecone index: sample-100-strings\n",
      "Index statistics:\n",
      "{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 4480}},\n",
      " 'total_vector_count': 4480,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class ResilientSageMakerEmbedder:\n",
    "    \"\"\"\n",
    "    A wrapper around SageMaker embedding endpoints with resilience features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint_name, max_text_length=512, region=\"us-east-1\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedder with a SageMaker endpoint.\n",
    "        \n",
    "        Args:\n",
    "            endpoint_name: The name of the SageMaker endpoint\n",
    "            max_text_length: Maximum text length to truncate to\n",
    "            region: AWS region for the endpoint\n",
    "        \"\"\"\n",
    "        import boto3\n",
    "        import json\n",
    "        import numpy as np\n",
    "        \n",
    "        self.sagemaker_runtime = boto3.client('sagemaker-runtime', region_name=region)\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.max_text_length = max_text_length\n",
    "        print(f\"Initialized ResilientSageMakerEmbedder for endpoint: {endpoint_name}\")\n",
    "\n",
    "    def _prepare_text(self, text):\n",
    "            \"\"\"Clean and prepare text for the embedding model.\"\"\"\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)\n",
    "            \n",
    "            # Remove excessive whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "            # Add E5 prefix\n",
    "            if not text.startswith(\"passage:\"):\n",
    "                text = f\"passage: {text}\"\n",
    "            \n",
    "            # Truncate if needed\n",
    "            if len(text) > self.max_text_length:\n",
    "                text = text[:self.max_text_length]\n",
    "                \n",
    "            return text\n",
    "    def generate_embeddings(self, texts):\n",
    "        \"\"\"Generate embeddings using SageMaker E5 endpoint\"\"\"\n",
    "        # Ensure texts is a list\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "            \n",
    "        try:\n",
    "            # Prepare input for E5 model\n",
    "            prepared_texts = [self._prepare_text(text) for text in texts]\n",
    "            \n",
    "            # Prepare payload with explicit pooling parameters\n",
    "            payload = {\n",
    "                \"inputs\": prepared_texts,\n",
    "                \"parameters\": {\n",
    "                    \"normalize\": True,\n",
    "                    \"pooling\": \"mean\",\n",
    "                    \"return_sentence_embedding\": True\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Call SageMaker endpoint\n",
    "            response = self.sagemaker_runtime.invoke_endpoint(\n",
    "                EndpointName=self.endpoint_name,\n",
    "                ContentType='application/json',\n",
    "                Body=json.dumps(payload)\n",
    "            )\n",
    "            \n",
    "            # Parse response\n",
    "            response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
    "            \n",
    "            # Process embeddings with proper pooling\n",
    "            embeddings = []\n",
    "            for emb in response_body:\n",
    "                emb_array = np.array(emb)\n",
    "                \n",
    "                # Handle token-level embeddings by taking mean across tokens\n",
    "                if len(emb_array.shape) > 1:\n",
    "                    # Average across all but the last dimension\n",
    "                    while len(emb_array.shape) > 1:\n",
    "                        emb_array = np.mean(emb_array, axis=0)\n",
    "                \n",
    "                # Ensure we have the right dimension (1024)\n",
    "                if emb_array.shape[0] != EMBEDDING_DIMENSION:\n",
    "                    if emb_array.shape[0] > EMBEDDING_DIMENSION:\n",
    "                        emb_array = emb_array[:EMBEDDING_DIMENSION]\n",
    "                    else:\n",
    "                        padded = np.zeros(EMBEDDING_DIMENSION)\n",
    "                        padded[:emb_array.shape[0]] = emb_array\n",
    "                        emb_array = padded\n",
    "                \n",
    "                embeddings.append(emb_array.tolist())\n",
    "            \n",
    "            return embeddings\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {str(e)}\")\n",
    "            return [self._create_random_unit_vector() for _ in range(len(texts))]\n",
    "    \n",
    "    def _create_random_unit_vector(self, dim=1024):\n",
    "        \"\"\"Create a random unit vector for fallback\"\"\"\n",
    "        import numpy as np\n",
    "        vec = np.random.normal(0, 1, size=dim)\n",
    "        return (vec / np.linalg.norm(vec)).tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def job_id_retrieval(pinecone_index):\n",
    "    job_ids = set()\n",
    "    stats = pinecone_index.describe_index_stats()\n",
    "    total_vectors = stats.get('total_vector_count', 0)\n",
    "    \n",
    "    # Try different metadata key variations\n",
    "    potential_keys = ['job_id', 'jobId', 'id', 'job_identifier']\n",
    "    \n",
    "    for namespace, ns_stats in stats.get('namespaces', {}).items():\n",
    "        print(f\"Checking namespace: {namespace}\")\n",
    "        try:\n",
    "            # Fetch vectors in this namespace\n",
    "            for key in potential_keys:\n",
    "                namespace_vectors = pinecone_index.query(\n",
    "                    vector=[0]*EMBEDDING_DIMENSION, \n",
    "                    top_k=total_vectors, \n",
    "                    namespace=namespace,\n",
    "                    filter={key: {'$exists': True}},\n",
    "                    include_metadata=True\n",
    "                )\n",
    "                \n",
    "                for match in namespace_vectors.get('matches', []):\n",
    "                    metadata = match.get('metadata', {})\n",
    "                    for potential_key in potential_keys:\n",
    "                        if potential_key in metadata:\n",
    "                            job_ids.add(metadata[potential_key])\n",
    "        except Exception as e:\n",
    "            print(f\"Error in namespace {namespace}: {e}\")\n",
    "    \n",
    "    return sorted(list(job_ids))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Execute and print results\n",
    "if pinecone_index:\n",
    "    print(\"Querying Pinecone for uploaded job IDs...\")\n",
    "    job_ids = job_id_retrieval(pinecone_index)\n",
    "    \n",
    "    if job_ids:\n",
    "        print(f\"\\nFound {len(job_ids)} unique job IDs in Pinecone:\")\n",
    "        for i, job_id in enumerate(job_ids):\n",
    "            print(f\"{i+1}. {job_id}\")\n",
    "    else:\n",
    "        print(\"No job IDs found in Pinecone\")\n",
    "else:\n",
    "    print(\"Pinecone index not initialized\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Querying Pinecone for uploaded job IDs...\n",
      "Checking namespace: \n",
      "No job IDs found in Pinecone\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "embedder = ResilientSageMakerEmbedder(\n",
    "        endpoint_name=SAGEMAKER_ENDPOINT_NAME,\n",
    "        region=AWS_REGION\n",
    "    )\n",
    "\n",
    "def semantic_search(query_title, pinecone_index, embedder=None, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Search for semantic similarity from vdb.\n",
    "    \n",
    "    Args:\n",
    "        query_title: The text to search for\n",
    "        pinecone_index: The Pinecone index to search\n",
    "        embedder: Embedding generator instance (optional)\n",
    "        similarity_threshold: Minimum similarity score (0-1) to include in results\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with job_id, text, and similarity score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create embedder if not provided\n",
    "        if embedder is None:\n",
    "            from random_embedding import RandomEmbeddingGenerator\n",
    "            print(\"No embedder provided. Creating RandomEmbeddingGenerator as fallback.\")\n",
    "            embedder = RandomEmbeddingGenerator()\n",
    "            \n",
    "        query_text = f\"{query_title}\"  \n",
    "        query_embedding = embedder.generate_embeddings([query_text])[0]\n",
    "        \n",
    "        search_results = pinecone_index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=100,  # Get enough results to filter\n",
    "            include_metadata=True,\n",
    "            include_values=False  # only need the scores, not the vectors\n",
    "        )\n",
    "        \n",
    "        similar_text = []\n",
    "        \n",
    "        # Pinecone v6+ returns objects instead of dictionaries\n",
    "        matches = getattr(search_results, 'matches', [])\n",
    "        \n",
    "        for match in matches:\n",
    "            # Extract match properties\n",
    "            score = getattr(match, 'score', 0)\n",
    "            metadata = getattr(match, 'metadata', {})\n",
    "            \n",
    "            # Skip if below threshold\n",
    "            if score < similarity_threshold:\n",
    "                continue\n",
    "                \n",
    "            # Extract job info from metadata\n",
    "            job_id = metadata.get('job_id', 'unknown')\n",
    "            text = metadata.get('text', metadata.get('text', '')).replace('Text: ', '')\n",
    "            \n",
    "            similar_text.append({\n",
    "                'job_id': job_id,\n",
    "                'text': text,\n",
    "                'similarity': score\n",
    "            })\n",
    "        \n",
    "        return similar_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in semantic search: {str(e)}\")\n",
    "        return []"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initialized ResilientSageMakerEmbedder for endpoint: e5-embeddings-pooled-2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Test the function with a sample job title\n",
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "pinecone_index = pc.Index(PINECONE_INDEX_NAME)\n",
    "\n",
    "test_query = \"Python\"\n",
    "results = semantic_search(\n",
    "    test_query, \n",
    "    pinecone_index=pinecone_index,\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "print(f\"Text similar to '{test_query}' (similarity > 0.8):\")\n",
    "for job in results:\n",
    "    print(f\"- {job['text']}, Score: {job['similarity']:.4f})\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text similar to 'Python' (similarity > 0.8):\n",
      "- Python, Score: 1.0000)\n",
      "- Python Scripting, Score: 0.9769)\n",
      "- Python development, Score: 0.9587)\n",
      "- Use Python, Score: 0.9516)\n",
      "- Numpy, Score: 0.9443)\n",
      "- PySpark, Score: 0.9364)\n",
      "- Scala, Score: 0.9331)\n",
      "- Perl, Score: 0.9291)\n",
      "- Typescript, Score: 0.9243)\n",
      "- Leverage PySpark, Score: 0.9214)\n",
      "- Bash, Score: 0.9189)\n",
      "- Kotlin, Score: 0.9187)\n",
      "- C#, Score: 0.9171)\n",
      "- Various scripting languages, Score: 0.9163)\n",
      "- TypeScript, Score: 0.9162)\n",
      "- VB.NET, Score: 0.9157)\n",
      "- C++, Score: 0.9139)\n",
      "- Utilize programming languages, Score: 0.9137)\n",
      "- Pandas, Score: 0.9126)\n",
      "- PowerShell, Score: 0.9124)\n",
      "- JavaScript, Score: 0.9105)\n",
      "- Bash scripting, Score: 0.9099)\n",
      "- Powershell, Score: 0.9090)\n",
      "- Ruby, Score: 0.9087)\n",
      "- Java, Score: 0.9078)\n",
      "- Node.JS, Score: 0.9066)\n",
      "- Javascript, Score: 0.9065)\n",
      "- UNIX Shell Scripting, Score: 0.9052)\n",
      "- CS, Score: 0.9049)\n",
      "- .NET Core C#, Score: 0.9031)\n",
      "- .NET, Score: 0.9020)\n",
      "- .Net, Score: 0.9018)\n",
      "- JAVA, Score: 0.9018)\n",
      "- Postgres, Score: 0.9009)\n",
      "- C++ coding, Score: 0.9004)\n",
      "- Shell scripting, Score: 0.8999)\n",
      "- VB, Score: 0.8989)\n",
      "- UNIX, Score: 0.8986)\n",
      "- Programming, Score: 0.8984)\n",
      "- Lambda, Score: 0.8983)\n",
      "- Scripting, Score: 0.8976)\n",
      "- all contemporary programming languages, Score: 0.8976)\n",
      "- Linux, Score: 0.8971)\n",
      "- Databricks, Score: 0.8960)\n",
      "- YAML, Score: 0.8959)\n",
      "- T-SQL, Score: 0.8958)\n",
      "- SQL, Score: 0.8956)\n",
      "- Jenkins CI, Score: 0.8954)\n",
      "- OpenText, Score: 0.8949)\n",
      "- Oracle PL/SQL, Score: 0.8933)\n",
      "- Scripting in PowerShell, Score: 0.8931)\n",
      "- IDE, Score: 0.8930)\n",
      "- Power Automate, Score: 0.8926)\n",
      "- Oracle SQL, Score: 0.8925)\n",
      "- MySQL, Score: 0.8917)\n",
      "- Gitlab, Score: 0.8917)\n",
      "- Ansible, Score: 0.8906)\n",
      "- programming languages, Score: 0.8900)\n",
      "- GitLab, Score: 0.8892)\n",
      "- GitHub, Score: 0.8880)\n",
      "- Snow SQL, Score: 0.8879)\n",
      "- .Net Core framework, Score: 0.8878)\n",
      "- Java web, Score: 0.8876)\n",
      "- GitLab CI/CD, Score: 0.8873)\n",
      "- SQL Database, Score: 0.8873)\n",
      "- Microsoft Stack, Score: 0.8868)\n",
      "- Linux+, Score: 0.8868)\n",
      "- SwiftUI, Score: 0.8862)\n",
      "- Programming skills, Score: 0.8862)\n",
      "- AI program, Score: 0.8859)\n",
      "- Java foundation, Score: 0.8856)\n",
      "- .Net framework, Score: 0.8853)\n",
      "- Git, Score: 0.8849)\n",
      "- OpenShift, Score: 0.8844)\n",
      "- scripting and programming languages, Score: 0.8841)\n",
      "- C# development environment, Score: 0.8836)\n",
      "- Visual Studio, Score: 0.8834)\n",
      "- Mongo DB, Score: 0.8834)\n",
      "- Oracle, Score: 0.8828)\n",
      "- jQuery, Score: 0.8826)\n",
      "- Microsoft Excel, Score: 0.8825)\n",
      "- Selenium, Score: 0.8824)\n",
      "- JAVA 1.8, Score: 0.8821)\n",
      "- ASP.Net, Score: 0.8820)\n",
      "- Java applications, Score: 0.8818)\n",
      "- Apache Airflow, Score: 0.8814)\n",
      "- Java development, Score: 0.8814)\n",
      "- NextJS, Score: 0.8813)\n",
      "- Hadoop, Score: 0.8813)\n",
      "- Xcode, Score: 0.8811)\n",
      "- MapReduce, Score: 0.8811)\n",
      "- MS Excel, Score: 0.8808)\n",
      "- Go, Score: 0.8802)\n",
      "- JIRA, Score: 0.8801)\n",
      "- SQL server, Score: 0.8801)\n",
      "- Optimizer, Score: 0.8800)\n",
      "- HTML, Score: 0.8797)\n",
      "- Microsoft Project, Score: 0.8797)\n",
      "- ASP. Net, Score: 0.8797)\n",
      "- OLAP, Score: 0.8792)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}