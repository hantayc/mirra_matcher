{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#03_embedding_generation\n",
    "- Sets up the embedding generator using SageMaker\n",
    "- Extracts and embeds strings from job description jsons\n",
    "- Uploads the embedded vectors to Pinecone\n",
    "- Includes batch processing for efficiency"
   ],
   "metadata": {
    "id": "dynvT9cEdx4s"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install pinecone"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip show pinecone"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Any, Union \n",
    "from tqdm import tqdm\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "Eh1Rdi4NVCIP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SageMaker endpoint configuration\n",
    "\n",
    "SAGEMAKER_ENDPOINT_NAME = \"e5-embeddings-huggingface\" \n",
    "AWS_REGION = \"us-east-1\"\n",
    "\n",
    "# Set Pinecone credentials directly\n",
    "PINECONE_API_KEY = \"pcsk_7VkStS_ifR3SH9d1MSkkju9kP7DUt5M16CpNyzi9dwNBm7iUqyXmbKZWQbC55ZzfSEaAB\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\" # matched with AWS region\n",
    "PINECONE_INDEX_NAME = \"sample-100\"\n",
    "\n",
    "EMBEDDING_DIMENSION = 1024\n",
    "\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIMENSION}\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "7rXs4HpmWK-w"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize Pinecone with credentials\n",
    "from pinecone import Pinecone\n",
    "\n",
    "def initialize_pinecone():\n",
    "    \"\"\"Initialize Pinecone and return the index\"\"\"\n",
    "    try:\n",
    "        # Initialize Pinecone client\n",
    "        pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        \n",
    "        # Check if the index exists\n",
    "        existing_indexes = pc.list_indexes().names()\n",
    "        print(f\"Available Pinecone indexes: {existing_indexes}\")\n",
    "        \n",
    "        if PINECONE_INDEX_NAME not in existing_indexes:\n",
    "            print(f\"Creating new index '{PINECONE_INDEX_NAME}'...\")\n",
    "            \n",
    "            # Create the index\n",
    "            pc.create_index(\n",
    "                name=PINECONE_INDEX_NAME,\n",
    "                dimension=EMBEDDING_DIMENSION,\n",
    "                metric=\"cosine\",\n",
    "                metadata_config={\n",
    "                    \"indexed\": [\n",
    "                        \"source_type\",\n",
    "                        \"requirement_level\",\n",
    "                        \"job_id\",\n",
    "                        \"resume_id\"\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            print(f\"Index '{PINECONE_INDEX_NAME}' created successfully\")\n",
    "        \n",
    "        # Connect to the index\n",
    "        index = pc.Index(PINECONE_INDEX_NAME)\n",
    "        print(f\"Connected to Pinecone index: {PINECONE_INDEX_NAME}\")\n",
    "        \n",
    "        return index\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Pinecone: {str(e)}\")\n",
    "        print(\"Please check your API key and environment settings.\")\n",
    "        # Return None to indicate initialization failed\n",
    "        return None\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone_index = initialize_pinecone()\n",
    "\n",
    "if pinecone_index:\n",
    "    # Check index stats\n",
    "    index_stats = pinecone_index.describe_index_stats()\n",
    "    # Print directly to avoid serialization issues\n",
    "    print(f\"Index statistics:\")\n",
    "    print(index_stats)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class ResilientSageMakerEmbedder:\n",
    "    \"\"\"\n",
    "    A wrapper around SageMaker embedding endpoints with resilience features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint_name, max_text_length=512, region=\"us-east-1\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedder with a SageMaker endpoint.\n",
    "        \n",
    "        Args:\n",
    "            endpoint_name: The name of the SageMaker endpoint\n",
    "            max_text_length: Maximum text length to truncate to\n",
    "            region: AWS region for the endpoint\n",
    "        \"\"\"\n",
    "        import boto3\n",
    "        import json\n",
    "        import numpy as np\n",
    "        \n",
    "        self.sagemaker_runtime = boto3.client('sagemaker-runtime', region_name=region)\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.max_text_length = max_text_length\n",
    "        print(f\"Initialized ResilientSageMakerEmbedder for endpoint: {endpoint_name}\")\n",
    "\n",
    "    def _prepare_text(self, text):\n",
    "            \"\"\"Clean and prepare text for the embedding model.\"\"\"\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)\n",
    "            \n",
    "            # Remove excessive whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "            # Add E5 prefix\n",
    "            if not text.startswith(\"passage:\"):\n",
    "                text = f\"passage: {text}\"\n",
    "            \n",
    "            # Truncate if needed\n",
    "            if len(text) > self.max_text_length:\n",
    "                text = text[:self.max_text_length]\n",
    "                \n",
    "            return text\n",
    "    def generate_embeddings(self, texts):\n",
    "        \"\"\"Generate embeddings using SageMaker E5 endpoint\"\"\"\n",
    "        # Ensure texts is a list\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "            \n",
    "        try:\n",
    "            # Prepare input for E5 model\n",
    "            prepared_texts = [self._prepare_text(text) for text in texts]\n",
    "            \n",
    "            # Prepare payload with explicit pooling parameters\n",
    "            payload = {\n",
    "                \"inputs\": prepared_texts,\n",
    "                \"parameters\": {\n",
    "                    \"normalize\": True,\n",
    "                    \"pooling\": \"mean\",\n",
    "                    \"return_sentence_embedding\": True\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = self.sagemaker_runtime.invoke_endpoint(\n",
    "                EndpointName=self.endpoint_name,\n",
    "                ContentType='application/json',\n",
    "                Body=json.dumps(payload)\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
    "            \n",
    "            embeddings = []\n",
    "            for emb in response_body:\n",
    "                emb_array = np.array(emb)\n",
    "                \n",
    "                if len(emb_array.shape) > 1:\n",
    "                    while len(emb_array.shape) > 1:\n",
    "                        emb_array = np.mean(emb_array, axis=0)\n",
    "                \n",
    "                if emb_array.shape[0] != EMBEDDING_DIMENSION:\n",
    "                    if emb_array.shape[0] > EMBEDDING_DIMENSION:\n",
    "                        emb_array = emb_array[:EMBEDDING_DIMENSION]\n",
    "                    else:\n",
    "                        padded = np.zeros(EMBEDDING_DIMENSION)\n",
    "                        padded[:emb_array.shape[0]] = emb_array\n",
    "                        emb_array = padded\n",
    "                \n",
    "                embeddings.append(emb_array.tolist())\n",
    "            \n",
    "            return embeddings\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {str(e)}\")\n",
    "            return [self._create_random_unit_vector() for _ in range(len(texts))]\n",
    "    \n",
    "    def _create_random_unit_vector(self, dim=1024):\n",
    "        \"\"\"Create a random unit vector for fallback\"\"\"\n",
    "        import numpy as np\n",
    "        vec = np.random.normal(0, 1, size=dim)\n",
    "        return (vec / np.linalg.norm(vec)).tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def run_embedding_pipeline():\n",
    "    \"\"\"\n",
    "    Complete pipeline to load jobs, generate embeddings, and upload to Pinecone\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Verify configuration\n",
    "    if not PINECONE_API_KEY:\n",
    "        print(\"ERROR: Pinecone API key is not set. Please set PINECONE_API_KEY in the configuration cell.\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Configuring embedding pipeline...\")\n",
    "    print(f\"Embedding dimension: {EMBEDDING_DIMENSION}\")\n",
    "    print(f\"Using SageMaker endpoint: {SAGEMAKER_ENDPOINT_NAME}\")\n",
    "    print(f\"Using Pinecone index: {PINECONE_INDEX_NAME}\")\n",
    "    \n",
    "    # Initialize Pinecone if it's not already initialized\n",
    "    if 'pinecone_index' not in globals() or pinecone_index is None:\n",
    "        print(\"Initializing Pinecone...\")\n",
    "        pinecone_index = initialize_pinecone()\n",
    "        if not pinecone_index:\n",
    "            print(\"ERROR: Failed to initialize Pinecone. Check your API key and index settings.\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"Using existing Pinecone connection\")\n",
    "    \n",
    "    # Display index stats\n",
    "    try:\n",
    "        index_stats = pinecone_index.describe_index_stats()\n",
    "        print(f\"Current index statistics:\")\n",
    "        print(f\"  - Total vectors: {index_stats.get('total_vector_count', 0)}\")\n",
    "        print(f\"  - Dimension: {index_stats.get('dimension', 0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not retrieve index stats: {str(e)}\")\n",
    "    \n",
    "    # Load jobs - use existing sample_jobs if available\n",
    "    if 'sample_jobs' in globals() and sample_jobs:\n",
    "        print(f\"Using {len(sample_jobs)} previously loaded jobs\")\n",
    "        jobs = sample_jobs\n",
    "    else:\n",
    "        print(\"Loading new job samples...\")\n",
    "        jobs = load_job_descriptions(limit=101)\n",
    "        if not jobs:\n",
    "            print(\"ERROR: No jobs loaded. Cannot proceed.\")\n",
    "            return False\n",
    "    \n",
    "    # Process jobs and upload\n",
    "    start_time = time.time()\n",
    "    stats = process_jobs_and_upload(jobs, pinecone_index, batch_size=64)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Print stats\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Processed {stats['total_jobs']} jobs\")\n",
    "    print(f\"Total strings extracted: {stats['total_strings']}\")\n",
    "    print(f\"Strings embedded: {stats['strings_embedded']}\")\n",
    "    print(f\"Vectors uploaded to Pinecone: {stats['vectors_uploaded']}\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Verify final index state\n",
    "    try:\n",
    "        final_stats = pinecone_index.describe_index_stats()\n",
    "        print(f\"Final index statistics:\")\n",
    "        print(f\"  - Total vectors: {final_stats.get('total_vector_count', 0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not retrieve final index stats: {str(e)}\")\n",
    "    \n",
    "    return stats"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def load_job_descriptions(limit=37310, skip=0, bucket=\"mirra-matcher-325\", prefix=\"data/processed/jobs/\"):\n",
    "    \"\"\"\n",
    "    Load job descriptions from S3 with pagination support and skip capability.\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of jobs to load\n",
    "        skip: Number of jobs to skip before starting to load\n",
    "        bucket: S3 bucket name\n",
    "        prefix: S3 prefix path\n",
    "        \n",
    "    Returns:\n",
    "        List of job description dictionaries\n",
    "    \"\"\"\n",
    "    jobs = []\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading job descriptions from S3: s3://{bucket}/{prefix}\")\n",
    "        print(f\"Skipping first {skip} jobs, loading up to {limit} jobs\")\n",
    "        \n",
    "        # Initialize pagination parameters\n",
    "        continuation_token = None\n",
    "        has_more = True\n",
    "        jobs_seen = 0\n",
    "        \n",
    "        while has_more and len(jobs) < limit:\n",
    "            # Prepare request parameters\n",
    "            list_params = {\n",
    "                'Bucket': bucket,\n",
    "                'Prefix': prefix,\n",
    "                'MaxKeys': 1000  # Request 1000 keys per request for efficiency\n",
    "            }\n",
    "            \n",
    "            # Add continuation token if we have one\n",
    "            if continuation_token:\n",
    "                list_params['ContinuationToken'] = continuation_token\n",
    "                \n",
    "            # Make the request\n",
    "            response = s3_client.list_objects_v2(**list_params)\n",
    "            \n",
    "            # Process the contents\n",
    "            if 'Contents' in response:\n",
    "                for obj in response['Contents']:\n",
    "                    jobs_seen += 1\n",
    "                    \n",
    "                    # Skip jobs until we reach our starting point\n",
    "                    if jobs_seen <= skip:\n",
    "                        continue\n",
    "                        \n",
    "                    key = obj['Key']\n",
    "                    if key.endswith(\".json\"):\n",
    "                        try:\n",
    "                            response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "                            job_data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "                            # Add job_id if not present\n",
    "                            if \"job_id\" not in job_data:\n",
    "                                job_data[\"job_id\"] = key.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "                            jobs.append(job_data)\n",
    "                            print(f\"Loaded job: {job_data.get('job_id')}\")\n",
    "                            \n",
    "                            # Stop if we've reached the limit\n",
    "                            if len(jobs) >= limit:\n",
    "                                break\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error loading job from {key}: {str(e)}\")\n",
    "            \n",
    "            # Check if there are more items to fetch\n",
    "            has_more = response.get('IsTruncated', False)\n",
    "            continuation_token = response.get('NextContinuationToken')\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading files from S3: {str(e)}\")\n",
    "\n",
    "    print(f\"Loaded {len(jobs)} job descriptions from S3\")\n",
    "    return jobs"
   ],
   "outputs": [],
   "metadata": {
    "id": "J6431PKCWaUC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_jobs = load_job_descriptions(limit=37310)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def safe_preview_job(job):\n",
    "    \"\"\"\n",
    "    Safely preview a job description with fallback values.\n",
    "    \n",
    "    Args:\n",
    "        job (dict): Job description dictionary\n",
    "    \n",
    "    Returns:\n",
    "        dict: Preview of job with safe access to values\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"job_id\": job.get(\"job_id\", \"Unknown\"),\n",
    "        \"job_title\": (\n",
    "            job.get(\"details\", {}).get(\"job_title\", [\"Unknown\"])[0] \n",
    "            if job.get(\"details\", {}).get(\"job_title\") \n",
    "            else \"Unknown\"\n",
    "        ),\n",
    "        \"company_name\": (\n",
    "            job.get(\"details\", {}).get(\"company_name\", [\"Unknown\"])[0] \n",
    "            if job.get(\"details\", {}).get(\"company_name\") \n",
    "            else \"Unknown\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "if sample_jobs:\n",
    "    try:\n",
    "        job_preview = safe_preview_job(sample_jobs[0])\n",
    "        print(\"\\nPreview of first job:\")\n",
    "        print(json.dumps(job_preview, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Error previewing job: {e}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Preview of first job:\n",
      "{\n",
      "  \"job_id\": \"00009cf2-7c46-4365-aa06-f3d142ea4819\",\n",
      "  \"job_title\": \"Information Technology Manager (MSO)\",\n",
      "  \"company_name\": \"AC Wellness Network LLC\"\n",
      "}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def extract_job_strings(job_data):\n",
    "    \"\"\"\n",
    "    Extract all relevant strings from a job description for embedding.\n",
    "    Does not add prefix information to strings.\n",
    "    \n",
    "    Args:\n",
    "        job_data: Job description dictionary\n",
    "        \n",
    "    Returns:\n",
    "        List of unique strings to be embedded\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    def extract_strings(data):\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                # Add the key itself if it's a string\n",
    "                if isinstance(key, str):\n",
    "                    result.append(key)\n",
    "                \n",
    "                # Process the value\n",
    "                extract_strings(value)\n",
    "                \n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                extract_strings(item)\n",
    "                \n",
    "        elif isinstance(data, str) and data.strip():\n",
    "            # For string values, add only the raw string\n",
    "            result.append(data.strip())\n",
    "    \n",
    "    extract_strings(job_data)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_result = []\n",
    "    for item in result:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            unique_result.append(item)\n",
    "    \n",
    "    return unique_result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def ensure_correct_vector_format(vector_data):\n",
    "    \"\"\"\n",
    "    Ensures that the vector values are in the correct format for Pinecone:\n",
    "    - Must be a flat list of float values, not nested lists\n",
    "    \n",
    "    Args:\n",
    "        vector_data: The vector data to check/fix\n",
    "        \n",
    "    Returns:\n",
    "        Properly formatted vector data\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Check if the vector values are nested lists\n",
    "    if isinstance(vector_data, list) and vector_data and isinstance(vector_data[0], list):\n",
    "        # It's a nested list - flatten it to a 1D array\n",
    "        print(\"WARNING: Found nested list in vector data, flattening to 1D array\")\n",
    "        flat_array = np.array(vector_data).flatten()\n",
    "        return flat_array.tolist()\n",
    "    \n",
    "    # Ensure it's a list of floats\n",
    "    if isinstance(vector_data, np.ndarray):\n",
    "        return vector_data.tolist()\n",
    "    \n",
    "    return vector_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def process_and_vectorize_strings(jobs, pinecone_index, embedder, batch_size=64, max_retries=3):\n",
    "    \"\"\"\n",
    "    Process job descriptions, extract unique strings, vectorize them, and upload to Pinecone.\n",
    "    - Excludes 'details' section\n",
    "    - Removes duplicate strings\n",
    "    - Stores only the string text in the metadata (no job_id)\n",
    "    \n",
    "    Args:\n",
    "        jobs: List of job description dictionaries\n",
    "        pinecone_index: Pinecone index instance\n",
    "        embedder: Embedding generator instance\n",
    "        batch_size: Batch size for processing\n",
    "        max_retries: Maximum number of retries for failed operations\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with processing statistics\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import uuid\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Validate inputs\n",
    "    if not jobs:\n",
    "        print(\"Error: No job descriptions provided\")\n",
    "        return {\"status\": \"error\", \"reason\": \"no_jobs\"}\n",
    "        \n",
    "    if not pinecone_index:\n",
    "        print(\"Error: No Pinecone index provided\")\n",
    "        return {\"status\": \"error\", \"reason\": \"no_index\"}\n",
    "    \n",
    "    # Initialize metrics\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract all unique strings across all jobs\n",
    "    print(f\"Extracting strings from {len(jobs)} job descriptions...\")\n",
    "    all_unique_strings = set()\n",
    "    \n",
    "    for job in jobs:\n",
    "        try:\n",
    "            job_strings = extract_job_strings(job)\n",
    "            all_unique_strings.update(job_strings)\n",
    "        except Exception as e:\n",
    "            job_id = job.get(\"job_id\", \"unknown\")\n",
    "            print(f\"Error extracting strings from job {job_id}: {str(e)}\")\n",
    "    \n",
    "    all_strings_list = list(all_unique_strings)\n",
    "    print(f\"Total unique strings: {len(all_strings_list)}\")\n",
    "    \n",
    "    # Generate embeddings for all unique strings\n",
    "    string_to_embedding = {}\n",
    "    if all_strings_list:\n",
    "        # Process in batches\n",
    "        for i in range(0, len(all_strings_list), batch_size):\n",
    "            batch = all_strings_list[i:i+batch_size]\n",
    "            batch_num = i//batch_size + 1\n",
    "            total_batches = (len(all_strings_list) - 1) // batch_size + 1\n",
    "            \n",
    "            print(f\"Processing batch {batch_num}/{total_batches} ({batch_num/total_batches*100:.1f}%)...\")\n",
    "            \n",
    "            # Retry logic\n",
    "            for retry in range(max_retries + 1):\n",
    "                try:\n",
    "                    # Generate embeddings\n",
    "                    batch_embeddings = embedder.generate_embeddings(batch)\n",
    "                    \n",
    "                    # Store embeddings\n",
    "                    for text, embedding in zip(batch, batch_embeddings):\n",
    "                        string_to_embedding[text] = embedding\n",
    "                    \n",
    "                    break  # Success, exit retry loop\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if retry < max_retries:\n",
    "                        sleep_time = 2 ** retry\n",
    "                        print(f\"Error in batch {batch_num}, retrying in {sleep_time}s: {str(e)}\")\n",
    "                        time.sleep(sleep_time)\n",
    "                    else:\n",
    "                        print(f\"Failed after {max_retries} retries for batch {batch_num}: {str(e)}\")\n",
    "    \n",
    "    # Upload vector embeddings to Pinecone\n",
    "    vectors_uploaded = 0\n",
    "    if string_to_embedding:\n",
    "        vectors_to_upload = []\n",
    "        \n",
    "        # Prepare vectors - one for each unique string, no job association\n",
    "        for i, (text, embedding) in enumerate(string_to_embedding.items()):\n",
    "            # Ensure proper vector format\n",
    "            vector_values = ensure_correct_vector_format(embedding)\n",
    "            \n",
    "            # Create a unique ID for each string\n",
    "            vector_id = f\"string_{i}_{uuid.uuid4()}\"\n",
    "            \n",
    "            # Create metadata with only the text\n",
    "            metadata = {\n",
    "                \"text\": text[:1000] if len(text) > 1000 else text\n",
    "            }\n",
    "            \n",
    "            # Add to upload list\n",
    "            vectors_to_upload.append({\n",
    "                \"id\": vector_id,\n",
    "                \"values\": vector_values,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "        \n",
    "        print(f\"Prepared {len(vectors_to_upload)} vectors for upload\")\n",
    "        \n",
    "        # Upload in batches\n",
    "        with tqdm(total=len(vectors_to_upload), desc=\"Uploading to Pinecone\") as pbar:\n",
    "            for i in range(0, len(vectors_to_upload), batch_size):\n",
    "                batch = vectors_to_upload[i:i+batch_size]\n",
    "                \n",
    "                # Try upload with retries\n",
    "                for retry in range(max_retries + 1):\n",
    "                    try:\n",
    "                        pinecone_index.upsert(vectors=batch)\n",
    "                        vectors_uploaded += len(batch)\n",
    "                        pbar.update(len(batch))\n",
    "                        break  # Success, exit retry loop\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        if retry < max_retries:\n",
    "                            sleep_time = 2 ** retry\n",
    "                            print(f\"Upload error, retrying in {sleep_time}s: {str(e)}\")\n",
    "                            time.sleep(sleep_time)\n",
    "                        else:\n",
    "                            print(f\"Failed to upload batch after {max_retries} retries: {str(e)}\")\n",
    "    \n",
    "    # Return comprehensive statistics\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"total_jobs\": len(jobs),\n",
    "        \"total_unique_strings\": len(all_strings_list),\n",
    "        \"strings_embedded\": len(string_to_embedding),\n",
    "        \"vectors_uploaded\": vectors_uploaded,\n",
    "        \"processing_time\": total_time\n",
    "    }\n",
    "\n",
    "\n",
    "# Modified search function to work with string-only vectors\n",
    "def semantic_string_search(query, pinecone_index, embedder, similarity_threshold=0.8, top_k=100):\n",
    "    \"\"\"\n",
    "    Search for semantically similar strings in Pinecone.\n",
    "    \n",
    "    Args:\n",
    "        query: The text to search for\n",
    "        pinecone_index: The Pinecone index to search\n",
    "        embedder: Embedding generator instance\n",
    "        similarity_threshold: Minimum similarity score (0-1) to include in results\n",
    "        top_k: Maximum number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with text content and similarity score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_embedding = embedder.generate_embeddings([query])[0]\n",
    "        \n",
    "        search_results = pinecone_index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            include_values=False\n",
    "        )\n",
    "        \n",
    "        similar_strings = []\n",
    "        \n",
    "        matches = getattr(search_results, 'matches', [])\n",
    "        \n",
    "        for match in matches:\n",
    "\n",
    "            score = getattr(match, 'score', 0)\n",
    "            metadata = getattr(match, 'metadata', {})\n",
    "            \n",
    "            if score < similarity_threshold:\n",
    "                continue\n",
    "                \n",
    "            text = metadata.get('text', '')\n",
    "            \n",
    "            similar_strings.append({\n",
    "                'text': text,\n",
    "                'similarity': score\n",
    "            })\n",
    "        \n",
    "        return similar_strings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in semantic search: {str(e)}\")\n",
    "        return []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def run_vectorization_pipeline():\n",
    "    # Load jobs\n",
    "    jobs = load_job_descriptions(limit=37310)\n",
    "    \n",
    "    # Initialize Pinecone and embedder\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    embedder = ResilientSageMakerEmbedder(\n",
    "        endpoint_name=SAGEMAKER_ENDPOINT_NAME, \n",
    "        region=AWS_REGION\n",
    "    )\n",
    "    \n",
    "    # Process and upload\n",
    "    stats = process_and_vectorize_strings(\n",
    "        jobs=jobs,\n",
    "        pinecone_index=pinecone_index,\n",
    "        embedder=embedder,\n",
    "        batch_size=4\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing complete!\")\n",
    "    print(f\"Total unique strings: {stats.get('total_unique_strings', 0)}\")\n",
    "    print(f\"Strings embedded: {stats.get('strings_embedded', 0)}\")\n",
    "    print(f\"Vectors uploaded: {stats.get('vectors_uploaded', 0)}\")\n",
    "    \n",
    "    # Test a search\n",
    "    query = \"Python\"\n",
    "    results = semantic_string_search(\n",
    "        query=query,\n",
    "        pinecone_index=pinecone_index,\n",
    "        embedder=embedder,\n",
    "        similarity_threshold=0.8\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nStrings similar to '{query}':\")\n",
    "    for i, result in enumerate(results[:10]):  # Show top 10\n",
    "        print(f\"{i+1}. {result['text']} (Score: {result['similarity']:.4f})\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## run_vectorization_pipeline()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def process_all_jobs_in_batches(batch_size=1000, start_index=0, max_jobs=37312):\n",
    "    \"\"\"\n",
    "    Process all jobs in batches of specified size, continuing until all jobs are processed.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of jobs to load and process in each batch\n",
    "        start_index: Index to start from (for resuming)\n",
    "        max_jobs: Maximum number of jobs to process in total\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with total statistics\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    # Path to store progress tracking\n",
    "    progress_file = \"embedding_progress.json\"\n",
    "    \n",
    "    # Load progress if exists\n",
    "    if os.path.exists(progress_file):\n",
    "        try:\n",
    "            with open(progress_file, 'r') as f:\n",
    "                progress = json.load(f)\n",
    "                processed_ids = set(progress.get(\"processed_job_ids\", []))\n",
    "                last_processed_index = progress.get(\"last_index\", start_index)\n",
    "                # Use the greater of the provided start_index or the last_index from the progress file\n",
    "                start_index = max(start_index, last_processed_index)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading progress file: {e}\")\n",
    "            processed_ids = set()\n",
    "    else:\n",
    "        processed_ids = set()\n",
    "    \n",
    "    # Initialize Pinecone and embedder\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    embedder = ResilientSageMakerEmbedder(\n",
    "        endpoint_name=SAGEMAKER_ENDPOINT_NAME, \n",
    "        region=AWS_REGION\n",
    "    )\n",
    "    \n",
    "    # Initialize statistics\n",
    "    total_stats = {\n",
    "        \"total_processed\": 0,\n",
    "        \"total_strings_embedded\": 0,\n",
    "        \"total_vectors_uploaded\": 0,\n",
    "        \"batches_completed\": 0,\n",
    "        \"errors\": 0\n",
    "    }\n",
    "    \n",
    "    current_index = start_index\n",
    "    remaining_jobs = max_jobs - len(processed_ids)\n",
    "    \n",
    "    print(f\"Starting batch processing from index {current_index}\")\n",
    "    print(f\"Already processed {len(processed_ids)} jobs\")\n",
    "    print(f\"Approximately {remaining_jobs} jobs remaining\")\n",
    "    \n",
    "    # Process batches until reaching max_jobs or all jobs are processed\n",
    "    while current_index < max_jobs:\n",
    "        batch_start_time = time.time()\n",
    "        print(f\"\\nProcessing batch starting at index {current_index}\")\n",
    "        \n",
    "        # Load a batch of jobs\n",
    "        loaded_jobs = load_job_descriptions(limit=batch_size, skip=current_index)\n",
    "        \n",
    "        if not loaded_jobs:\n",
    "            print(f\"No more jobs to load after index {current_index}. Processing complete.\")\n",
    "            break\n",
    "            \n",
    "        print(f\"Loaded {len(loaded_jobs)} jobs for this batch\")\n",
    "        \n",
    "        # Filter out already processed jobs\n",
    "        batch_jobs = []\n",
    "        for job in loaded_jobs:\n",
    "            job_id = job.get(\"job_id\")\n",
    "            if job_id and job_id not in processed_ids:\n",
    "                batch_jobs.append(job)\n",
    "        \n",
    "        if not batch_jobs:\n",
    "            print(f\"All jobs in this batch have already been processed. Moving to next batch.\")\n",
    "            current_index += len(loaded_jobs)\n",
    "            continue\n",
    "            \n",
    "        print(f\"After filtering duplicates: {len(batch_jobs)} jobs to process\")\n",
    "        \n",
    "        try:\n",
    "            # Process this batch of jobs\n",
    "            batch_stats = process_and_vectorize_strings(\n",
    "                jobs=batch_jobs,\n",
    "                pinecone_index=pinecone_index,\n",
    "                embedder=embedder,\n",
    "                batch_size=10  # This is the embedder batch size, not the job batch size\n",
    "            )\n",
    "            \n",
    "            # Update statistics\n",
    "            total_stats[\"total_processed\"] += len(batch_jobs)\n",
    "            total_stats[\"total_strings_embedded\"] += batch_stats.get(\"strings_embedded\", 0)\n",
    "            total_stats[\"total_vectors_uploaded\"] += batch_stats.get(\"vectors_uploaded\", 0)\n",
    "            total_stats[\"batches_completed\"] += 1\n",
    "            \n",
    "            # Update processed IDs\n",
    "            for job in batch_jobs:\n",
    "                job_id = job.get(\"job_id\")\n",
    "                if job_id:\n",
    "                    processed_ids.add(job_id)\n",
    "            \n",
    "            # Save progress\n",
    "            progress = {\n",
    "                \"last_index\": current_index + len(loaded_jobs),\n",
    "                \"total_processed\": len(processed_ids),  # Total number of unique jobs processed\n",
    "                \"processed_job_ids\": list(processed_ids)\n",
    "            }\n",
    "            \n",
    "            with open(progress_file, 'w') as f:\n",
    "                json.dump(progress, f)\n",
    "                            batch_time = time.time() - batch_start_time\n",
    "            print(f\"Batch completed in {batch_time:.2f} seconds\")\n",
    "            print(f\"Processed {len(batch_jobs)} jobs in this batch\")\n",
    "            print(f\"Total unique strings in batch: {batch_stats.get('total_unique_strings', 0)}\")\n",
    "            print(f\"Strings embedded in batch: {batch_stats.get('strings_embedded', 0)}\")\n",
    "            print(f\"Vectors uploaded in batch: {batch_stats.get('vectors_uploaded', 0)}\")\n",
    "            print(f\"Total processed so far: {len(processed_ids)} jobs\")\n",
    "            \n",
    "            if total_stats[\"batches_completed\"] > 0:\n",
    "                avg_time_per_batch = batch_time\n",
    "                remaining_batches = (max_jobs - current_index - len(loaded_jobs)) // batch_size + 1\n",
    "                est_remaining_time = remaining_batches * avg_time_per_batch\n",
    "                hours, remainder = divmod(est_remaining_time, 3600)\n",
    "                minutes, seconds = divmod(remainder, 60)\n",
    "                print(f\"Estimated remaining time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {current_index}: {str(e)}\")\n",
    "            total_stats[\"errors\"] += 1\n",
    "            \n",
    "            progress = {\n",
    "                \"last_index\": current_index + len(loaded_jobs),\n",
    "                \"total_processed\": len(processed_ids),\n",
    "                \"processed_job_ids\": list(processed_ids),\n",
    "                \"last_error\": str(e)\n",
    "            }\n",
    "            \n",
    "            with open(progress_file, 'w') as f:\n",
    "                json.dump(progress, f)\n",
    "        \n",
    "        current_index += len(loaded_jobs)\n",
    "        \n",
    "        if len(processed_ids) >= max_jobs:\n",
    "            print(f\"Reached maximum number of jobs to process ({max_jobs}). Stopping.\")\n",
    "            break\n",
    "    \n",
    "    # Final report\n",
    "    print(\"\\n===== Processing Complete =====\")\n",
    "    print(f\"Total jobs processed: {total_stats['total_processed']}\")\n",
    "    print(f\"Total unique strings embedded: {total_stats['total_strings_embedded']}\")\n",
    "    print(f\"Total vectors uploaded to Pinecone: {total_stats['total_vectors_uploaded']}\")\n",
    "    print(f\"Total batches processed: {total_stats['batches_completed']}\")\n",
    "    print(f\"Errors encountered: {total_stats['errors']}\")\n",
    "    \n",
    "    return total_stats\n",
    "\n",
    "# stats = process_all_jobs_in_batches(batch_size=1000, start_index=0, max_jobs=37310)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Run the full batch processing\n",
    "def run_full_process():\n",
    "    \"\"\"\n",
    "    Run the full batch processing for all 37,000+ jobs\n",
    "    \"\"\"\n",
    "    # Define the parameters\n",
    "    batch_size = 1000  \n",
    "    start_index = 0  \n",
    "    max_jobs = 37310\n",
    "    \n",
    "    print(f\"Starting full batch processing with batch size of {batch_size}\")\n",
    "    print(f\"Will process up to {max_jobs} jobs starting from index {start_index}\")\n",
    "    \n",
    "    try:\n",
    "        stats = process_all_jobs_in_batches(\n",
    "            batch_size=batch_size, \n",
    "            start_index=start_index, \n",
    "            max_jobs=max_jobs\n",
    "        )\n",
    "        \n",
    "        print(\"\\n===== FINAL SUMMARY =====\")\n",
    "        print(f\"Total jobs processed: {stats['total_processed']}\")\n",
    "        print(f\"Total strings embedded: {stats['total_strings_embedded']}\")\n",
    "        print(f\"Total vectors uploaded: {stats['total_vectors_uploaded']}\")\n",
    "        print(f\"Total batches completed: {stats['batches_completed']}\")\n",
    "        print(f\"Errors encountered: {stats['errors']}\")\n",
    "        \n",
    "        try:\n",
    "            pinecone_index = initialize_pinecone()\n",
    "            \n",
    "            final_stats = pinecone_index.describe_index_stats()\n",
    "            print(\"\\n===== PINECONE INDEX STATISTICS =====\")\n",
    "            print(f\"Total vectors in index: {final_stats.get('total_vector_count', 0)}\")\n",
    "            print(f\"Dimension: {final_stats.get('dimension', 0)}\")\n",
    "            print(f\"Metric: {final_stats.get('metric', 'unknown')}\")\n",
    "            \n",
    "            namespaces = final_stats.get('namespaces', {})\n",
    "            for ns_name, ns_info in namespaces.items():\n",
    "                ns_name_display = f\"'{ns_name}'\" if ns_name else \"default\"\n",
    "                print(f\"Namespace {ns_name_display}: {ns_info.get('vector_count', 0)} vectors\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting final Pinecone statistics: {str(e)}\")\n",
    "        \n",
    "        return stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during batch processing: {str(e)}\")\n",
    "        raise"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# result = process_jobs_in_batches(batch_size=1000, start_index=0)\n",
    "# print(f\"Processing complete! {result['total_processed']} new jobs processed.\")\n",
    "# print(f\"Next starting index: {result['last_index']}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run_full_process()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# For a specific job, list all its strings\n",
    "if job_ids:\n",
    "    job_id = job_ids[0]  # Take the first job\n",
    "    strings = list_all_strings_for_job(pinecone_index, job_id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedder = ResilientSageMakerEmbedder(\n",
    "        endpoint_name=SAGEMAKER_ENDPOINT_NAME,\n",
    "        region=AWS_REGION\n",
    "    )\n",
    "\n",
    "def semantic_search(query_title, pinecone_index, embedder=None, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Search for semantically similar job titles in Pinecone.\n",
    "    \n",
    "    Args:\n",
    "        query_title: The job title to search for\n",
    "        pinecone_index: The Pinecone index to search\n",
    "        embedder: Embedding generator instance (optional)\n",
    "        similarity_threshold: Minimum similarity score (0-1) to include in results\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with job_id, job_title, and similarity score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create embedder if not provided\n",
    "        if embedder is None:\n",
    "            from random_embedding import RandomEmbeddingGenerator\n",
    "            print(\"No embedder provided. Creating RandomEmbeddingGenerator as fallback.\")\n",
    "            embedder = RandomEmbeddingGenerator()\n",
    "            \n",
    "        \n",
    "        query_text = f\"{query_title}\"  # You may need to adjust the format\n",
    "        query_embedding = embedder.generate_embeddings([query_text])[0]\n",
    "        \n",
    "        search_results = pinecone_index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=100,  # Get enough results to filter\n",
    "            include_metadata=True,\n",
    "            include_values=False  # only need the scores, not the vectors\n",
    "        )\n",
    "        \n",
    "        similar_text = []\n",
    "        \n",
    "        # Pinecone v6+ returns objects instead of dictionaries\n",
    "        matches = getattr(search_results, 'matches', [])\n",
    "        \n",
    "        for match in matches:\n",
    "            # Extract match properties\n",
    "            score = getattr(match, 'score', 0)\n",
    "            metadata = getattr(match, 'metadata', {})\n",
    "            \n",
    "            # Skip if below threshold\n",
    "            if score < similarity_threshold:\n",
    "                continue\n",
    "                \n",
    "            # Extract text\n",
    "            text = metadata.get('text', metadata.get('text', '')).replace('Text: ', '')\n",
    "            \n",
    "            similar_text.append({\n",
    "                'text': text,\n",
    "                'similarity': score\n",
    "            })\n",
    "        \n",
    "        return similar_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in semantic search: {str(e)}\")\n",
    "        return []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test the function with a sample string\n",
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "pinecone_index = pc.Index(PINECONE_INDEX_NAME)\n",
    "\n",
    "test_query = \"\"\n",
    "results = semantic_search(\n",
    "    test_query, \n",
    "    pinecone_index=pinecone_index,\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "print(f\"Jobs similar to '{test_query}' (similarity > 0.8):\")\n",
    "for job in results:\n",
    "    print(f\"- {job['text']}, Score: {job['similarity']:.4f})\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}