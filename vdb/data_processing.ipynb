{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_QjVRyydaBc"
   },
   "source": [
    "#01_data_processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xtQNBd-ZM0uX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fhN32Q4jM4mL"
   },
   "outputs": [],
   "source": [
    "# S3 Configuration\n",
    "S3_BUCKET = \"mirra-matcher-325\"  # S3 bucket name\n",
    "S3_RAW_PATH = \"final_inference_data.csv\"  # S3 path to raw XLSX file\n",
    "S3_PROCESSED_PREFIX = \"data/processed/jobs/\"  # S3 prefix for processed JSON files\n",
    "\n",
    "# S3 utility functions\n",
    "def s3_file_exists(bucket, key):\n",
    "    \"\"\"Check if a file exists in an S3 bucket.\"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket, Key=key)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        # The file does not exist or don't have permission\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W-ADE2ohNgqJ"
   },
   "outputs": [],
   "source": [
    "def s3_write_json(data, bucket, key):\n",
    "    \"\"\"Write JSON data directly to S3.\"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        # Convert dict to JSON string\n",
    "        json_data = json.dumps(data, indent=2)\n",
    "        # Upload as bytes\n",
    "        s3_client.put_object(\n",
    "            Body=json_data.encode('utf-8'),\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            ContentType='application/json'\n",
    "        )\n",
    "        print(f\"Successfully wrote JSON to s3://{bucket}/{key}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing JSON to S3: {str(e)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vV9LjkyYNi4D"
   },
   "outputs": [],
   "source": [
    "def load_data(bucket=\"mirra-matcher-325\", key=\"data/raw/final_inference_data.csv\"):\n",
    "    \"\"\"\n",
    "    Load job description data from the file in S3.\n",
    "    Only loads directly from S3 without local file handling.\n",
    "    \n",
    "    Args:\n",
    "        bucket: S3 bucket name\n",
    "        key: S3 key for the file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with job description data or None if file not found\n",
    "    \"\"\"\n",
    "    # Check if file exists in S3\n",
    "    if not s3_file_exists(bucket, key):\n",
    "        print(f\"File not found in S3: s3://{bucket}/{key}\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Create a direct read from S3 using pandas\n",
    "        s3_client = boto3.client('s3')\n",
    "        s3_resource = boto3.resource('s3')\n",
    "        \n",
    "        # Get the object directly from S3\n",
    "        obj = s3_resource.Object(bucket, key)\n",
    "        \n",
    "        # Use pandas to read XLSX directly from S3 object body\n",
    "        df = pd.read_csv(obj.get()['Body'])\n",
    "        \n",
    "        print(f\"Successfully loaded {len(df)} rows from S3: s3://{bucket}/{key}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from S3: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQjocC3sa2co"
   },
   "outputs": [],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CRMAkJPAa7eM"
   },
   "outputs": [],
   "source": [
    "def extract_qualifications(data):\n",
    "    \"\"\"\n",
    "    Extracts qualifications from the data in json_data column.\n",
    "    Uses ast.literal_eval to handle Python dictionary literals.\n",
    "    Returns an empty dict on error.\n",
    "    \"\"\"\n",
    "    import ast\n",
    "    \n",
    "    if pd.isna(data):\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        # If it's already a dictionary, use it directly\n",
    "        if isinstance(data, dict):\n",
    "            return data\n",
    "            \n",
    "        # For string data, try ast.literal_eval first\n",
    "        elif isinstance(data, str):\n",
    "            try:\n",
    "                return ast.literal_eval(data)\n",
    "            except (SyntaxError, ValueError):\n",
    "                # If literal_eval fails, try json.loads\n",
    "                try:\n",
    "                    return json.loads(data)\n",
    "                except json.JSONDecodeError:\n",
    "                    # Last resort: just return empty dict\n",
    "                    return {}\n",
    "            \n",
    "        # For other types, convert to string and try ast.literal_eval\n",
    "        else:\n",
    "            try:\n",
    "                return ast.literal_eval(str(data))\n",
    "            except (SyntaxError, ValueError):\n",
    "                return {}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing data: {type(e).__name__}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5Sz_ENJa-Xq"
   },
   "outputs": [],
   "source": [
    "df[\"qualifications\"] = df[\"json_data\"].apply(extract_qualifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nNumber of rows with non-empty qualifications: {(df['qualifications'].apply(len) > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBbZlO1wbAbe"
   },
   "outputs": [],
   "source": [
    "# create a unique ID for each job if not present\n",
    "if \"id\" not in df.columns or df[\"id\"].isna().any():\n",
    "    print(\"Adding job IDs where missing...\")\n",
    "    if \"id\" not in df.columns:\n",
    "        df[\"id\"] = [f\"job_{uuid.uuid4()}\" for _ in range(len(df))]\n",
    "    else:\n",
    "        df.loc[df[\"id\"].isna(), \"id\"] = [f\"job_{uuid.uuid4()}\" for _ in range(df[\"id\"].isna().sum())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLssFhzqbGW5"
   },
   "outputs": [],
   "source": [
    "# save each job as an individual JSON file\n",
    "saved_count = 0\n",
    "for _, row in df.iterrows():\n",
    "    job_id = row[\"id\"]\n",
    "    qualifications = row[\"qualifications\"]\n",
    "\n",
    "    # Skip rows with empty qualifications\n",
    "    if not qualifications:\n",
    "        continue\n",
    "\n",
    "    # Ensure job_id is in the qualifications\n",
    "    qualifications[\"job_id\"] = job_id\n",
    "\n",
    "    # Save to S3\n",
    "    output_key = f\"{S3_PROCESSED_PREFIX}{job_id}.json\"\n",
    "    if s3_write_json(qualifications, S3_BUCKET, output_key):\n",
    "        saved_count += 1\n",
    "\n",
    "print(f\"\\nProcessed and saved {saved_count} job descriptions to s3://{S3_BUCKET}/{S3_PROCESSED_PREFIX}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
